name: databricks/databricks
resources:
    databricks_access_control_rule_set:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_access_control_rule_set
        title: databricks_access_control_rule_set Resource
        examples:
            - name: automation_sp_rule_set
              manifest: |-
                {
                  "grant_rules": [
                    {
                      "principals": [
                        "${data.databricks_group.ds.acl_principal_id}"
                      ],
                      "role": "roles/servicePrincipal.user"
                    }
                  ],
                  "name": "accounts/${local.account_id}/servicePrincipals/${databricks_service_principal.automation_sp.application_id}/ruleSets/default"
                }
              dependencies:
                databricks_service_principal.automation_sp: |-
                    {
                      "display_name": "SP_FOR_AUTOMATION"
                    }
            - name: automation_sp_rule_set
              manifest: |-
                {
                  "grant_rules": [
                    {
                      "principals": [
                        "${databricks_group.ds.acl_principal_id}"
                      ],
                      "role": "roles/servicePrincipal.user"
                    }
                  ],
                  "name": "accounts/${local.account_id}/servicePrincipals/${databricks_service_principal.automation_sp.application_id}/ruleSets/default"
                }
              dependencies:
                databricks_group.ds: |-
                    {
                      "display_name": "Data Science"
                    }
                databricks_service_principal.automation_sp: |-
                    {
                      "display_name": "SP_FOR_AUTOMATION"
                    }
            - name: automation_sp_rule_set
              manifest: |-
                {
                  "grant_rules": [
                    {
                      "principals": [
                        "${databricks_group.ds.acl_principal_id}"
                      ],
                      "role": "roles/servicePrincipal.user"
                    }
                  ],
                  "name": "accounts/${local.account_id}/servicePrincipals/${databricks_service_principal.automation_sp.application_id}/ruleSets/default"
                }
              dependencies:
                databricks_group.ds: |-
                    {
                      "display_name": "Data Science"
                    }
                databricks_service_principal.automation_sp: |-
                    {
                      "application_id": "00000000-0000-0000-0000-000000000000",
                      "display_name": "SP_FOR_AUTOMATION"
                    }
            - name: automation_sp_rule_set
              manifest: |-
                {
                  "grant_rules": [
                    {
                      "principals": [
                        "${databricks_group.ds.acl_principal_id}"
                      ],
                      "role": "roles/servicePrincipal.user"
                    }
                  ],
                  "name": "accounts/${local.account_id}/servicePrincipals/${databricks_service_principal.automation_sp.application_id}/ruleSets/default"
                }
              dependencies:
                databricks_group.ds: |-
                    {
                      "display_name": "Data Science"
                    }
                databricks_service_principal.automation_sp: |-
                    {
                      "display_name": "SP_FOR_AUTOMATION"
                    }
            - name: ds_group_rule_set
              manifest: |-
                {
                  "grant_rules": [
                    {
                      "principals": [
                        "${data.databricks_user.john.acl_principal_id}"
                      ],
                      "role": "roles/group.manager"
                    }
                  ],
                  "name": "accounts/${local.account_id}/groups/${databricks_group.ds.id}/ruleSets/default"
                }
            - name: account_rule_set
              manifest: |-
                {
                  "grant_rules": [
                    {
                      "principals": [
                        "${data.databricks_user.john.acl_principal_id}"
                      ],
                      "role": "roles/group.manager"
                    },
                    {
                      "principals": [
                        "${data.databricks_group.ds.acl_principal_id}"
                      ],
                      "role": "roles/servicePrincipal.manager"
                    },
                    {
                      "principals": [
                        "${data.databricks_group.marketplace_admins.acl_principal_id}"
                      ],
                      "role": "roles/marketplace.admin"
                    }
                  ],
                  "name": "accounts/${local.account_id}/ruleSets/default"
                }
        argumentDocs:
            grant_rules: '- (Required) The access control rules to be granted by this rule set, consisting of a set of principals and roles to be granted to them.'
            grant_rules.principals: '- (Required) a list of principals who are granted a role. The following format is supported:'
            grant_rules.role: '- (Required) Role to be granted. The supported roles are listed below. For more information about these roles, refer to service principal roles, group roles or marketplace roles.'
            groups/{groupname}: (also exposed as acl_principal_id attribute of databricks_group resource).
            id: '- ID of the access control rule set - the same as name.'
            name: '- (Required) Unique identifier of a rule set. The name determines the resource to which the rule set applies. Currently, only default rule sets are supported. The following rule set formats are supported:'
            roles/group.manager: '- Manager of a group.'
            roles/marketplace.admin: '- Admin of marketplace.'
            roles/servicePrincipal.manager: '- Manager of a service principal.'
            roles/servicePrincipal.user: '- User of a service principal.'
            servicePrincipals/{applicationId}: (also exposed as acl_principal_id attribute of databricks_service_principal resource).
            users/{username}: (also exposed as acl_principal_id attribute of databricks_user resource).
        importStatements: []
    databricks_alert:
        subCategory: Databricks SQL
        description: '""subcategory: "Databricks SQL"'
        name: databricks_alert
        title: databricks_alert Resource
        examples:
            - name: alert
              manifest: |-
                {
                  "condition": [
                    {
                      "op": "GREATER_THAN",
                      "operand": [
                        {
                          "column": [
                            {
                              "name": "value"
                            }
                          ]
                        }
                      ],
                      "threshold": [
                        {
                          "value": [
                            {
                              "double_value": 42
                            }
                          ]
                        }
                      ]
                    }
                  ],
                  "display_name": "TF new alert",
                  "parent_path": "${databricks_directory.shared_dir.path}",
                  "query_id": "${databricks_query.this.id}"
                }
              references:
                parent_path: databricks_directory.shared_dir.path
                query_id: databricks_query.this.id
              dependencies:
                databricks_directory.shared_dir: |-
                    {
                      "path": "/Shared/Queries"
                    }
                databricks_query.this: |-
                    {
                      "display_name": "My Query Name",
                      "parent_path": "${databricks_directory.shared_dir.path}",
                      "query_text": "SELECT 42 as value",
                      "warehouse_id": "${databricks_sql_endpoint.example.id}"
                    }
            - name: alert
              manifest: |-
                {
                  "condition": [
                    {
                      "op": "GREATER_THAN",
                      "operand": [
                        {
                          "column": [
                            {
                              "name": "value"
                            }
                          ]
                        }
                      ],
                      "threshold": [
                        {
                          "value": [
                            {
                              "double_value": 42
                            }
                          ]
                        }
                      ]
                    }
                  ],
                  "display_name": "My Alert",
                  "parent_path": "${databricks_directory.shared_dir.path}",
                  "query_id": "${databricks_query.this.id}"
                }
              references:
                parent_path: databricks_directory.shared_dir.path
                query_id: databricks_query.this.id
        argumentDocs:
            bool_value: '- boolean value (true or false) to compare against boolean results.'
            column: '- (Required, Block) Block describing the column from the query result to use for comparison in alert evaluation:'
            condition: '- (Required) Trigger conditions of the alert. Block consists of the following attributes:'
            create_time: '- The timestamp string indicating when the alert was created.'
            custom_body: '- (Optional, String) Custom body of alert notification, if it exists. See Alerts API reference for custom templating instructions.'
            custom_subject: '- (Optional, String) Custom subject of alert notification, if it exists. This includes email subject, Slack notification header, etc. See Alerts API reference for custom templating instructions.'
            databricks_permissions: .
            databricks_sql_alert: ', for example, by executing the terraform state show databricks_sql_alert.alert command.'
            display_name: '- (Required, String) Name of the alert.'
            double_value: '- double value to compare against integer and double results.'
            empty_result_state: '- (Optional, String Enum) Alert state if the result is empty (UNKNOWN, OK, TRIGGERED)'
            id: '- unique ID of the Alert.'
            lifecycle_state: '- The workspace state of the alert. Used for tracking trashed status. (Possible values are ACTIVE or TRASHED).'
            name: '- (Required, String) Name of the column.'
            notify_on_ok: '- (Optional, Boolean) Whether to notify alert subscribers when alert returns back to normal.'
            op: '- (Required, String Enum) Operator used for comparison in alert evaluation. (Enum: GREATER_THAN, GREATER_THAN_OR_EQUAL, LESS_THAN, LESS_THAN_OR_EQUAL, EQUAL, NOT_EQUAL, IS_NULL)'
            operand: '- (Required, Block) Name of the column from the query result to use for comparison in alert evaluation:'
            options: 'block is converted into the condition block with the following changes:'
            owner_user_name: '- (Optional, String) Alert owner''s username.'
            parent: (if exists) is renamed to parent_path attribute and should be converted from folders/object_id to the actual path.
            parent_path: '- (Optional, String) The path to a workspace folder containing the alert. The default is the user''s home folder.  If changed, the alert will be recreated.'
            query_id: '- (Required, String) ID of the query evaluated by the alert.'
            rearm: attribute is renamed to seconds_to_retrigger.
            seconds_to_retrigger: '- (Optional, Integer) Number of seconds an alert must wait after being triggered to rearm itself. After rearming, it can be triggered again. If 0 or not specified, the alert will not be triggered again.'
            state: '- Current state of the alert''s trigger status (UNKNOWN, OK, TRIGGERED). This field is set to UNKNOWN if the alert has not yet been evaluated or ran into an error during the last evaluation.'
            string_value: '- string value to compare against string results.'
            terraform import databricks_alert.alert <alert-id>: command.
            terraform import.databricks_permissions: .
            terraform import.import: 'and removed blocks like this:'
            terraform import.terraform apply: command to apply changes.
            terraform import.terraform plan: command to check possible changes, such as value type change, etc.
            terraform plan: command to check possible changes, such as value type change, etc.
            terraform state rm databricks_sql_alert.alert: command.
            threshold: '- (Optional for IS_NULL operation, Block) Threshold value used for comparison in alert evaluation:'
            trigger_time: '- The timestamp string when the alert was last triggered if the alert has been triggered before.'
            update_time: '- The timestamp string indicating when the alert was updated.'
            value: '- (Required, Block) actual value used in comparison (one of the attributes is required):'
        importStatements: []
    databricks_artifact_allowlist:
        subCategory: Unity Catalog
        description: '""subcategory: "Unity Catalog"'
        name: databricks_artifact_allowlist
        title: databricks_artifact_allowlist Resource
        examples:
            - name: init_scripts
              manifest: |-
                {
                  "artifact_matcher": [
                    {
                      "artifact": "/Volumes/inits",
                      "match_type": "PREFIX_MATCH"
                    }
                  ],
                  "artifact_type": "INIT_SCRIPT"
                }
        argumentDocs:
            artifact_matcher.artifact: '- The artifact path or maven coordinate.'
            artifact_matcher.match_type: '- The pattern matching type of the artifact. Only PREFIX_MATCH is supported.'
            artifact_type: '- The artifact type of the allowlist. Can be INIT_SCRIPT, LIBRARY_JAR or LIBRARY_MAVEN. Change forces creation of a new resource.'
            created_at: '-  Time at which this artifact allowlist was set.'
            created_by: '-  Identity that set the artifact allowlist.'
            id: '- ID of the artifact allow list in form of metastore_id|artifact_type.'
            metastore_id: '- ID of the parent metastore.'
        importStatements: []
    databricks_automatic_cluster_update_setting Resource:
        subCategory: Settings
        description: '""subcategory: "Settings"'
        name: databricks_automatic_cluster_update_setting Resource
        title: databricks_automatic_cluster_update_setting Resource
        argumentDocs:
            automatic_cluster_update_workspace: (Required) block with following attributes
            day_of_week: '- the day of the week in uppercase, e.g. MONDAY or SUNDAY'
            enabled: '- (Required) The configuration details.'
            frequency: '- one of the FIRST_OF_MONTH, SECOND_OF_MONTH, THIRD_OF_MONTH, FOURTH_OF_MONTH, FIRST_AND_THIRD_OF_MONTH, SECOND_AND_FOURTH_OF_MONTH, EVERY_WEEK.'
            hours: '- hour to perform update: 0-23'
            maintenance_window: block that defines the maintenance frequency with the following arguments
            minutes: '- minute to perform update: 0-59'
            restart_even_if_no_updates_available: '- (Optional) To force clusters and other compute resources to restart during the maintenance window regardless of the availability of a new update.'
            week_day_based_schedule: block with the following arguments
            window_start_time: block that defines the time of your maintenance window. The default timezone is UTC and cannot be changed.
        importStatements: []
    databricks_budget:
        subCategory: FinOps
        description: '""subcategory: "FinOps"'
        name: databricks_budget
        title: databricks_budget Resource
        examples:
            - name: this
              manifest: |-
                {
                  "alert_configurations": [
                    {
                      "action_configurations": [
                        {
                          "action_type": "EMAIL_NOTIFICATION",
                          "target": "abc@gmail.com"
                        }
                      ],
                      "quantity_threshold": "840",
                      "quantity_type": "LIST_PRICE_DOLLARS_USD",
                      "time_period": "MONTH",
                      "trigger_type": "CUMULATIVE_SPENDING_EXCEEDED"
                    }
                  ],
                  "display_name": "databricks-workspace-budget",
                  "filter": [
                    {
                      "tags": [
                        {
                          "key": "Team",
                          "value": [
                            {
                              "operator": "IN",
                              "values": [
                                "Data Science"
                              ]
                            }
                          ]
                        },
                        {
                          "key": "Environment",
                          "value": [
                            {
                              "operator": "IN",
                              "values": [
                                "Development"
                              ]
                            }
                          ]
                        }
                      ],
                      "workspace_id": [
                        {
                          "operator": "IN",
                          "values": [
                            1234567890098765
                          ]
                        }
                      ]
                    }
                  ]
                }
        argumentDocs:
            account_id: '- The ID of the Databricks Account.'
            action_configurations: '- (Required) List of action configurations to take when the budget alert is triggered. Consists of the following fields:'
            action_type: '- (Required, String Enum) The type of action to take when the budget alert is triggered. (Enum: EMAIL_NOTIFICATION)'
            budget_configuration_id: '- The ID of the budget configuration.'
            display_name: '- (Required) Name of the budget in Databricks Account.'
            key: '- (Required, String) The key of the tag.'
            operator: '- (Required, String Enum) The operator to use for the filter. (Enum: IN)'
            quantity_threshold: '- (Required, String) The threshold for the budget alert to determine if it is in a triggered state. The number is evaluated based on quantity_type.'
            quantity_type: '- (Required, String Enum) The way to calculate cost for this budget alert. This is what quantity_threshold is measured in. (Enum: LIST_PRICE_DOLLARS_USD)'
            tags: '- (Optional) List of tags to filter by. Consists of the following fields:'
            target: '- (Required, String) The target of the action. For EMAIL_NOTIFICATION, this is the email address to send the notification to.'
            time_period: '- (Required, String Enum) The time window of usage data for the budget. (Enum: MONTH)'
            trigger_type: '- (Required, String Enum) The evaluation method to determine when this budget alert is in a triggered state. (Enum: CUMULATIVE_SPENDING_EXCEEDED)'
            value: '- (Required) Consists of the following fields:'
            values: '- (Required, List of numbers) The values to filter by.'
            workspace_id: '- (Optional) Filter by workspace ID (if empty, include usage all usage for this account). Consists of the following fields:'
        importStatements: []
    databricks_catalog:
        subCategory: Unity Catalog
        description: '""subcategory: "Unity Catalog"'
        name: databricks_catalog
        title: databricks_catalog Resource
        examples:
            - name: sandbox
              manifest: |-
                {
                  "comment": "this catalog is managed by terraform",
                  "name": "sandbox",
                  "properties": {
                    "purpose": "testing"
                  }
                }
        argumentDocs:
            comment: '- (Optional) User-supplied free-form text.'
            connection_name: '- (Optional) For Foreign Catalogs: the name of the connection to an external data source. Changes forces creation of a new resource.'
            enable_predictive_optimization: '- (Optional) Whether predictive optimization should be enabled for this object and objects under it. Can be ENABLE, DISABLE or INHERIT'
            force_destroy: '- (Optional) Delete catalog regardless of its contents.'
            id: '- ID of this catalog - same as the name.'
            isolation_mode: '- (Optional) Whether the catalog is accessible from all workspaces or a specific set of workspaces. Can be ISOLATED or OPEN. Setting the catalog to ISOLATED will automatically allow access from the current workspace.'
            metastore_id: '- ID of the parent metastore.'
            name: '- Name of Catalog relative to parent metastore.'
            options: '- (Optional) For Foreign Catalogs: the name of the entity from an external data source that maps to a catalog. For example, the database name in a PostgreSQL server.'
            owner: '- (Optional) Username/groupname/sp application_id of the catalog owner.'
            properties: '- (Optional) Extensible Catalog properties.'
            provider_name: '- (Optional) For Delta Sharing Catalogs: the name of the delta sharing provider. Change forces creation of a new resource.'
            share_name: '- (Optional) For Delta Sharing Catalogs: the name of the share under the share provider. Change forces creation of a new resource.'
            storage_root: '- (Optional if storage_root is specified for the metastore) Managed location of the catalog. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the metastore root location. Change forces creation of a new resource.'
        importStatements: []
    databricks_catalog_workspace_binding:
        subCategory: Unity Catalog
        description: '""subcategory: "Unity Catalog"'
        name: databricks_catalog_workspace_binding
        title: databricks_catalog_workspace_binding Resource
        examples:
            - name: sandbox
              manifest: |-
                {
                  "securable_name": "${databricks_catalog.sandbox.name}",
                  "workspace_id": "${databricks_mws_workspaces.other.workspace_id}"
                }
              references:
                securable_name: databricks_catalog.sandbox.name
                workspace_id: databricks_mws_workspaces.other.workspace_id
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "isolation_mode": "ISOLATED",
                      "name": "sandbox"
                    }
        argumentDocs:
            binding_type: '- Binding mode. Default to BINDING_TYPE_READ_WRITE. Possible values are BINDING_TYPE_READ_ONLY, BINDING_TYPE_READ_WRITE'
            securable_name: '- Name of securable. Change forces creation of a new resource.'
            securable_type: '- Type of securable. Default to catalog. Change forces creation of a new resource.'
            workspace_id: '- ID of the workspace. Change forces creation of a new resource.'
        importStatements: []
    databricks_cluster:
        subCategory: Compute
        description: '""subcategory: "Compute"'
        name: databricks_cluster
        title: databricks_cluster Resource
        examples:
            - name: shared_autoscaling
              manifest: |-
                {
                  "autoscale": [
                    {
                      "max_workers": 50,
                      "min_workers": 1
                    }
                  ],
                  "autotermination_minutes": 20,
                  "cluster_name": "Shared Autoscaling",
                  "node_type_id": "${data.databricks_node_type.smallest.id}",
                  "spark_version": "${data.databricks_spark_version.latest_lts.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
                spark_version: data.databricks_spark_version.latest_lts.id
            - name: shared_autoscaling
              manifest: |-
                {
                  "autoscale": [
                    {
                      "max_workers": 50,
                      "min_workers": 1
                    }
                  ],
                  "autotermination_minutes": 20,
                  "cluster_name": "Shared Autoscaling",
                  "node_type_id": "${data.databricks_node_type.smallest.id}",
                  "spark_conf": {
                    "spark.databricks.io.cache.enabled": true,
                    "spark.databricks.io.cache.maxDiskUsage": "50g",
                    "spark.databricks.io.cache.maxMetaDataCache": "1g"
                  },
                  "spark_version": "${data.databricks_spark_version.latest_lts.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
                spark_version: data.databricks_spark_version.latest_lts.id
            - name: single_node
              manifest: |-
                {
                  "autotermination_minutes": 20,
                  "cluster_name": "Single Node",
                  "custom_tags": {
                    "ResourceClass": "SingleNode"
                  },
                  "node_type_id": "${data.databricks_node_type.smallest.id}",
                  "spark_conf": {
                    "spark.databricks.cluster.profile": "singleNode",
                    "spark.master": "local[*]"
                  },
                  "spark_version": "${data.databricks_spark_version.latest_lts.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
                spark_version: data.databricks_spark_version.latest_lts.id
            - name: cluster_with_table_access_control
              manifest: |-
                {
                  "autotermination_minutes": 20,
                  "cluster_name": "Shared High-Concurrency",
                  "custom_tags": {
                    "ResourceClass": "Serverless"
                  },
                  "node_type_id": "${data.databricks_node_type.smallest.id}",
                  "spark_conf": {
                    "spark.databricks.cluster.profile": "serverless",
                    "spark.databricks.repl.allowedLanguages": "python,sql"
                  },
                  "spark_version": "${data.databricks_spark_version.latest_lts.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
                spark_version: data.databricks_spark_version.latest_lts.id
            - name: this
              manifest: |-
                {
                  "autoscale": [
                    {
                      "max_workers": 50,
                      "min_workers": 1
                    }
                  ],
                  "autotermination_minutes": 20,
                  "aws_attributes": [
                    {
                      "availability": "SPOT",
                      "first_on_demand": 1,
                      "spot_bid_price_percent": 100,
                      "zone_id": "us-east-1"
                    }
                  ],
                  "cluster_name": "Shared Autoscaling",
                  "node_type_id": "${data.databricks_node_type.smallest.id}",
                  "spark_version": "${data.databricks_spark_version.latest.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
                spark_version: data.databricks_spark_version.latest.id
            - name: this
              manifest: |-
                {
                  "autoscale": [
                    {
                      "max_workers": 50,
                      "min_workers": 1
                    }
                  ],
                  "autotermination_minutes": 20,
                  "azure_attributes": [
                    {
                      "availability": "SPOT_WITH_FALLBACK_AZURE",
                      "first_on_demand": 1,
                      "spot_bid_max_price": 100
                    }
                  ],
                  "cluster_name": "Shared Autoscaling",
                  "node_type_id": "${data.databricks_node_type.smallest.id}",
                  "spark_version": "${data.databricks_spark_version.latest.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
                spark_version: data.databricks_spark_version.latest.id
            - name: this
              manifest: |-
                {
                  "autoscale": [
                    {
                      "max_workers": 50,
                      "min_workers": 1
                    }
                  ],
                  "autotermination_minutes": 20,
                  "cluster_name": "Shared Autoscaling",
                  "gcp_attributes": [
                    {
                      "availability": "PREEMPTIBLE_WITH_FALLBACK_GCP",
                      "zone_id": "AUTO"
                    }
                  ],
                  "node_type_id": "${data.databricks_node_type.smallest.id}",
                  "spark_version": "${data.databricks_spark_version.latest.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
                spark_version: data.databricks_spark_version.latest.id
            - name: this
              manifest: |-
                {
                  "docker_image": [
                    {
                      "basic_auth": [
                        {
                          "password": "${azurerm_container_registry.this.admin_password}",
                          "username": "${azurerm_container_registry.this.admin_username}"
                        }
                      ],
                      "url": "${docker_registry_image.this.name}"
                    }
                  ]
                }
              references:
                docker_image.basic_auth.password: azurerm_container_registry.this.admin_password
                docker_image.basic_auth.username: azurerm_container_registry.this.admin_username
                docker_image.url: docker_registry_image.this.name
              dependencies:
                docker_registry_image.this: |-
                    {
                      "build": [
                        {}
                      ],
                      "name": "${azurerm_container_registry.this.login_server}/sample:latest"
                    }
            - name: with_nfs
              manifest: |-
                {
                  "cluster_mount_info": [
                    {
                      "local_mount_dir_path": "/mnt/nfs-test",
                      "network_filesystem_info": [
                        {
                          "mount_options": "sec=sys,vers=3,nolock,proto=tcp",
                          "server_address": "${local.storage_account}.blob.core.windows.net"
                        }
                      ],
                      "remote_mount_dir_path": "${local.storage_account}/${local.storage_container}"
                    }
                  ]
                }
            - name: with_nfs
              manifest: |-
                {
                  "workload_type": [
                    {
                      "clients": [
                        {
                          "jobs": false,
                          "notebooks": true
                        }
                      ]
                    }
                  ]
                }
        argumentDocs:
            AUTO: ': Databricks picks an availability zone to schedule the cluster on.'
            HA: '(default): High availability, spread nodes across availability zones for a Databricks deployment region.'
            allow_cluster_create: argument set would still be able to create clusters, but within the boundary of the policy.
            apply_policy_default_values: '- (Optional) Whether to use policy default values for missing cluster attributes.'
            autoscale.max_workers: '- (Optional) The maximum number of workers to which the cluster can scale up when overloaded. max_workers must be strictly greater than min_workers.'
            autoscale.min_workers: '- (Optional) The minimum number of workers to which the cluster can scale down when underutilized. It is also the initial number of workers the cluster will have after creation.'
            autoscale.spark.databricks.cluster.profile: must have value singleNode
            autoscale.spark.master: must have prefix local, like local[*]
            autotermination_minutes: '- (Optional) Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to 60.  We highly recommend having this setting present for Interactive/BI clusters.'
            aws_attributes.availability: '- (Optional) Availability type used for all subsequent nodes past the first_on_demand ones. Valid values are SPOT, SPOT_WITH_FALLBACK and ON_DEMAND. Note: If first_on_demand is zero, this availability type will be used for the entire cluster. Backend default value is SPOT_WITH_FALLBACK and could change in the future'
            aws_attributes.ebs_volume_count: '- (Optional) The number of volumes launched for each instance. You can choose up to 10 volumes. This feature is only enabled for supported node types. Legacy node types cannot specify custom EBS volumes. For node types with no instance store, at least one EBS volume needs to be specified; otherwise, cluster creation will fail. These EBS volumes will be mounted at /ebs0, /ebs1, and etc. Instance store volumes will be mounted at /local_disk0, /local_disk1, and etc. If EBS volumes are attached, Databricks will configure Spark to use only the EBS volumes for scratch storage because heterogeneously sized scratch devices can lead to inefficient disk utilization. If no EBS volumes are attached, Databricks will configure Spark to use instance store volumes. If EBS volumes are specified, then the Spark configuration spark.local.dir will be overridden.'
            aws_attributes.ebs_volume_size: '- (Optional) The size of each EBS volume (in GiB) launched for each instance. For general purpose SSD, this value must be within the range 100 - 4096. For throughput optimized HDD, this value must be within the range 500 - 4096. Custom EBS volumes cannot be specified for the legacy node types (memory-optimized and compute-optimized).'
            aws_attributes.ebs_volume_type: '- (Optional) The type of EBS volumes that will be launched with this cluster. Valid values are GENERAL_PURPOSE_SSD or THROUGHPUT_OPTIMIZED_HDD. Use this option only if you''re not picking Delta Optimized  node types.'
            aws_attributes.first_on_demand: '- (Optional) The first first_on_demand nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, first_on_demand nodes will be placed on on-demand instances, and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster. Backend default value is 1 and could change in the future'
            aws_attributes.instance_profile_arn: '- (Optional) Nodes for this cluster will only be placed on AWS instances with this instance profile. Please see databricks_instance_profile resource documentation for extended examples on adding a valid instance profile using Terraform.'
            aws_attributes.spot_bid_price_percent: '- (Optional) The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the cluster needs a new i3.xlarge spot instance, then the max price is half of the price of on-demand i3.xlarge instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand i3.xlarge instances. If not specified, the default value is 100. When spot instances are requested for this cluster, only spot instances whose max price percentage matches this field will be considered. For safety, we enforce this field to be no more than 10000.'
            aws_attributes.zone_id: '- (Required) Identifier for the availability zone/datacenter in which the cluster resides. This string will be of a form like us-west-2a. The provided availability zone must be in the same region as the Databricks deployment. For example, us-west-2a is not a valid zone ID if the Databricks deployment resides in the us-east-1 region. Enable automatic availability zone selection ("Auto-AZ"), by setting the value auto. Databricks selects the AZ based on available IPs in the workspace subnets and retries in other availability zones if AWS returns insufficient capacity errors.'
            azure_attributes.availability: '- (Optional) Availability type used for all subsequent nodes past the first_on_demand ones. Valid values are SPOT_AZURE, SPOT_WITH_FALLBACK_AZURE, and ON_DEMAND_AZURE. Note: If first_on_demand is zero, this availability type will be used for the entire cluster.'
            azure_attributes.first_on_demand: '- (Optional) The first first_on_demand nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, first_on_demand nodes will be placed on on-demand instances, and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster.'
            azure_attributes.spot_bid_max_price: '- (Optional) The max bid price used for Azure spot instances. You can set this to greater than or equal to the current spot price. You can also set this to -1, which specifies that the instance cannot be evicted on the basis of price. The price for the instance will be the current price for spot instances or the price for a standard instance.'
            canned_acl: '- (Optional) Set canned access control list, e.g. bucket-owner-full-control. If canned_cal is set, the cluster instance profile must have s3:PutObjectAcl permission on the destination bucket and prefix. The full list of possible canned ACLs can be found here. By default, only the object owner gets full control. If you are using a cross-account role for writing data, you may want to set bucket-owner-full-control to make bucket owners able to read the logs.'
            cluster_mount_info.local_mount_dir_path: '- (Required) path inside the Spark container.'
            cluster_mount_info.network_filesystem_info: '- block specifying connection. It consists of:'
            cluster_mount_info.remote_mount_dir_path: '- (Optional) string specifying path to mount on the remote service.'
            cluster_name: '- (Optional) Cluster name, which doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string.'
            custom_tags: '- (Optional) Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS EC2 instances and EBS volumes) with these tags in addition to default_tags. If a custom cluster tag has the same name as a default cluster tag, the custom tag is prefixed with an x_ when it is propagated.'
            data_security_mode: '- (Optional) Select the security features of the cluster. Unity Catalog requires SINGLE_USER or USER_ISOLATION mode. LEGACY_PASSTHROUGH for passthrough cluster and LEGACY_TABLE_ACL for Table ACL cluster. If omitted, default security features are enabled. To disable security features use NONE or legacy mode NO_ISOLATION. In the Databricks UI, this has been recently been renamed Access Mode and USER_ISOLATION has been renamed Shared, but use these terms here.'
            dbfs:/mnt/name: .
            default_tags: '- (map) Tags that are added by Databricks by default, regardless of any custom_tags that may have been added. These include: Vendor: Databricks, Creator: <username_of_creator>, ClusterName: <name_of_cluster>, ClusterId: <id_of_cluster>, Name: , and any workspace and pool tags.'
            destination: '- S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.'
            docker_image.basic_auth: '- (Optional) basic_auth.username and basic_auth.password for Docker repository. Docker registry credentials are encrypted when they are stored in Databricks internal storage and when they are passed to a registry upon fetching Docker images at cluster launch. However, other authenticated and authorized API users of this workspace can access the username and password.'
            docker_image.url: '- URL for the Docker image'
            driver_instance_pool_id: (Optional) - similar to instance_pool_id, but for driver node. If omitted, and instance_pool_id is specified, then the driver will be allocated from that pool.
            driver_node_type_id: '- (Optional) The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as node_type_id defined above.'
            enable_elastic_disk: '- (Optional) If you don’t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance’s local storage). To scale down EBS usage, make sure you have autotermination_minutes and autoscale attributes set. More documentation available at cluster configuration page.'
            enable_encryption: '- (Optional) Enable server-side encryption, false by default.'
            enable_local_disk_encryption: '- (Optional) Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster’s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.'
            encryption_type: '- (Optional) The encryption type, it could be sse-s3 or sse-kms. It is used only when encryption is enabled, and the default type is sse-s3.'
            endpoint: '- (Optional) S3 endpoint, e.g. https://s3-us-west-2.amazonaws.com. Either region or endpoint needs to be set. If both are set, the endpoint is used.'
            gcp_attributes.availability: ', and will be removed soon.'
            gcp_attributes.boot_disk_size: (optional, int) Boot disk size in GB
            gcp_attributes.google_service_account: '- (Optional, string) Google Service Account email address that the cluster uses to authenticate with Google Identity. This field is used for authentication with the GCS and BigQuery data sources.'
            gcp_attributes.local_ssd_count: (optional, int) Number of local SSD disks (each is 375GB in size) that will be attached to each node of the cluster.
            gcp_attributes.use_preemptible_executors: '- (Optional, bool) if we should use preemptible executors (GCP documentation). Warning: this field is deprecated in favor of'
            gcp_attributes.zone_id: '(optional)  Identifier for the availability zone in which the cluster resides. This can be one of the following:'
            id: '- Canonical unique identifier for the cluster.'
            idempotency_token: '- (Optional) An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster''s ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.'
            instance_pool_id: (Optional - required if node_type_id is not given) - To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster’s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to TERMINATED, the instances it used are returned to the pool and reused by a different cluster.
            instance_profile_arn: (AWS only) can control which data a given cluster can access through cloud-native controls.
            is_pinned: '- (Optional) boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters'' maximum number is limited to 100, so apply may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).'
            kms_key: '- (Optional) KMS key used if encryption is enabled and encryption type is set to sse-kms.'
            mount_options: '- (Optional) string that will be passed as options passed to the mount command.'
            no_wait: '- (Optional) If true, the provider will not wait for the cluster to reach RUNNING state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).'
            node_type_id: '- (Required - optional if instance_pool_id is given) Any supported databricks_node_type id. If instance_pool_id is specified, this field is not needed.'
            num_workers: '- (Optional) Number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes.'
            policy_id: '- (Optional) Identifier of Cluster Policy to validate cluster and preset certain defaults. The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters. For example, when you specify policy_id of external metastore policy, you still have to fill in relevant keys for spark_conf.  If relevant fields aren''t filled in, then it will cause the configuration drift detected on each plan/apply, and Terraform will try to apply the detected changes.'
            region: '- (Optional) S3 region, e.g. us-west-2. Either region or endpoint must be set. If both are set, the endpoint is used.'
            runtime_engine: '- (Optional) The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: PHOTON, STANDARD.'
            server_address: '- (Required) host name.'
            single_user_name: '- (Optional) The optional user name of the user to assign to an interactive cluster. This field is required when using data_security_mode set to SINGLE_USER or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).'
            spark.databricks.cluster.profile: set to serverless
            spark.databricks.repl.allowedLanguages: 'set to a list of supported languages, for example: python,sql, or python,sql,r.  Scala is not supported!'
            spark_conf: '- (Optional) Map with key-value pairs to fine-tune Spark clusters, where you can provide custom Spark configuration properties in a cluster configuration.'
            spark_env_vars: '- (Optional) Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X=''Y'') while launching the driver and workers.'
            spark_version: '- (Required) Runtime version of the cluster. Any supported databricks_spark_version id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.'
            ssh_public_keys: '- (Optional) SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.'
            state: '- (string) State of the cluster.'
            workload_type.jobs: '- (Optional) boolean flag defining if it''s possible to run Databricks Jobs on this cluster. Default: true.'
            workload_type.notebooks: '- (Optional) boolean flag defining if it''s possible to run notebooks on this cluster. Default: true.'
        importStatements: []
    databricks_cluster_policy:
        subCategory: Compute
        description: '""subcategory: "Compute"'
        name: databricks_cluster_policy
        title: databricks_cluster_policy Resource
        examples:
            - name: fair_use
              manifest: |-
                {
                  "definition": "${jsonencode(merge(local.default_policy, var.policy_overrides))}",
                  "libraries": [
                    {
                      "pypi": [
                        {
                          "package": "databricks-sdk==0.12.0"
                        }
                      ]
                    },
                    {
                      "maven": [
                        {
                          "coordinates": "com.oracle.database.jdbc:ojdbc8:XXXX"
                        }
                      ]
                    }
                  ],
                  "name": "${var.team} cluster policy"
                }
              dependencies:
                databricks_permissions.can_use_cluster_policyinstance_profile: |-
                    {
                      "access_control": [
                        {
                          "group_name": "${var.team}",
                          "permission_level": "CAN_USE"
                        }
                      ],
                      "cluster_policy_id": "${databricks_cluster_policy.fair_use.id}"
                    }
            - name: personal_vm
              manifest: |-
                {
                  "name": "Personal Compute",
                  "policy_family_definition_overrides": "${jsonencode(local.personal_vm_override)}",
                  "policy_family_id": "personal-vm"
                }
        argumentDocs:
            Free form: policy and create fully-configurable clusters.
            definition: '- Policy definition: JSON document expressed in Databricks Policy Definition Language. Cannot be used with policy_family_id'
            description: '- (Optional) Additional human-readable description of the cluster policy.'
            id: '- Canonical unique identifier for the cluster policy. This is equal to policy_id.'
            max_clusters_per_user: '- (Optional, integer) Maximum number of clusters allowed per user. When omitted, there is no limit. If specified, value must be greater than zero.'
            name: '- the name of the built-in cluster policy.'
            policy_family_definition_overrides: '- settings to override in the built-in cluster policy.'
            policy_family_id: '- the ID of the cluster policy family used for built-in cluster policy.'
            policy_id: '- Canonical unique identifier for the cluster policy.'
            spark_version: parameter in databricks_cluster and other resources.
        importStatements: []
    databricks_compliance_security_profile_setting Resource:
        subCategory: Settings
        description: '""subcategory: "Settings"'
        name: databricks_compliance_security_profile_setting Resource
        title: databricks_compliance_security_profile_setting Resource
        argumentDocs:
            compliance_security_profile_workspace: 'block with following attributes:'
            compliance_standards: '- (Required) Enable one or more compliance standards on the workspace, e.g. HIPAA, PCI_DSS, FEDRAMP_MODERATE'
            is_enabled: '- (Required) Enable the Compliance Security Profile on the workspace'
        importStatements: []
    databricks_connection:
        subCategory: Unity Catalog
        description: '""subcategory: "Unity Catalog"'
        name: databricks_connection
        title: databricks_connection Resource
        examples:
            - name: mysql
              manifest: |-
                {
                  "comment": "this is a connection to mysql db",
                  "connection_type": "MYSQL",
                  "name": "mysql_connection",
                  "options": {
                    "host": "test.mysql.database.azure.com",
                    "password": "password",
                    "port": "3306",
                    "user": "user"
                  },
                  "properties": {
                    "purpose": "testing"
                  }
                }
            - name: bigquery
              manifest: |-
                {
                  "comment": "this is a connection to BQ",
                  "connection_type": "BIGQUERY",
                  "name": "bq_connection",
                  "options": {
                    "GoogleServiceAccountKeyJson": "${jsonencode({\n      \"type\" : \"service_account\",\n      \"project_id\" : \"PROJECT_ID\",\n      \"private_key_id\" : \"KEY_ID\",\n      \"private_key\" : \"-----BEGIN PRIVATE KEY-----\\nPRIVATE_KEY\\n-----END PRIVATE KEY-----\\n\",\n      \"client_email\" : \"SERVICE_ACCOUNT_EMAIL\",\n      \"client_id\" : \"CLIENT_ID\",\n      \"auth_uri\" : \"https://accounts.google.com/o/oauth2/auth\",\n      \"token_uri\" : \"https://accounts.google.com/o/oauth2/token\",\n      \"auth_provider_x509_cert_url\" : \"https://www.googleapis.com/oauth2/v1/certs\",\n      \"client_x509_cert_url\" : \"https://www.googleapis.com/robot/v1/metadata/x509/SERVICE_ACCOUNT_EMAIL\",\n      \"universe_domain\" : \"googleapis.com\"\n    })}"
                  },
                  "properties": {
                    "purpose": "testing"
                  }
                }
        argumentDocs:
            comment: '- (Optional) Free-form text.'
            connection_type: '- Connection type. BIGQUERY MYSQL POSTGRESQL SNOWFLAKE REDSHIFT SQLDW SQLSERVER, SALESFORCE or DATABRICKS are supported. Up-to-date list of connection type supported'
            id: '- ID of this connection in form of <metastore_id>|<name>.'
            name: '- Name of the Connection.'
            options: '- The key value of options required by the connection, e.g. host, port, user, password or GoogleServiceAccountKeyJson. Please consult the documentation for the required option.'
            owner: '- (Optional) Name of the connection owner.'
            properties: '-  (Optional) Free-form connection properties.'
        importStatements: []
    databricks_custom_app_integration:
        subCategory: Apps
        description: '""subcategory: "Apps"'
        name: databricks_custom_app_integration
        title: databricks_custom_app_integration Resource
        examples:
            - name: this
              manifest: |-
                {
                  "name": "custom_integration_name",
                  "redirect_urls": [
                    "https://example.com"
                  ],
                  "scopes": [
                    "all-apis"
                  ],
                  "token_access_policy": [
                    {
                      "access_token_ttl_in_minutes": 15,
                      "refresh_token_ttl_in_minutes": 30
                    }
                  ]
                }
        argumentDocs:
            access_token_ttl_in_minutes: '- access token time to live (TTL) in minutes.'
            client_id: '- OAuth client-id generated by Databricks'
            client_secret: '- OAuth client-secret generated by the Databricks if this is a confidential OAuth app.'
            confidential: '- Indicates whether an OAuth client secret is required to authenticate this client. Default to false. Change requires a new resource.'
            integration_id: '- Unique integration id for the custom OAuth app.'
            name: '- (Required) Name of the custom OAuth app. Change requires a new resource.'
            redirect_urls: '- List of OAuth redirect urls.'
            refresh_token_ttl_in_minutes: '- refresh token TTL in minutes. The TTL of refresh token cannot be lower than TTL of access token.'
            scopes: '- OAuth scopes granted to the application. Supported scopes: all-apis, sql, offline_access, openid, profile, email.'
        importStatements: []
    databricks_dashboard:
        subCategory: Workspace
        description: '""subcategory: "Workspace"'
        name: databricks_dashboard
        title: databricks_dashboard Resource
        examples:
            - name: dashboard
              manifest: |-
                {
                  "display_name": "New Dashboard",
                  "embed_credentials": false,
                  "parent_path": "/Shared/provider-test",
                  "serialized_dashboard": "{\"pages\":[{\"name\":\"new_name\",\"displayName\":\"New Page\"}]}",
                  "warehouse_id": "${data.databricks_sql_warehouse.starter.id}"
                }
              references:
                warehouse_id: data.databricks_sql_warehouse.starter.id
            - name: dashboard
              manifest: |-
                {
                  "display_name": "New Dashboard",
                  "embed_credentials": false,
                  "file_path": "${path.module}/dashboard.json",
                  "parent_path": "/Shared/provider-test",
                  "warehouse_id": "${data.databricks_sql_warehouse.starter.id}"
                }
              references:
                warehouse_id: data.databricks_sql_warehouse.starter.id
        argumentDocs:
            display_name: '- (Required) The display name of the dashboard.'
            embed_credentials: '- (Optional) Whether to embed credentials in the dashboard. Default is true.'
            file_path: '- (Optional) The path to the dashboard JSON file. Conflicts with serialized_dashboard.'
            id: '- The unique ID of the dashboard.'
            parent_path: '- (Required) The workspace path of the folder containing the dashboard. Includes leading slash and no trailing slash.  If folder doesn''t exist, it will be created.'
            serialized_dashboard: '- (Optional) The contents of the dashboard in serialized string form. Conflicts with file_path.'
            warehouse_id: '- (Required) The warehouse ID used to run the dashboard.'
        importStatements: []
    databricks_dbfs_file:
        subCategory: Storage
        description: '""subcategory: "Storage"'
        name: databricks_dbfs_file
        title: databricks_dbfs_file Resource
        examples:
            - name: this
              manifest: |-
                {
                  "path": "/tmp/main.tf",
                  "source": "${path.module}/main.tf"
                }
            - name: this
              manifest: |-
                {
                  "content_base64": "${base64encode(\u003c\u003c-EOT\n    Hello, world!\n    Module is ${abspath(path.module)}\n    EOT\n  )}",
                  "path": "/tmp/this.txt"
                }
            - name: app
              manifest: |-
                {
                  "path": "/FileStore/baz.whl",
                  "source": "${path.module}/baz.whl"
                }
              dependencies:
                databricks_library.app: |-
                    {
                      "cluster_id": "${each.key}",
                      "for_each": "${data.databricks_clusters.all.ids}",
                      "whl": "${databricks_dbfs_file.app.dbfs_path}"
                    }
        argumentDocs:
            content_base64: '- Encoded file contents. Conflicts with source. Use of content_base64 is discouraged, as it''s increasing memory footprint of Terraform state and should only be used in exceptional circumstances, like creating a data pipeline configuration file.'
            dbfs:/mnt/name: .
            dbfs_path: '- Path, but with dbfs: prefix.'
            file_size: '- The file size of the file that is being tracked by this resource in bytes.'
            id: '- Same as path.'
            path: '- (Required) The path of the file in which you wish to save.'
            source: '- The full absolute path to the file. Conflicts with content_base64.'
        importStatements: []
    databricks_default_namespace_setting:
        subCategory: Settings
        description: '""subcategory: "Settings"'
        name: databricks_default_namespace_setting
        title: databricks_default_namespace_setting Resource
        examples:
            - name: this
              manifest: |-
                {
                  "namespace": [
                    {
                      "value": "namespace_value"
                    }
                  ]
                }
        argumentDocs:
            namespace: '- (Required) The configuration details.'
            value: '- (Required) The value for the setting.'
        importStatements: []
    databricks_directory:
        subCategory: Workspace
        description: '""subcategory: "Workspace"'
        name: databricks_directory
        title: databricks_directory Resource
        examples:
            - name: my_custom_directory
              manifest: |-
                {
                  "path": "/my_custom_directory"
                }
        argumentDocs:
            delete_recursive: '- Whether or not to trigger a recursive delete of this directory and its resources when deleting this on Terraform. Defaults to false'
            id: '- Path of directory on workspace'
            object_id: '- Unique identifier for a DIRECTORY'
            path: '- (Required) The absolute path of the directory, beginning with "/", e.g. "/Demo".'
            spark_version: parameter in databricks_cluster and other resources.
            workspace_path: '- path on Workspace File System (WSFS) in form of /Workspace + path'
        importStatements: []
    databricks_enhanced_security_monitoring_setting Resource:
        subCategory: Settings
        description: '""subcategory: "Settings"'
        name: databricks_enhanced_security_monitoring_setting Resource
        title: databricks_enhanced_security_monitoring_setting Resource
        argumentDocs:
            enhanced_security_monitoring_workspace: 'block with following attributes:'
            is_enabled: '- (Required) Enable the Enhanced Security Monitoring on the workspace'
        importStatements: []
    databricks_entitlements:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_entitlements
        title: databricks_entitlements Resource
        examples:
            - name: me
              manifest: |-
                {
                  "allow_cluster_create": true,
                  "allow_instance_pool_create": true,
                  "user_id": "${data.databricks_user.me.id}"
                }
              references:
                user_id: data.databricks_user.me.id
            - name: this
              manifest: |-
                {
                  "allow_cluster_create": true,
                  "allow_instance_pool_create": true,
                  "service_principal_id": "${data.databricks_service_principal.this.sp_id}"
                }
              references:
                service_principal_id: data.databricks_service_principal.this.sp_id
            - name: workspace-users
              manifest: |-
                {
                  "allow_cluster_create": true,
                  "allow_instance_pool_create": true,
                  "group_id": "${data.databricks_group.users.id}"
                }
              references:
                group_id: data.databricks_group.users.id
        argumentDocs:
            allow_cluster_create: '-  (Optional) Allow the principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and cluster_id argument. Everyone without allow_cluster_create argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.'
            allow_instance_pool_create: '-  (Optional) Allow the principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and instance_pool_id argument.'
            databricks_sql_access: '- (Optional) This is a field to allow the principal to have access to Databricks SQL feature in User Interface and through databricks_sql_endpoint.'
            group/group_id: '- group group_id.'
            group_id: '- Canonical unique identifier for the group.'
            service_principal_id: '- Canonical unique identifier for the service principal.'
            spn/spn_id: '- service principal spn_id.'
            user/user_id: '- user user_id.'
            user_id: '-  Canonical unique identifier for the user.'
            workspace_access: '- (Optional) This is a field to allow the principal to have access to Databricks Workspace.'
        importStatements: []
    databricks_external_location:
        subCategory: Unity Catalog
        description: '""subcategory: "Unity Catalog"'
        name: databricks_external_location
        title: databricks_external_location Resource
        examples:
            - name: some
              manifest: |-
                {
                  "comment": "Managed by TF",
                  "credential_name": "${databricks_storage_credential.external.id}",
                  "name": "external",
                  "url": "s3://${aws_s3_bucket.external.id}/some"
                }
              references:
                credential_name: databricks_storage_credential.external.id
              dependencies:
                databricks_grants.some: |-
                    {
                      "external_location": "${databricks_external_location.some.id}",
                      "grant": [
                        {
                          "principal": "Data Engineers",
                          "privileges": [
                            "CREATE_EXTERNAL_TABLE",
                            "READ_FILES"
                          ]
                        }
                      ]
                    }
                databricks_storage_credential.external: |-
                    {
                      "aws_iam_role": [
                        {
                          "role_arn": "${aws_iam_role.external_data_access.arn}"
                        }
                      ],
                      "comment": "Managed by TF",
                      "name": "${aws_iam_role.external_data_access.name}"
                    }
            - name: some
              manifest: |-
                {
                  "comment": "Managed by TF",
                  "credential_name": "${databricks_storage_credential.external.id}",
                  "depends_on": [
                    "${databricks_metastore_assignment.this}"
                  ],
                  "name": "external",
                  "url": "${format(\"abfss://%s@%s.dfs.core.windows.net\",\n    azurerm_storage_container.ext_storage.name,\n  azurerm_storage_account.ext_storage.name)}"
                }
              references:
                credential_name: databricks_storage_credential.external.id
              dependencies:
                databricks_grants.some: |-
                    {
                      "external_location": "${databricks_external_location.some.id}",
                      "grant": [
                        {
                          "principal": "Data Engineers",
                          "privileges": [
                            "CREATE_EXTERNAL_TABLE",
                            "READ_FILES"
                          ]
                        }
                      ]
                    }
                databricks_storage_credential.external: |-
                    {
                      "azure_service_principal": [
                        {
                          "application_id": "${azuread_application.ext_cred.application_id}",
                          "client_secret": "${azuread_application_password.ext_cred.value}",
                          "directory_id": "${var.tenant_id}"
                        }
                      ],
                      "comment": "Managed by TF",
                      "depends_on": [
                        "${databricks_metastore_assignment.this}"
                      ],
                      "name": "${azuread_application.ext_cred.display_name}"
                    }
            - name: some
              manifest: |-
                {
                  "comment": "Managed by TF",
                  "credential_name": "${databricks_storage_credential.ext.id}",
                  "name": "the-ext-location",
                  "url": "gs://${google_storage_bucket.ext_bucket.name}"
                }
              references:
                credential_name: databricks_storage_credential.ext.id
              dependencies:
                databricks_storage_credential.ext: |-
                    {
                      "databricks_gcp_service_account": [
                        {}
                      ],
                      "name": "the-creds"
                    }
        argumentDocs:
            access_point: '- (Optional) The ARN of the s3 access point to use with the external location (AWS).'
            comment: '- (Optional) User-supplied free-form text.'
            credential_name: '- Name of the databricks_storage_credential to use with this external location.'
            databricks_external_location: are objects that combine a cloud storage path with a Storage Credential that can be used to access the location.
            encryption_details: '- (Optional) The options for Server-Side Encryption to be used by each Databricks s3 client when connecting to S3 cloud storage (AWS).'
            force_destroy: '- (Optional) Destroy external location regardless of its dependents.'
            force_update: '- (Optional) Update external location regardless of its dependents.'
            id: '- ID of this external location - same as name.'
            isolation_mode: '- (Optional) Whether the external location is accessible from all workspaces or a specific set of workspaces. Can be ISOLATION_MODE_ISOLATED or ISOLATION_MODE_OPEN. Setting the external location to ISOLATION_MODE_ISOLATED will automatically allow access from the current workspace.'
            name: '- Name of External Location, which must be unique within the databricks_metastore. Change forces creation of a new resource.'
            owner: '- (Optional) Username/groupname/sp application_id of the external location owner.'
            read_only: '- (Optional) Indicates whether the external location is read-only.'
            skip_validation: '- (Optional) Suppress validation errors if any & force save the external location'
            url: '- Path URL in cloud storage, of the form: s3://[bucket-host]/[bucket-dir] (AWS), abfss://[user]@[host]/[path] (Azure), gs://[bucket-host]/[bucket-dir] (GCP).'
        importStatements: []
    databricks_file:
        subCategory: Storage
        description: '""subcategory: "Storage"'
        name: databricks_file
        title: databricks_file Resource
        examples:
            - name: this
              manifest: |-
                {
                  "path": "${databricks_volume.this.volume_path}/fileName",
                  "source": "/full/path/on/local/system"
                }
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "metastore_id": "${databricks_metastore.this.id}",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.name}",
                      "comment": "this schema is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
                databricks_volume.this: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.name}",
                      "comment": "this volume is managed by terraform",
                      "name": "quickstart_volume",
                      "schema_name": "${databricks_schema.things.name}",
                      "volume_type": "MANAGED"
                    }
            - name: init_script
              manifest: |-
                {
                  "content_base64": "${base64encode(\u003c\u003c-EOT\n    #!/bin/bash\n    echo \"Hello World\"\n    EOT\n  )}",
                  "path": "${databricks_volume.this.volume_path}/fileName"
                }
        argumentDocs:
            content_base64: '- Contents in base 64 format. Conflicts with source.'
            file_size: '- The file size of the file that is being tracked by this resource in bytes.'
            id: '- Same as path.'
            modification_time: '- The last time stamp when the file was modified'
            path: '- The path of the file in which you wish to save. For example, /Volumes/main/default/volume1/file.txt.'
            source: '- The full absolute path to the file. Conflicts with content_base64.'
        importStatements: []
    databricks_git_credential:
        subCategory: Workspace
        description: '""subcategory: "Workspace"'
        name: databricks_git_credential
        title: databricks_git_credential Resource
        examples:
            - name: ado
              manifest: |-
                {
                  "git_provider": "azureDevOpsServices",
                  "git_username": "myuser",
                  "personal_access_token": "sometoken"
                }
        argumentDocs:
            force: '- (Optional) specify if settings need to be enforced - right now, Databricks allows only single Git credential, so if it''s already configured, the apply operation will fail.'
            git_provider: '-  (Required) case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult Git Credentials API documentation): gitHub, gitHubEnterprise, bitbucketCloud, bitbucketServer, azureDevOpsServices, gitLab, gitLabEnterpriseEdition, awsCodeCommit.'
            git_username: '- (Required) user name at Git provider.'
            id: '- identifier of specific Git credential'
            personal_access_token: '- (Required) The personal access token used to authenticate to the corresponding Git provider. If value is not provided, it''s sourced from the first environment variable of GITHUB_TOKEN, GITLAB_TOKEN, or AZDO_PERSONAL_ACCESS_TOKEN, that has a non-empty value.'
        importStatements: []
    databricks_global_init_script:
        subCategory: Workspace
        description: '""subcategory: "Workspace"'
        name: databricks_global_init_script
        title: databricks_global_init_script Resource
        examples:
            - name: init1
              manifest: |-
                {
                  "name": "my init script",
                  "source": "${path.module}/init.sh"
                }
            - name: init2
              manifest: |-
                {
                  "content_base64": "${base64encode(\u003c\u003c-EOT\n    #!/bin/bash\n    echo \"hello world\"\n    EOT\n  )}",
                  "name": "hello script"
                }
        argumentDocs:
            content_base64: '- The base64-encoded source code global init script. Conflicts with source. Use of content_base64 is discouraged, as it''s increasing memory footprint of Terraform state and should only be used in exceptional circumstances'
            dbfs:/mnt/name: .
            enabled: '(bool, optional default: false) specifies if the script is enabled for execution, or not'
            id: '- ID assigned to a global init script by API'
            name: (string, required) - the name of the script.  It should be unique
            position: '(integer, optional default: null) - the position of a global init script, where 0 represents the first global init script to run, 1 is the second global init script to run, and so on. When omitted, the script gets the last position.'
            source: '- Path to script''s source code on local filesystem. Conflicts with content_base64'
        importStatements: []
    databricks_grant:
        subCategory: Unity Catalog
        description: '""subcategory: "Unity Catalog"'
        name: databricks_grant
        title: databricks_grant Resource
        examples:
            - name: sandbox_data_engineers
              manifest: |-
                {
                  "metastore": "metastore_id",
                  "principal": "Data Engineers",
                  "privileges": [
                    "CREATE_CATALOG",
                    "CREATE_EXTERNAL_LOCATION"
                  ]
                }
            - name: sandbox_data_sharer
              manifest: |-
                {
                  "metastore": "metastore_id",
                  "principal": "Data Sharer",
                  "privileges": [
                    "CREATE_RECIPIENT",
                    "CREATE_SHARE"
                  ]
                }
            - name: sandbox_data_scientists
              manifest: |-
                {
                  "catalog": "${databricks_catalog.sandbox.name}",
                  "principal": "Data Scientists",
                  "privileges": [
                    "USE_CATALOG",
                    "USE_SCHEMA",
                    "CREATE_TABLE",
                    "SELECT"
                  ]
                }
              references:
                catalog: databricks_catalog.sandbox.name
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
            - name: sandbox_data_engineers
              manifest: |-
                {
                  "catalog": "${databricks_catalog.sandbox.name}",
                  "principal": "Data Engineers",
                  "privileges": [
                    "USE_CATALOG",
                    "USE_SCHEMA",
                    "CREATE_SCHEMA",
                    "CREATE_TABLE",
                    "MODIFY"
                  ]
                }
              references:
                catalog: databricks_catalog.sandbox.name
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
            - name: sandbox_data_analyst
              manifest: |-
                {
                  "catalog": "${databricks_catalog.sandbox.name}",
                  "principal": "Data Analyst",
                  "privileges": [
                    "USE_CATALOG",
                    "USE_SCHEMA",
                    "SELECT"
                  ]
                }
              references:
                catalog: databricks_catalog.sandbox.name
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
            - name: things
              manifest: |-
                {
                  "principal": "Data Engineers",
                  "privileges": [
                    "USE_SCHEMA",
                    "MODIFY"
                  ],
                  "schema": "${databricks_schema.things.id}"
                }
              references:
                schema: databricks_schema.things.id
              dependencies:
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.id}",
                      "comment": "this schema is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
            - name: customers_data_engineers
              manifest: |-
                {
                  "principal": "Data Engineers",
                  "privileges": [
                    "MODIFY",
                    "SELECT"
                  ],
                  "table": "main.reporting.customers"
                }
            - name: customers_data_analysts
              manifest: |-
                {
                  "principal": "Data Analysts",
                  "privileges": [
                    "SELECT"
                  ],
                  "table": "main.reporting.customers"
                }
            - name: things
              manifest: |-
                {
                  "for_each": "${data.databricks_tables.things.ids}",
                  "principal": "sensitive",
                  "privileges": [
                    "SELECT",
                    "MODIFY"
                  ],
                  "table": "${each.value}"
                }
              references:
                for_each: data.databricks_tables.things.ids
                table: each.value
            - name: customer360
              manifest: |-
                {
                  "principal": "Data Analysts",
                  "privileges": [
                    "SELECT"
                  ],
                  "table": "main.reporting.customer360"
                }
            - name: customers
              manifest: |-
                {
                  "for_each": "${data.databricks_views.customers.ids}",
                  "principal": "sensitive",
                  "privileges": [
                    "SELECT",
                    "MODIFY"
                  ],
                  "table": "${each.value}"
                }
              references:
                for_each: data.databricks_views.customers.ids
                table: each.value
            - name: volume
              manifest: |-
                {
                  "principal": "Data Engineers",
                  "privileges": [
                    "WRITE_VOLUME"
                  ],
                  "volume": "${databricks_volume.this.id}"
                }
              references:
                volume: databricks_volume.this.id
              dependencies:
                databricks_volume.this: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.name}",
                      "comment": "this volume is managed by terraform",
                      "name": "quickstart_volume",
                      "schema_name": "${databricks_schema.things.name}",
                      "storage_location": "${databricks_external_location.some.url}",
                      "volume_type": "EXTERNAL"
                    }
            - name: customers_data_engineers
              manifest: |-
                {
                  "model": "main.reporting.customer_model",
                  "principal": "Data Engineers",
                  "privileges": [
                    "APPLY_TAG",
                    "EXECUTE"
                  ]
                }
            - name: customers_data_analysts
              manifest: |-
                {
                  "model": "main.reporting.customer_model",
                  "principal": "Data Analysts",
                  "privileges": [
                    "EXECUTE"
                  ]
                }
            - name: udf_data_engineers
              manifest: |-
                {
                  "function": "main.reporting.udf",
                  "principal": "Data Engineers",
                  "privileges": [
                    "EXECUTE"
                  ]
                }
            - name: udf_data_analysts
              manifest: |-
                {
                  "function": "main.reporting.udf",
                  "principal": "Data Analysts",
                  "privileges": [
                    "EXECUTE"
                  ]
                }
            - name: external_creds
              manifest: |-
                {
                  "principal": "Data Engineers",
                  "privileges": [
                    "CREATE_EXTERNAL_TABLE"
                  ],
                  "storage_credential": "${databricks_storage_credential.external.id}"
                }
              references:
                storage_credential: databricks_storage_credential.external.id
              dependencies:
                databricks_storage_credential.external: |-
                    {
                      "aws_iam_role": [
                        {
                          "role_arn": "${aws_iam_role.external_data_access.arn}"
                        }
                      ],
                      "comment": "Managed by TF",
                      "name": "${aws_iam_role.external_data_access.name}"
                    }
            - name: some_data_engineers
              manifest: |-
                {
                  "external_location": "${databricks_external_location.some.id}",
                  "principal": "Data Engineers",
                  "privileges": [
                    "CREATE_EXTERNAL_TABLE",
                    "READ_FILES"
                  ]
                }
              references:
                external_location: databricks_external_location.some.id
              dependencies:
                databricks_external_location.some: |-
                    {
                      "comment": "Managed by TF",
                      "credential_name": "${databricks_storage_credential.external.id}",
                      "name": "external",
                      "url": "s3://${aws_s3_bucket.external.id}/some"
                    }
            - name: some_service_principal
              manifest: |-
                {
                  "external_location": "${databricks_external_location.some.id}",
                  "principal": "${databricks_service_principal.my_sp.application_id}",
                  "privileges": [
                    "USE_SCHEMA",
                    "MODIFY"
                  ]
                }
              references:
                external_location: databricks_external_location.some.id
                principal: databricks_service_principal.my_sp.application_id
              dependencies:
                databricks_external_location.some: |-
                    {
                      "comment": "Managed by TF",
                      "credential_name": "${databricks_storage_credential.external.id}",
                      "name": "external",
                      "url": "s3://${aws_s3_bucket.external.id}/some"
                    }
            - name: some_group
              manifest: |-
                {
                  "external_location": "${databricks_external_location.some.id}",
                  "principal": "${databricks_group.my_group.display_name}",
                  "privileges": [
                    "USE_SCHEMA",
                    "MODIFY"
                  ]
                }
              references:
                external_location: databricks_external_location.some.id
                principal: databricks_group.my_group.display_name
              dependencies:
                databricks_external_location.some: |-
                    {
                      "comment": "Managed by TF",
                      "credential_name": "${databricks_storage_credential.external.id}",
                      "name": "external",
                      "url": "s3://${aws_s3_bucket.external.id}/some"
                    }
            - name: some_user
              manifest: |-
                {
                  "external_location": "${databricks_external_location.some.id}",
                  "principal": "${databricks_group.my_user.user_name}",
                  "privileges": [
                    "USE_SCHEMA",
                    "MODIFY"
                  ]
                }
              references:
                external_location: databricks_external_location.some.id
                principal: databricks_group.my_user.user_name
              dependencies:
                databricks_external_location.some: |-
                    {
                      "comment": "Managed by TF",
                      "credential_name": "${databricks_storage_credential.external.id}",
                      "name": "external",
                      "url": "s3://${aws_s3_bucket.external.id}/some"
                    }
            - name: some
              manifest: |-
                {
                  "foreign_connection": "${databricks_connection.mysql.name}",
                  "principal": "Data Engineers",
                  "privileges": [
                    "CREATE_FOREIGN_CATALOG",
                    "USE_CONNECTION"
                  ]
                }
              references:
                foreign_connection: databricks_connection.mysql.name
              dependencies:
                databricks_connection.mysql: |-
                    {
                      "comment": "this is a connection to mysql db",
                      "connection_type": "MYSQL",
                      "name": "mysql_connection",
                      "options": {
                        "host": "test.mysql.database.azure.com",
                        "password": "password",
                        "port": "3306",
                        "user": "user"
                      },
                      "properties": {
                        "purpose": "testing"
                      }
                    }
            - name: some
              manifest: |-
                {
                  "principal": "${databricks_recipient.some.name}",
                  "privileges": [
                    "SELECT"
                  ],
                  "share": "${databricks_share.some.name}"
                }
              references:
                principal: databricks_recipient.some.name
                share: databricks_share.some.name
              dependencies:
                databricks_recipient.some: |-
                    {
                      "name": "my_recipient"
                    }
                databricks_share.some: |-
                    {
                      "name": "my_share"
                    }
        argumentDocs:
            principal: '- User name, group name or service principal application ID.'
            privileges: '- One or more privileges that are specific to a securable type.'
        importStatements: []
    databricks_grants:
        subCategory: Unity Catalog
        description: '""subcategory: "Unity Catalog"'
        name: databricks_grants
        title: databricks_grants Resource
        examples:
            - name: sandbox
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "CREATE_CATALOG",
                        "CREATE_EXTERNAL_LOCATION"
                      ]
                    },
                    {
                      "principal": "Data Sharer",
                      "privileges": [
                        "CREATE_RECIPIENT",
                        "CREATE_SHARE"
                      ]
                    }
                  ],
                  "metastore": "metastore_id"
                }
            - name: sandbox
              manifest: |-
                {
                  "catalog": "${databricks_catalog.sandbox.name}",
                  "grant": [
                    {
                      "principal": "Data Scientists",
                      "privileges": [
                        "USE_CATALOG",
                        "USE_SCHEMA",
                        "CREATE_TABLE",
                        "SELECT"
                      ]
                    },
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "USE_CATALOG",
                        "USE_SCHEMA",
                        "CREATE_SCHEMA",
                        "CREATE_TABLE",
                        "MODIFY"
                      ]
                    },
                    {
                      "principal": "Data Analyst",
                      "privileges": [
                        "USE_CATALOG",
                        "USE_SCHEMA",
                        "SELECT"
                      ]
                    }
                  ]
                }
              references:
                catalog: databricks_catalog.sandbox.name
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
            - name: things
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "USE_SCHEMA",
                        "MODIFY"
                      ]
                    }
                  ],
                  "schema": "${databricks_schema.things.id}"
                }
              references:
                schema: databricks_schema.things.id
              dependencies:
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.id}",
                      "comment": "this schema is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
            - name: customers
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "MODIFY",
                        "SELECT"
                      ]
                    },
                    {
                      "principal": "Data Analysts",
                      "privileges": [
                        "SELECT"
                      ]
                    }
                  ],
                  "table": "main.reporting.customers"
                }
            - name: things
              manifest: |-
                {
                  "for_each": "${data.databricks_tables.things.ids}",
                  "grant": [
                    {
                      "principal": "sensitive",
                      "privileges": [
                        "SELECT",
                        "MODIFY"
                      ]
                    }
                  ],
                  "table": "${each.value}"
                }
              references:
                for_each: data.databricks_tables.things.ids
                table: each.value
            - name: customer360
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "Data Analysts",
                      "privileges": [
                        "SELECT"
                      ]
                    }
                  ],
                  "table": "main.reporting.customer360"
                }
            - name: customers
              manifest: |-
                {
                  "for_each": "${data.databricks_views.customers.ids}",
                  "grant": [
                    {
                      "principal": "sensitive",
                      "privileges": [
                        "SELECT",
                        "MODIFY"
                      ]
                    }
                  ],
                  "table": "${each.value}"
                }
              references:
                for_each: data.databricks_views.customers.ids
                table: each.value
            - name: volume
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "WRITE_VOLUME"
                      ]
                    }
                  ],
                  "volume": "${databricks_volume.this.id}"
                }
              references:
                volume: databricks_volume.this.id
              dependencies:
                databricks_volume.this: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.name}",
                      "comment": "this volume is managed by terraform",
                      "name": "quickstart_volume",
                      "schema_name": "${databricks_schema.things.name}",
                      "storage_location": "${databricks_external_location.some.url}",
                      "volume_type": "EXTERNAL"
                    }
            - name: customers
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "APPLY_TAG",
                        "EXECUTE"
                      ]
                    },
                    {
                      "principal": "Data Analysts",
                      "privileges": [
                        "EXECUTE"
                      ]
                    }
                  ],
                  "model": "main.reporting.customer_model"
                }
            - name: udf
              manifest: |-
                {
                  "function": "main.reporting.udf",
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "EXECUTE"
                      ]
                    },
                    {
                      "principal": "Data Analysts",
                      "privileges": [
                        "EXECUTE"
                      ]
                    }
                  ]
                }
            - name: external_creds
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "CREATE_EXTERNAL_TABLE"
                      ]
                    }
                  ],
                  "storage_credential": "${databricks_storage_credential.external.id}"
                }
              references:
                storage_credential: databricks_storage_credential.external.id
              dependencies:
                databricks_storage_credential.external: |-
                    {
                      "aws_iam_role": [
                        {
                          "role_arn": "${aws_iam_role.external_data_access.arn}"
                        }
                      ],
                      "comment": "Managed by TF",
                      "name": "${aws_iam_role.external_data_access.name}"
                    }
            - name: some
              manifest: |-
                {
                  "external_location": "${databricks_external_location.some.id}",
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "CREATE_EXTERNAL_TABLE",
                        "READ_FILES"
                      ]
                    },
                    {
                      "principal": "${databricks_service_principal.my_sp.application_id}",
                      "privileges": [
                        "CREATE_EXTERNAL_TABLE",
                        "READ_FILES"
                      ]
                    },
                    {
                      "principal": "${databricks_group.my_group.display_name}",
                      "privileges": [
                        "CREATE_EXTERNAL_TABLE",
                        "READ_FILES"
                      ]
                    },
                    {
                      "principal": "${databricks_group.my_user.user_name}",
                      "privileges": [
                        "CREATE_EXTERNAL_TABLE",
                        "READ_FILES"
                      ]
                    }
                  ]
                }
              references:
                external_location: databricks_external_location.some.id
                grant.principal: databricks_group.my_user.user_name
              dependencies:
                databricks_external_location.some: |-
                    {
                      "comment": "Managed by TF",
                      "credential_name": "${databricks_storage_credential.external.id}",
                      "name": "external",
                      "url": "s3://${aws_s3_bucket.external.id}/some"
                    }
            - name: some
              manifest: |-
                {
                  "foreign_connection": "${databricks_connection.mysql.name}",
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "CREATE_FOREIGN_CATALOG",
                        "USE_CONNECTION"
                      ]
                    }
                  ]
                }
              references:
                foreign_connection: databricks_connection.mysql.name
              dependencies:
                databricks_connection.mysql: |-
                    {
                      "comment": "this is a connection to mysql db",
                      "connection_type": "MYSQL",
                      "name": "mysql_connection",
                      "options": {
                        "host": "test.mysql.database.azure.com",
                        "password": "password",
                        "port": "3306",
                        "user": "user"
                      },
                      "properties": {
                        "purpose": "testing"
                      }
                    }
            - name: some
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "${databricks_recipient.some.name}",
                      "privileges": [
                        "SELECT"
                      ]
                    }
                  ],
                  "share": "${databricks_share.some.name}"
                }
              references:
                grant.principal: databricks_recipient.some.name
                share: databricks_share.some.name
              dependencies:
                databricks_recipient.some: |-
                    {
                      "name": "my_recipient"
                    }
                databricks_share.some: |-
                    {
                      "name": "my_share"
                    }
        argumentDocs:
            databricks_grants.principal: '- User name, group name or service principal application ID.'
            databricks_grants.privileges: '- One or more privileges that are specific to a securable type.'
        importStatements: []
    databricks_group:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_group
        title: databricks_group Resource
        examples:
            - name: this
              manifest: |-
                {
                  "allow_cluster_create": true,
                  "allow_instance_pool_create": true,
                  "display_name": "Some Group"
                }
            - name: this
              manifest: |-
                {
                  "allow_cluster_create": true,
                  "allow_instance_pool_create": true,
                  "display_name": "Some Group"
                }
              dependencies:
                databricks_group_member.vip_member: |-
                    {
                      "group_id": "${databricks_group.this.id}",
                      "member_id": "${databricks_user.this.id}"
                    }
                databricks_user.this: |-
                    {
                      "user_name": "someone@example.com"
                    }
            - name: this
              manifest: |-
                {
                  "display_name": "Some Group",
                  "provider": "${databricks.mws}"
                }
              references:
                provider: databricks.mws
            - name: this
              manifest: |-
                {
                  "display_name": "Some Group",
                  "provider": "${databricks.azure_account}"
                }
              references:
                provider: databricks.azure_account
        argumentDocs:
            acl_principal_id: '- identifier for use in databricks_access_control_rule_set, e.g. groups/Some Group.'
            allow_cluster_create: '-  (Optional) This is a field to allow the group to have cluster create privileges. More fine grained permissions could be assigned with databricks_permissions and cluster_id argument. Everyone without allow_cluster_create argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.'
            allow_instance_pool_create: '-  (Optional) This is a field to allow the group to have instance pool create privileges. More fine grained permissions could be assigned with databricks_permissions and instance_pool_id argument.'
            databricks_sql_access: '- (Optional) This is a field to allow the group to have access to Databricks SQL feature in User Interface and through databricks_sql_endpoint.'
            display_name: '-  (Required) This is the display name for the given group.'
            external_id: '- (Optional) ID of the group in an external identity provider.'
            force: '- (Optional) Ignore cannot create group: Group with name X already exists. errors and implicitly import the specific group into Terraform state, enforcing entitlements defined in the instance of resource. This functionality is experimental and is designed to simplify corner cases, like Azure Active Directory synchronisation.'
            id: '-  The id for the group object.'
            workspace_access: '- (Optional) This is a field to allow the group to have access to Databricks Workspace.'
        importStatements: []
    databricks_group_instance_profile:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_group_instance_profile
        title: databricks_group_instance_profile Resource
        examples:
            - name: my_group_instance_profile
              manifest: |-
                {
                  "group_id": "${databricks_group.my_group.id}",
                  "instance_profile_id": "${databricks_instance_profile.instance_profile.id}"
                }
              references:
                group_id: databricks_group.my_group.id
                instance_profile_id: databricks_instance_profile.instance_profile.id
              dependencies:
                databricks_group.my_group: |-
                    {
                      "display_name": "my_group_name"
                    }
                databricks_instance_profile.instance_profile: |-
                    {
                      "instance_profile_arn": "my_instance_profile_arn"
                    }
        argumentDocs:
            group_id: '- (Required) This is the id of the group resource.'
            id: '- The id in the format <group_id>|<instance_profile_id>.'
            instance_profile_id: '-  (Required) This is the id of the instance profile resource.'
        importStatements: []
    databricks_group_member:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_group_member
        title: databricks_group_member Resource
        examples:
            - name: ab
              manifest: |-
                {
                  "group_id": "${databricks_group.a.id}",
                  "member_id": "${databricks_group.b.id}"
                }
              references:
                group_id: databricks_group.a.id
                member_id: databricks_group.b.id
              dependencies:
                databricks_group.a: |-
                    {
                      "display_name": "A"
                    }
                databricks_group.b: |-
                    {
                      "display_name": "B"
                    }
                databricks_user.bradley: |-
                    {
                      "user_name": "bradley@example.com"
                    }
            - name: bb
              manifest: |-
                {
                  "group_id": "${databricks_group.b.id}",
                  "member_id": "${databricks_user.bradley.id}"
                }
              references:
                group_id: databricks_group.b.id
                member_id: databricks_user.bradley.id
              dependencies:
                databricks_group.a: |-
                    {
                      "display_name": "A"
                    }
                databricks_group.b: |-
                    {
                      "display_name": "B"
                    }
                databricks_user.bradley: |-
                    {
                      "user_name": "bradley@example.com"
                    }
        argumentDocs:
            group_id: '- (Required) This is the id of the group resource.'
            id: '- The id for the databricks_group_member object which is in the format <group_id>|<member_id>.'
            member_id: '- (Required) This is the id of the group, service principal, or user.'
        importStatements: []
    databricks_group_role:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_group_role
        title: databricks_group_role Resource
        examples:
            - name: my_group_instance_profile
              manifest: |-
                {
                  "group_id": "${databricks_group.my_group.id}",
                  "role": "${databricks_instance_profile.instance_profile.id}"
                }
              references:
                group_id: databricks_group.my_group.id
                role: databricks_instance_profile.instance_profile.id
              dependencies:
                databricks_group.my_group: |-
                    {
                      "display_name": "my_group_name"
                    }
                databricks_instance_profile.instance_profile: |-
                    {
                      "instance_profile_arn": "my_instance_profile_arn"
                    }
            - name: my_group_account_admin
              manifest: |-
                {
                  "group_id": "${databricks_group.my_group.id}",
                  "role": "account_admin"
                }
              references:
                group_id: databricks_group.my_group.id
              dependencies:
                databricks_group.my_group: |-
                    {
                      "display_name": "my_group_name"
                    }
        argumentDocs:
            group_id: '- (Required) This is the id of the group resource.'
            id: '- The id for the databricks_group_role object which is in the format <group_id>|<role>.'
            role: '- (Required) Either a role name or the ARN/ID of the instance profile resource.'
        importStatements: []
    databricks_instance_pool:
        subCategory: Compute
        description: '""subcategory: "Compute"'
        name: databricks_instance_pool
        title: databricks_instance_pool Resource
        examples:
            - name: smallest_nodes
              manifest: |-
                {
                  "aws_attributes": [
                    {
                      "availability": "ON_DEMAND",
                      "spot_bid_price_percent": "100",
                      "zone_id": "us-east-1a"
                    }
                  ],
                  "disk_spec": [
                    {
                      "disk_count": 1,
                      "disk_size": 80,
                      "disk_type": [
                        {
                          "ebs_volume_type": "GENERAL_PURPOSE_SSD"
                        }
                      ]
                    }
                  ],
                  "idle_instance_autotermination_minutes": 10,
                  "instance_pool_name": "Smallest Nodes",
                  "max_capacity": 300,
                  "min_idle_instances": 0,
                  "node_type_id": "${data.databricks_node_type.smallest.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
            - name: this
              manifest: |-
                {
                  "preloaded_docker_image": [
                    {
                      "basic_auth": [
                        {
                          "password": "${azurerm_container_registry.this.admin_password}",
                          "username": "${azurerm_container_registry.this.admin_username}"
                        }
                      ],
                      "url": "${docker_registry_image.this.name}"
                    }
                  ]
                }
              references:
                preloaded_docker_image.basic_auth.password: azurerm_container_registry.this.admin_password
                preloaded_docker_image.basic_auth.username: azurerm_container_registry.this.admin_username
                preloaded_docker_image.url: docker_registry_image.this.name
              dependencies:
                docker_registry_image.this: |-
                    {
                      "build": [
                        {}
                      ],
                      "name": "${azurerm_container_registry.this.login_server}/sample:latest"
                    }
        argumentDocs:
            1 - 1023: GiB
            1- 1023: GiB
            100 - 4096: GiB
            500 - 4096: GiB
            availability: '- (Optional) (String) Availability type used for all instances in the pool. Only ON_DEMAND and SPOT are supported.'
            azure_attributes.availability: '- (Optional) Availability type used for all nodes. Valid values are SPOT_AZURE and ON_DEMAND_AZURE.'
            azure_attributes.spot_bid_max_price: '- (Optional) The max bid price used for Azure spot instances. You can set this to greater than or equal to the current spot price. You can also set this to -1, which specifies that the instance cannot be evicted on the basis of price. The price for the instance will be the current price for spot instances or the price for a standard instance.'
            custom_tags: '- (Optional) (Map) Additional tags for instance pool resources. Databricks tags all pool resources (e.g. AWS & Azure instances and Disk volumes). The tags of the instance pool will propagate to the clusters using the pool (see the official documentation). Attempting to set the same tags in both cluster and instance pool will raise an error. Databricks allows at most 43 custom tags.'
            disk_count: '- (Optional) (Integer) The number of disks to attach to each instance. This feature is only enabled for supported node types. Users can choose up to the limit of the disks supported by the node type. For node types with no local disk, at least one disk needs to be specified.'
            disk_size: '- (Optional) (Integer) The size of each disk (in GiB) to attach.'
            enable_elastic_disk: '- (Optional) (Bool) Autoscaling Local Storage: when enabled, the instances in the pool dynamically acquire additional disk space when they are running low on disk space.'
            gcp_attributes.gcp_availability: '- (Optional) Availability type used for all nodes. Valid values are PREEMPTIBLE_GCP, PREEMPTIBLE_WITH_FALLBACK_GCP and ON_DEMAND_GCP, default: ON_DEMAND_GCP.'
            gcp_attributes.local_ssd_count: (Optional, Int) Number of local SSD disks (each is 375GB in size) that will be attached to each node of the cluster.
            gcp_attributes.zone_id: '- (Optional) Identifier for the availability zone/datacenter in which the cluster resides. This string will be of a form like us-central1-a. The provided availability zone must be in the same region as the Databricks workspace.'
            id: '- Canonical unique identifier for the instance pool.'
            idle_instance_autotermination_minutes: '- (Required) (Integer) The number of minutes that idle instances in excess of the min_idle_instances are maintained by the pool before being terminated. If not specified, excess idle instances are terminated automatically after a default timeout period. If specified, the time must be between 0 and 10000 minutes. If you specify 0, excess idle instances are removed as soon as possible.'
            instance_pool_name: '- (Required) (String) The name of the instance pool. This is required for create and edit operations. It must be unique, non-empty, and less than 100 characters.'
            max_capacity: '- (Optional) (Integer) The maximum number of instances the pool can contain, including both idle instances and ones in use by clusters. Once the maximum capacity is reached, you cannot create new clusters from the pool and existing clusters cannot autoscale up until some instances are made idle in the pool via cluster termination or down-scaling. There is no default limit, but as a best practice, this should be set based on anticipated usage.'
            min_idle_instances: '- (Optional) (Integer) The minimum number of idle instances maintained by the pool. This is in addition to any instances in use by active clusters.'
            node_type_id: '- (Required) (String) The node type for the instances in the pool. All clusters attached to the pool inherit this node type and the pool’s idle instances are allocated based on this type. You can retrieve a list of available node types by using the List Node Types API call.'
            preloaded_docker_image.basic_auth: '- (Optional) basic_auth.username and basic_auth.password for Docker repository. Docker registry credentials are encrypted when they are stored in Databricks internal storage and when they are passed to a registry upon fetching Docker images at cluster launch. However, other authenticated and authorized API users of this workspace can access the username and password.'
            preloaded_docker_image.url: '- URL for the Docker image'
            preloaded_spark_versions: '- (Optional) (List) A list with at most one runtime version the pool installs on each instance. Pool clusters that use a preloaded runtime version start faster as they do not have to wait for the image to download. You can retrieve them via databricks_spark_version data source or via  Runtime Versions API call.'
            spot_bid_price_percent: '- (Optional) (Integer) The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the instance pool needs a new i3.xlarge spot instance, then the max price is half of the price of on-demand i3.xlarge instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand i3.xlarge instances. If not specified, the default value is 100. When spot instances are requested for this instance pool, only spot instances whose max price percentage matches this field are considered. For safety, this field cannot be greater than 10000.'
            zone_id: '- (Optional) (String) Identifier for the availability zone/datacenter in which the instance pool resides. This string is of the form like "us-west-2a". The provided availability zone must be in the same region as the Databricks deployment. For example, "us-west-2a" is not a valid zone ID if the Databricks deployment resides in the "us-east-1" region. If not specified, a default zone is used. You can find the list of available zones as well as the default value by using the List Zones API.'
        importStatements: []
    databricks_instance_profile:
        subCategory: Deployment
        description: '""subcategory: "Deployment"'
        name: databricks_instance_profile
        title: databricks_instance_profile Resource
        examples:
            - name: shared
              manifest: |-
                {
                  "instance_profile_arn": "${aws_iam_instance_profile.shared.arn}"
                }
              references:
                instance_profile_arn: aws_iam_instance_profile.shared.arn
              dependencies:
                aws_iam_instance_profile.shared: |-
                    {
                      "name": "shared-instance-profile",
                      "role": "${aws_iam_role.role_for_s3_access.name}"
                    }
                aws_iam_policy.pass_role_for_s3_access: |-
                    {
                      "name": "shared-pass-role-for-s3-access",
                      "path": "/",
                      "policy": "${data.aws_iam_policy_document.pass_role_for_s3_access.json}"
                    }
                aws_iam_role.role_for_s3_access: |-
                    {
                      "assume_role_policy": "${data.aws_iam_policy_document.assume_role_for_ec2.json}",
                      "description": "Role for shared access",
                      "name": "shared-ec2-role-for-s3"
                    }
                aws_iam_role_policy_attachment.cross_account: |-
                    {
                      "policy_arn": "${aws_iam_policy.pass_role_for_s3_access.arn}",
                      "role": "${var.crossaccount_role_name}"
                    }
                databricks_cluster.this: |-
                    {
                      "autoscale": [
                        {
                          "max_workers": 50,
                          "min_workers": 1
                        }
                      ],
                      "autotermination_minutes": 20,
                      "aws_attributes": [
                        {
                          "availability": "SPOT",
                          "first_on_demand": 1,
                          "instance_profile_arn": "${databricks_instance_profile.shared.id}",
                          "spot_bid_price_percent": 100,
                          "zone_id": "us-east-1"
                        }
                      ],
                      "cluster_name": "Shared Autoscaling",
                      "node_type_id": "${data.databricks_node_type.smallest.id}",
                      "spark_version": "${data.databricks_spark_version.latest.id}"
                    }
            - name: this
              manifest: |-
                {
                  "instance_profile_arn": "${aws_iam_instance_profile.shared.arn}"
                }
              references:
                instance_profile_arn: aws_iam_instance_profile.shared.arn
              dependencies:
                databricks_group_instance_profile.all: |-
                    {
                      "group_id": "${data.databricks_group.users.id}",
                      "instance_profile_id": "${databricks_instance_profile.this.id}"
                    }
            - name: this
              manifest: |-
                {
                  "iam_role_arn": "${aws_iam_role.this.arn}",
                  "instance_profile_arn": "${aws_iam_instance_profile.this.arn}"
                }
              references:
                iam_role_arn: aws_iam_role.this.arn
                instance_profile_arn: aws_iam_instance_profile.this.arn
              dependencies:
                aws_iam_instance_profile.this: |-
                    {
                      "name": "my-databricks-sql-serverless-instance-profile",
                      "role": "${aws_iam_role.this.name}"
                    }
                aws_iam_role.this: |-
                    {
                      "assume_role_policy": "${data.aws_iam_policy_document.sql_serverless_assume_role.json}",
                      "name": "my-databricks-sql-serverless-role"
                    }
        argumentDocs:
            iam_role_arn: '- (Optional) The AWS IAM role ARN of the role associated with the instance profile. It must have the form arn:aws:iam::<account-id>:role/<name>. This field is required if your role name and instance profile name do not match and you want to use the instance profile with Databricks SQL Serverless.'
            id: '- ARN for EC2 Instance Profile, that is registered with Databricks.'
            instance_profile_arn: '- (Required) ARN attribute of aws_iam_instance_profile output, the EC2 instance profile association to AWS IAM role. This ARN would be validated upon resource creation.'
            is_meta_instance_profile: '- (Optional) Whether the instance profile is a meta instance profile. Used only in IAM credential passthrough.'
            skip_validation: '- (Optional) For advanced usage only. If validation fails with an error message that does not indicate an IAM related permission issue, (e.g. "Your requested instance type is not supported in your requested availability zone"), you can pass this flag to skip the validation and forcibly add the instance profile.'
        importStatements: []
    databricks_ip_access_list:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_ip_access_list
        title: databricks_ip_access_list Resource
        examples:
            - name: allowed-list
              manifest: |-
                {
                  "depends_on": [
                    "${databricks_workspace_conf.this}"
                  ],
                  "ip_addresses": [
                    "1.1.1.1",
                    "1.2.3.0/24",
                    "1.2.5.0/24"
                  ],
                  "label": "allow_in",
                  "list_type": "ALLOW"
                }
              dependencies:
                databricks_workspace_conf.this: |-
                    {
                      "custom_config": {
                        "enableIpAccessLists": true
                      }
                    }
        argumentDocs:
            enabled: '- (Optional) Boolean true or false indicating whether this list should be active.  Defaults to true'
            id: '- Canonical unique identifier for the IP Access List, same as list_id.'
            ip_addresses: '- A string list of IP addresses and CIDR ranges.'
            label: '-  This is the display name for the given IP ACL List.'
            list_id: '- Canonical unique identifier for the IP Access List.'
            list_type: '-  Can only be "ALLOW" or "BLOCK".'
        importStatements: []
    databricks_job:
        subCategory: Compute
        description: '""subcategory: "Compute"'
        name: databricks_job
        title: databricks_job Resource
        examples:
            - name: this
              manifest: |-
                {
                  "description": "This job executes multiple tasks on a shared job cluster, which will be provisioned as part of execution, and terminated once all tasks are finished.",
                  "job_cluster": [
                    {
                      "job_cluster_key": "j",
                      "new_cluster": [
                        {
                          "node_type_id": "${data.databricks_node_type.smallest.id}",
                          "num_workers": 2,
                          "spark_version": "${data.databricks_spark_version.latest.id}"
                        }
                      ]
                    }
                  ],
                  "name": "Job with multiple tasks",
                  "task": [
                    {
                      "new_cluster": [
                        {
                          "node_type_id": "${data.databricks_node_type.smallest.id}",
                          "num_workers": 1,
                          "spark_version": "${data.databricks_spark_version.latest.id}"
                        }
                      ],
                      "notebook_task": [
                        {
                          "notebook_path": "${databricks_notebook.this.path}"
                        }
                      ],
                      "task_key": "a"
                    },
                    {
                      "depends_on": [
                        {
                          "task_key": "a"
                        }
                      ],
                      "existing_cluster_id": "${databricks_cluster.shared.id}",
                      "spark_jar_task": [
                        {
                          "main_class_name": "com.acme.data.Main"
                        }
                      ],
                      "task_key": "b"
                    },
                    {
                      "job_cluster_key": "j",
                      "notebook_task": [
                        {
                          "notebook_path": "${databricks_notebook.this.path}"
                        }
                      ],
                      "task_key": "c"
                    },
                    {
                      "pipeline_task": [
                        {
                          "pipeline_id": "${databricks_pipeline.this.id}"
                        }
                      ],
                      "task_key": "d"
                    }
                  ]
                }
              references:
                job_cluster.new_cluster.node_type_id: data.databricks_node_type.smallest.id
                job_cluster.new_cluster.spark_version: data.databricks_spark_version.latest.id
                task.existing_cluster_id: databricks_cluster.shared.id
                task.new_cluster.node_type_id: data.databricks_node_type.smallest.id
                task.new_cluster.spark_version: data.databricks_spark_version.latest.id
                task.notebook_task.notebook_path: databricks_notebook.this.path
                task.pipeline_task.pipeline_id: databricks_pipeline.this.id
            - name: sql_aggregation_job
              manifest: |-
                {
                  "name": "Example SQL Job",
                  "task": [
                    {
                      "sql_task": [
                        {
                          "query": [
                            {
                              "query_id": "${databricks_sql_query.agg_query.id}"
                            }
                          ],
                          "warehouse_id": "${databricks_sql_endpoint.sql_job_warehouse.id}"
                        }
                      ],
                      "task_key": "run_agg_query"
                    },
                    {
                      "sql_task": [
                        {
                          "dashboard": [
                            {
                              "dashboard_id": "${databricks_sql_dashboard.dash.id}",
                              "subscriptions": [
                                {
                                  "user_name": "user@domain.com"
                                }
                              ]
                            }
                          ],
                          "warehouse_id": "${databricks_sql_endpoint.sql_job_warehouse.id}"
                        }
                      ],
                      "task_key": "run_dashboard"
                    },
                    {
                      "sql_task": [
                        {
                          "alert": [
                            {
                              "alert_id": "${databricks_sql_alert.alert.id}",
                              "subscriptions": [
                                {
                                  "user_name": "user@domain.com"
                                }
                              ]
                            }
                          ],
                          "warehouse_id": "${databricks_sql_endpoint.sql_job_warehouse.id}"
                        }
                      ],
                      "task_key": "run_alert"
                    }
                  ]
                }
              references:
                task.sql_task.alert.alert_id: databricks_sql_alert.alert.id
                task.sql_task.dashboard.dashboard_id: databricks_sql_dashboard.dash.id
                task.sql_task.query.query_id: databricks_sql_query.agg_query.id
                task.sql_task.warehouse_id: databricks_sql_endpoint.sql_job_warehouse.id
            - name: this
              manifest: |-
                {
                  "library": [
                    {
                      "pypi": [
                        {
                          "package": "databricks-mosaic==0.3.14"
                        }
                      ]
                    }
                  ]
                }
            - name: this
              manifest: |-
                {
                  "run_as": [
                    {
                      "service_principal_name": "8d23ae77-912e-4a19-81e4-b9c3f5cc9349"
                    }
                  ]
                }
            - name: this
              manifest: |-
                {
                  "tags": {
                    "environment": "dev",
                    "owner": "dream-team"
                  }
                }
            - name: this
              manifest: |-
                {
                  "name": "Terraform Demo (${data.databricks_current_user.me.alphanumeric})",
                  "new_cluster": [
                    {
                      "node_type_id": "${data.databricks_node_type.smallest.id}",
                      "num_workers": 1,
                      "spark_version": "${data.databricks_spark_version.latest.id}"
                    }
                  ],
                  "notebook_task": [
                    {
                      "notebook_path": "${databricks_notebook.this.path}"
                    }
                  ]
                }
              references:
                new_cluster.node_type_id: data.databricks_node_type.smallest.id
                new_cluster.spark_version: data.databricks_spark_version.latest.id
                notebook_task.notebook_path: databricks_notebook.this.path
              dependencies:
                databricks_notebook.this: |-
                    {
                      "content_base64": "${base64encode(\u003c\u003c-EOT\n    # created from ${abspath(path.module)}\n    display(spark.range(10))\n    EOT\n  )}",
                      "language": "PYTHON",
                      "path": "${data.databricks_current_user.me.home}/Terraform"
                    }
        argumentDocs:
            '*_task': '- (Required) one of the specific task blocks described below:'
            alert: '- (Optional) block consisting of following fields:'
            alert_id: '- (Required) (String) identifier of the Databricks Alert (databricks_alert).'
            alert_on_last_attempt: '- (Optional) (Bool) do not send notifications to recipients specified in on_start for the retried runs and do not send notifications to recipients specified in on_failure until the last retry of the run.'
            always_running: '- (Optional, Deprecated) (Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with parameters specified in spark_jar_task or spark_submit_task or spark_python_task or notebook_task blocks.'
            autotermination_minutes: ', is_pinned, workload_type aren''t supported!'
            base_parameters: '- (Optional) (Map) Base parameters to be used for each run of this job. If the run is initiated by a call to run-now with parameters specified, the two parameters maps will be merged. If the same key is specified in base_parameters and in run-now, the value from run-now will be used. If the notebook takes a parameter that is not specified in the job’s base_parameters or the run-now override parameters, the default value from the notebook will be used. Retrieve these parameters in a notebook using dbutils.widgets.get.'
            branch: '- name of the Git branch to use. Conflicts with tag and commit.'
            budget_policy_id: '- (Optional) The ID of the user-specified budget policy to use for this job. If not specified, a default budget policy may be applied when creating or modifying the job.'
            catalog: '- (Optional) The name of the catalog to use inside Unity Catalog.'
            client: '- (Required, string) client version used by the environment.'
            commands: '- (Required) (Array) Series of dbt commands to execute in sequence. Every command must start with "dbt".'
            commit: '- hash of Git commit to use. Conflicts with branch and tag.'
            concurrency: '- (Optional) Controls the number of active iteration task runs. Default is 20, maximum allowed is 100.'
            continuous: '- (Optional) Configuration block to configure pause status. See continuous Configuration Block.'
            continuous.pause_status: '- (Optional) Indicate whether this continuous job is paused or not. Either PAUSED or UNPAUSED. When the pause_status field is omitted in the block, the server will default to using UNPAUSED as a value for pause_status.'
            control_run_state: '- (Optional) (Bool) If true, the Databricks provider will stop and start the job as needed to ensure that the active run for the job reflects the deployed configuration. For continuous jobs, the provider respects the pause_status by stopping the current active run. This flag cannot be set for non-continuous jobs.'
            custom_subject: '- (Optional) string specifying a custom subject of email sent.'
            dashboard: '- (Optional) block consisting of following fields:'
            dashboard_id: '- (Required) (String) identifier of the Databricks SQL Dashboard databricks_sql_dashboard.'
            default: '- (Required) Default value of the parameter.'
            dependencies: '- (list of strings) List of pip dependencies, as supported by the version of pip in this environment. Each dependency is a pip requirement file line.  See API docs for more information.'
            depends_on: '- (Optional) block specifying dependency(-ies) for a given task.'
            description: '- (Optional) An optional description for the job. The maximum length is 1024 characters in UTF-8 encoding.'
            disable_auto_optimization: '- (Optional) A flag to disable auto optimization in serverless tasks.'
            email_notifications: '- (Optional) (List) An optional set of email addresses notified when runs of this job begins, completes or fails. The default behavior is to not send any emails. This field is a block and is documented below.'
            enabled: '- (Required) If true, enable queueing for the job.'
            entry_point: '- (Optional) Python function as entry point for the task'
            environment_key: '- (Optional) identifier of an environment block that is used to specify libraries.  Required for some tasks (spark_python_task, python_wheel_task, ...) running on serverless compute.'
            existing_cluster_id: '- (Optional) Identifier of the interactive cluster to run job on.  Note: running tasks on interactive clusters may lead to increased costs!'
            file: '- (Optional) block consisting of single string fields:'
            full_refresh: '- (Optional) (Bool) Specifies if there should be full refresh of the pipeline.'
            git_source: '- (Optional) Specifices the a Git repository for task source code. See git_source Configuration Block below.'
            health: '- (Optional) An optional block that specifies the health conditions for the job documented below.'
            id: '- ID of the system notification that is notified when an event defined in webhook_notifications is triggered.'
            inputs: '- (Required) (String) Array for task to iterate on. This can be a JSON string or a reference to an array parameter.'
            interval: '- (Required) Specifies the interval at which the job should run. This value is required.'
            is_pinned: '- isn''t supported'
            job_cluster: '- (Optional) A list of job databricks_cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings. Multi-task syntax'
            job_cluster.job_cluster_key: '- (Required) Identifier that can be referenced in task block, so that cluster is shared between tasks'
            job_cluster.new_cluster: '- Block with almost the same set of parameters as for databricks_cluster resource, except following (check the REST API documentation for full list of supported parameters):'
            job_cluster_key: '- (Optional) Identifier of the Job cluster specified in the job_cluster block.'
            job_id: '- (Required)(String) ID of the job'
            job_parameters: '- (Optional)(Map) Job parameters for the task'
            left: '- The left operand of the condition task. It could be a string value, job state, or a parameter reference.'
            library: '- (Optional) (List) An optional list of libraries to be installed on the cluster that will execute the job. See library Configuration Block below.'
            main_class_name: '- (Optional) The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library. The code should use SparkContext.getOrCreate to obtain a Spark context; otherwise, runs of the job will fail.'
            max_concurrent_runs: '- (Optional) (Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to 1.'
            max_retries: '- (Optional) (Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a FAILED or INTERNAL_ERROR lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry. A run can have the following lifecycle state: PENDING, RUNNING, TERMINATING, TERMINATED, SKIPPED or INTERNAL_ERROR.'
            metric: '- (Required) string specifying the metric to check.  The only supported metric is RUN_DURATION_SECONDS (check Jobs REST API documentation for the latest information).'
            min_retry_interval_millis: '- (Optional) (Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.'
            min_time_between_triggers_seconds: '- (Optional) If set, the trigger starts a run only after the specified amount of time passed since the last time the trigger fired. The minimum allowed value is 60 seconds.'
            name: '- (Optional) An optional name for the job. The default value is Untitled.'
            name.existing_cluster_id: '- (Optional) If existing_cluster_id, the ID of an existing cluster that will be used for all runs of this job. When running jobs on an existing cluster, you may need to manually restart the cluster if it stops responding. We strongly suggest to use new_cluster for greater reliability.'
            name.new_cluster: '- (Optional) Same set of parameters as for databricks_cluster resource.'
            named_parameters: '- (Optional) Named parameters for the task'
            new_cluster: '- (Optional) Task will run on a dedicated cluster.  See databricks_cluster documentation for specification. Some parameters, such as'
            no_alert_for_canceled_runs: '- (Optional) (Bool) don''t send alert for cancelled runs.'
            no_alert_for_skipped_runs: '- (Optional) (Bool) don''t send alert for skipped runs. (It''s recommended to use the corresponding setting in the notification_settings configuration block).'
            notebook_path: '- (Required) The path of the databricks_notebook to be run in the Databricks workspace or remote repository. For notebooks stored in the Databricks workspace, the path must be absolute and begin with a slash. For notebooks stored in a remote repository, the path must be relative. This field is required.'
            notification_settings: '- (Optional) An optional block controlling the notification settings on the job level documented below.'
            on_duration_warning_threshold_exceeded: '- (Optional) (List) list of emails to notify when the duration of a run exceeds the threshold specified by the RUN_DURATION_SECONDS metric in the health block.'
            on_failure: '- (Optional) (List) list of emails to notify when the run fails.'
            on_start: '- (Optional) (List) list of emails to notify when the run starts.'
            on_success: '- (Optional) (List) list of emails to notify when the run completes successfully.'
            op: '- The string specifying the operation used to compare operands.  Currently, following operators are supported: EQUAL_TO, GREATER_THAN, GREATER_THAN_OR_EQUAL, LESS_THAN, LESS_THAN_OR_EQUAL, NOT_EQUAL. (Check the API docs for the latest information).'
            outcome: '- (Optional, string) Can only be specified on condition task dependencies. The outcome of the dependent task that must be met for this task to run. Possible values are "true" or "false".'
            package_name: '- (Optional) Name of Python package'
            parameter: '- (Optional) Specifices job parameter for the job. See parameter Configuration Block'
            parameters: '- (Optional) Parameters for the task'
            path: '- If source is GIT: Relative path to the file in the repository specified in the git_source block with SQL commands to execute. If source is WORKSPACE: Absolute path to the file in the workspace with SQL commands to execute.'
            pause_subscriptions: '- (Optional) flag that specifies if subscriptions are paused or not.'
            pipeline_id: '- (Required) The pipeline''s unique ID.'
            profiles_directory: '- (Optional) The relative path to the directory in the repository specified by git_source where dbt should look in for the profiles.yml file. If not specified, defaults to the repository''s root directory. Equivalent to passing --profile-dir to a dbt command.'
            project_directory: '- (Required when source is WORKSPACE) The path where dbt should look for dbt_project.yml. Equivalent to passing --project-dir to the dbt CLI.'
            provider: '- (Optional, if it''s possible to detect Git provider by host name) case insensitive name of the Git provider.  Following values are supported right now (could be a subject for change, consult Repos API documentation): gitHub, gitHubEnterprise, bitbucketCloud, bitbucketServer, azureDevOpsServices, gitLab, gitLabEnterpriseEdition.'
            python_file: '- (Required) The URI of the Python file to be executed. databricks_dbfs_file, cloud file URIs (e.g. s3:/, abfss:/, gs:/), workspace paths and remote repository are supported. For Python files stored in the Databricks workspace, the path must be absolute and begin with /Repos. For files stored in a remote repository, the path must be relative. This field is required.'
            query: '- (Optional) block consisting of single string field: query_id - identifier of the Databricks Query (databricks_query).'
            queue: '- (Optional) The queue status for the job. See queue Configuration Block below.'
            retry_on_timeout: '- (Optional) (Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.'
            right: '- The right operand of the condition task. It could be a string value, job state, or parameter reference.'
            rules: '- (List) list of rules that are represented as objects with the following attributes:'
            run_as: '- (Optional) The user or the service prinicipal the job runs as. See run_as Configuration Block below.'
            run_as.service_principal_name: '- (Optional) The application ID of an active service principal. Setting this field requires the servicePrincipal/user role.'
            run_as.user_name: '- (Optional) The email of an active workspace user. Non-admin users can only set this field to their own email.'
            run_if: '- (Optional) An optional value indicating the condition that determines whether the task should be run once its dependencies have been completed. One of ALL_SUCCESS, AT_LEAST_ONE_SUCCESS, NONE_FAILED, ALL_DONE, AT_LEAST_ONE_FAILED or ALL_FAILED. When omitted, defaults to ALL_SUCCESS.'
            schedule: '- (Optional) An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. See schedule Configuration Block below.'
            schedule.pause_status: '- (Optional) Indicate whether this schedule is paused or not. Either PAUSED or UNPAUSED. When the pause_status field is omitted and a schedule is provided, the server will default to using UNPAUSED as a value for pause_status.'
            schedule.quartz_cron_expression: '- (Required) A Cron expression using Quartz syntax that describes the schedule for a job. This field is required.'
            schedule.timezone_id: '- (Required) A Java timezone ID. The schedule for a job will be resolved with respect to this timezone. See Java TimeZone for details. This field is required.'
            schema: '- (Optional) The name of the schema dbt should run in. Defaults to default.'
            source: '- (Optional) The source of the project. Possible values are WORKSPACE and GIT.  Defaults to GIT if a git_source block is present in the job definition.'
            spark_version: parameter in databricks_cluster and other resources.
            spec: '- block describing the Environment. Consists of following attributes:'
            subscriptions: '- (Optional) a list of subscription blocks consisting out of one of the required fields: user_name for user emails or destination_id - for Alert destination''s identifier.'
            tag: '- name of the Git branch to use. Conflicts with branch and commit.'
            tags: '- (Optional) An optional map of the tags associated with the job. See tags Configuration Map'
            task: '- (Optional) A list of task specification that the job will execute. See task Configuration Block below.'
            task_key: '- (Required) string specifying an unique key for a given task.'
            timeout_seconds: '- (Optional) (Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.'
            trigger: '- (Optional) The conditions that triggers the job to start. See trigger Configuration Block below.'
            trigger.file_arrival: '- (Optional) configuration block to define a trigger for File Arrival events consisting of following attributes:'
            trigger.pause_status: '- (Optional) Indicate whether this trigger is paused or not. Either PAUSED or UNPAUSED. When the pause_status field is omitted in the block, the server will default to using UNPAUSED as a value for pause_status.'
            trigger.periodic: '- (Optional) configuration block to define a trigger for Periodic Triggers consisting of the following attributes:'
            unit: '- (Required) Options are {"DAYS", "HOURS", "WEEKS"}.'
            url: '- (Required) URL to be monitored for file arrivals. The path must point to the root or a subpath of the external location. Please note that the URL must have a trailing slash character (/).'
            value: '- (Required) integer value used to compare to the given metric.'
            wait_after_last_change_seconds: '- (Optional) If set, the trigger starts a run only after no file activity has occurred for the specified amount of time. This makes it possible to wait for a batch of incoming files to arrive before triggering a run. The minimum allowed value is 60 seconds.'
            warehouse_id: '- (Optional) The ID of the SQL warehouse that dbt should execute against.'
            webhook_notification.on_duration_warning_threshold_exceeded: '- (Optional) (List) list of notification IDs to call when the duration of a run exceeds the threshold specified by the RUN_DURATION_SECONDS metric in the health block.'
            webhook_notification.on_failure: '- (Optional) (List) list of notification IDs to call when the run fails. A maximum of 3 destinations can be specified.'
            webhook_notification.on_start: '- (Optional) (List) list of notification IDs to call when the run starts. A maximum of 3 destinations can be specified.'
            webhook_notification.on_success: '- (Optional) (List) list of notification IDs to call when the run completes successfully. A maximum of 3 destinations can be specified.'
            webhook_notifications: '- (Optional) (List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes or fails. The default behavior is to not send any notifications. This field is a block and is documented below.'
            workload_type: '- isn''t supported'
        importStatements: []
    databricks_lakehouse_monitor:
        subCategory: Unity Catalog
        description: '""subcategory: "Unity Catalog"'
        name: databricks_lakehouse_monitor
        title: databricks_lakehouse_monitor Resource
        examples:
            - name: testTimeseriesMonitor
              manifest: |-
                {
                  "assets_dir": "/Shared/provider-test/databricks_lakehouse_monitoring/${databricks_sql_table.myTestTable.name}",
                  "output_schema_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}",
                  "table_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}.${databricks_sql_table.myTestTable.name}",
                  "time_series": [
                    {
                      "granularities": [
                        "1 hour"
                      ],
                      "timestamp_col": "timestamp"
                    }
                  ]
                }
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.id}",
                      "comment": "this database is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
                databricks_sql_table.myTestTable: |-
                    {
                      "catalog_name": "main",
                      "column": [
                        {
                          "name": "timestamp",
                          "type": "int"
                        }
                      ],
                      "data_source_format": "DELTA",
                      "name": "bar",
                      "schema_name": "${databricks_schema.things.name}",
                      "table_type": "MANAGED"
                    }
            - name: testMonitorInference
              manifest: |-
                {
                  "assets_dir": "/Shared/provider-test/databricks_lakehouse_monitoring/${databricks_table.myTestTable.name}",
                  "inference_log": [
                    {
                      "granularities": [
                        "1 hour"
                      ],
                      "model_id_col": "model_id",
                      "prediction_col": "prediction",
                      "problem_type": "PROBLEM_TYPE_REGRESSION",
                      "timestamp_col": "timestamp"
                    }
                  ],
                  "output_schema_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}",
                  "table_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}.${databricks_table.myTestTable.name}"
                }
            - name: testMonitorInference
              manifest: |-
                {
                  "assets_dir": "/Shared/provider-test/databricks_lakehouse_monitoring/${databricks_table.myTestTable.name}",
                  "output_schema_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}",
                  "snapshot": [
                    {}
                  ],
                  "table_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}.${databricks_table.myTestTable.name}"
                }
        argumentDocs:
            assets_dir: '- (Required) - The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)'
            baseline_table_name: |-
                - Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline
                table.
            custom_metrics: '- Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).'
            dashboard_id: '- The ID of the generated dashboard.'
            data_classification_config: '- The data classification config for the monitor'
            definition: '- create metric definition'
            drift_metrics_table_name: '- The full name of the drift metrics table. Format: catalog_name.schema_name.table_name.'
            granularities: '-  List of granularities to use when aggregating data into time windows based on their timestamp.'
            id: '-  ID of this monitor is the same as the full table name of the format {catalog}.{schema_name}.{table_name}'
            inference_log: '- Configuration for the inference log monitor'
            input_columns: '- Columns on the monitored table to apply the custom metrics to.'
            label_col: '- Column of the model label'
            model_id_col: '- Column of the model id or version'
            monitor_version: '- The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted'
            name: '- Name of the custom metric.'
            notifications: '- The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name email_addresses containing a list of emails to notify:'
            on_failure: '- who to send notifications to on monitor failure.'
            on_new_classification_tag_detected: '- Who to send notifications to when new data classification tags are detected.'
            output_data_type: '- The output type of the custom metric.'
            output_schema_name: '- (Required) - Schema where output metric tables are created'
            pause_status: '- optional string field that indicates whether a schedule is paused (PAUSED) or not (UNPAUSED).'
            prediction_col: '- Column of the model prediction'
            prediction_proba_col: '- Column of the model prediction probabilities'
            problem_type: '- Problem type the model aims to solve. Either PROBLEM_TYPE_CLASSIFICATION or PROBLEM_TYPE_REGRESSION'
            profile_metrics_table_name: '- The full name of the profile metrics table. Format: catalog_name.schema_name.table_name.'
            quartz_cron_expression: '- string expression that determines when to run the monitor. See Quartz documentation for examples.'
            schedule: '- The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:'
            skip_builtin_dashboard: '- Whether to skip creating a default dashboard summarizing data quality metrics.'
            slicing_exprs: '- List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.'
            snapshot: '- Configuration for monitoring snapshot tables.'
            status: '- Status of the Monitor'
            table_name: '- (Required) - The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}'
            time_series: '- Configuration for monitoring timeseries tables.'
            timestamp_col: '- Column of the timestamp of predictions'
            timezone_id: '- string with timezone id (e.g., PST) in which to evaluate the Quartz expression.'
            type: '- The type of the custom metric.'
            warehouse_id: '- Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.'
        importStatements: []
    databricks_library:
        subCategory: Compute
        description: '""subcategory: "Compute"'
        name: databricks_library
        title: databricks_library Resource
        examples:
            - name: cli
              manifest: |-
                {
                  "cluster_id": "${each.key}",
                  "for_each": "${data.databricks_clusters.all.ids}",
                  "pypi": [
                    {
                      "package": "databricks-cli"
                    }
                  ]
                }
              references:
                cluster_id: each.key
                for_each: data.databricks_clusters.all.ids
            - name: app
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.this.id}",
                  "jar": "${databricks_dbfs_file.app.dbfs_path}"
                }
              references:
                cluster_id: databricks_cluster.this.id
                jar: databricks_dbfs_file.app.dbfs_path
              dependencies:
                databricks_dbfs_file.app: |-
                    {
                      "path": "/FileStore/app-0.0.1.jar",
                      "source": "${path.module}/app-0.0.1.jar"
                    }
            - name: deequ
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.this.id}",
                  "maven": [
                    {
                      "coordinates": "com.amazon.deequ:deequ:1.0.4",
                      "exclusions": [
                        "org.apache.avro:avro"
                      ]
                    }
                  ]
                }
              references:
                cluster_id: databricks_cluster.this.id
            - name: app
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.this.id}",
                  "whl": "${databricks_dbfs_file.app.dbfs_path}"
                }
              references:
                cluster_id: databricks_cluster.this.id
                whl: databricks_dbfs_file.app.dbfs_path
              dependencies:
                databricks_dbfs_file.app: |-
                    {
                      "path": "/FileStore/baz.whl",
                      "source": "${path.module}/baz.whl"
                    }
            - name: fbprophet
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.this.id}",
                  "pypi": [
                    {
                      "package": "fbprophet==0.6"
                    }
                  ]
                }
              references:
                cluster_id: databricks_cluster.this.id
            - name: libraries
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.this.id}",
                  "requirements": "/Workspace/path/to/requirements.txt"
                }
              references:
                cluster_id: databricks_cluster.this.id
            - name: app
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.this.id}",
                  "egg": "${databricks_dbfs_file.app.dbfs_path}"
                }
              references:
                cluster_id: databricks_cluster.this.id
                egg: databricks_dbfs_file.app.dbfs_path
              dependencies:
                databricks_dbfs_file.app: |-
                    {
                      "path": "/FileStore/foo.egg",
                      "source": "${path.module}/foo.egg"
                    }
            - name: rkeops
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.this.id}",
                  "cran": [
                    {
                      "package": "rkeops"
                    }
                  ]
                }
              references:
                cluster_id: databricks_cluster.this.id
        argumentDocs:
            dbfs:/mnt/name: .
        importStatements: []
    databricks_metastore:
        subCategory: Unity Catalog
        description: '""subcategory: "Unity Catalog"'
        name: databricks_metastore
        title: databricks_metastore Resource
        examples:
            - name: this
              manifest: |-
                {
                  "force_destroy": true,
                  "name": "primary",
                  "owner": "uc admins",
                  "region": "us-east-1",
                  "storage_root": "s3://${aws_s3_bucket.metastore.id}/metastore"
                }
              dependencies:
                databricks_metastore_assignment.this: |-
                    {
                      "metastore_id": "${databricks_metastore.this.id}",
                      "workspace_id": "${local.workspace_id}"
                    }
            - name: this
              manifest: |-
                {
                  "force_destroy": true,
                  "name": "primary",
                  "owner": "uc admins",
                  "region": "eastus",
                  "storage_root": "${format(\"abfss://%s@%s.dfs.core.windows.net/\",\n    azurerm_storage_container.unity_catalog.name,\n  azurerm_storage_account.unity_catalog.name)}"
                }
              dependencies:
                databricks_metastore_assignment.this: |-
                    {
                      "metastore_id": "${databricks_metastore.this.id}",
                      "workspace_id": "${local.workspace_id}"
                    }
            - name: this
              manifest: |-
                {
                  "force_destroy": true,
                  "name": "primary",
                  "owner": "uc admins",
                  "region": "${us-east1}",
                  "storage_root": "gs://${google_storage_bucket.unity_metastore.name}"
                }
              references:
                region: us-east1
              dependencies:
                databricks_metastore_assignment.this: |-
                    {
                      "metastore_id": "${databricks_metastore.this.id}",
                      "workspace_id": "${local.workspace_id}"
                    }
        argumentDocs:
            delta_sharing_organization_name: '- (Optional) The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing. Once this is set it cannot be removed and can only be modified to another valid value. To delete this value please taint and recreate the resource.'
            delta_sharing_recipient_token_lifetime_in_seconds: '- (Optional) Required along with delta_sharing_scope. Used to set expiration duration in seconds on recipient data access tokens. Set to 0 for unlimited duration.'
            delta_sharing_scope: '- (Optional) Required along with delta_sharing_recipient_token_lifetime_in_seconds. Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL.  INTERNAL only allows sharing within the same account, and INTERNAL_AND_EXTERNAL allows cross account sharing and token based sharing.'
            force_destroy: '- (Optional) Destroy metastore regardless of its contents.'
            id: '- system-generated ID of this Unity Catalog Metastore.'
            name: '- Name of metastore.'
            owner: '- (Optional) Username/groupname/sp application_id of the metastore owner.'
            region: '- (Mandatory for account-level) The region of the metastore'
            storage_root: '- (Optional) Path on cloud storage account, where managed databricks_table are stored. Change forces creation of a new resource. If no storage_root is defined for the metastore, each catalog must have a storage_root defined.'
        importStatements: []
    databricks_metastore_assignment:
        subCategory: Unity Catalog
        description: '""subcategory: "Unity Catalog"'
        name: databricks_metastore_assignment
        title: databricks_metastore_assignment Resource
        examples:
            - name: this
              manifest: |-
                {
                  "metastore_id": "${databricks_metastore.this.id}",
                  "workspace_id": "${local.workspace_id}"
                }
              references:
                metastore_id: databricks_metastore.this.id
                workspace_id: local.workspace_id
              dependencies:
                databricks_metastore.this: |-
                    {
                      "force_destroy": true,
                      "name": "primary",
                      "owner": "uc admins",
                      "region": "us-east-1",
                      "storage_root": "s3://${aws_s3_bucket.metastore.id}/metastore"
                    }
        argumentDocs:
            default_catalog_name: '- (Deprecated) Default catalog used for this assignment. Please use databricks_default_namespace_setting instead.'
            id: '- ID of this metastore assignment in form of <workspace_id>|<metastore_id>.'
            metastore_id: '- Unique identifier of the parent Metastore'
            workspace_id: '- id of the workspace for the assignment'
        importStatements: []
    databricks_metastore_data_access:
        subCategory: Unity Catalog
        description: '""subcategory: "Unity Catalog"'
        name: databricks_metastore_data_access
        title: databricks_metastore_data_access Resource
        examples:
            - name: this
              manifest: |-
                {
                  "aws_iam_role": [
                    {
                      "role_arn": "${aws_iam_role.metastore_data_access.arn}"
                    }
                  ],
                  "is_default": true,
                  "metastore_id": "${databricks_metastore.this.id}",
                  "name": "${aws_iam_role.metastore_data_access.name}"
                }
              references:
                aws_iam_role.role_arn: aws_iam_role.metastore_data_access.arn
                metastore_id: databricks_metastore.this.id
                name: aws_iam_role.metastore_data_access.name
              dependencies:
                databricks_metastore.this: |-
                    {
                      "force_destroy": true,
                      "name": "primary",
                      "owner": "uc admins",
                      "region": "us-east-1",
                      "storage_root": "s3://${aws_s3_bucket.metastore.id}/metastore"
                    }
            - name: this
              manifest: |-
                {
                  "azure_managed_identity": [
                    {
                      "access_connector_id": "${var.access_connector_id}"
                    }
                  ],
                  "is_default": true,
                  "metastore_id": "${databricks_metastore.this.id}",
                  "name": "mi_dac"
                }
              references:
                azure_managed_identity.access_connector_id: var.access_connector_id
                metastore_id: databricks_metastore.this.id
              dependencies:
                databricks_metastore.this: |-
                    {
                      "force_destroy": true,
                      "name": "primary",
                      "owner": "uc admins",
                      "region": "eastus",
                      "storage_root": "${format(\"abfss://%s@%s.dfs.core.windows.net/\",\n    azurerm_storage_container.unity_catalog.name,\n  azurerm_storage_account.unity_catalog.name)}"
                    }
        argumentDocs:
            id: '- ID of this data access configuration in form of <metastore_id>|<name>.'
            is_default: '-  whether to set this credential as the default for the metastore. In practice, this should always be true.'
        importStatements: []
    databricks_mlflow_experiment:
        subCategory: MLflow
        description: '""subcategory: "MLflow"'
        name: databricks_mlflow_experiment
        title: databricks_mlflow_experiment Resource
        examples:
            - name: this
              manifest: |-
                {
                  "artifact_location": "dbfs:/tmp/my-experiment",
                  "description": "My MLflow experiment description",
                  "name": "${data.databricks_current_user.me.home}/Sample"
                }
        argumentDocs:
            artifact_location: '- Path to dbfs:/ or s3:// artifact location of the MLflow experiment.'
            description: '- The description of the MLflow experiment.'
            id: '- ID of the MLflow experiment.'
            name: '- (Required) Name of MLflow experiment. It must be an absolute path within the Databricks workspace, e.g. /Users/<some-username>/my-experiment. For more information about changes to experiment naming conventions, see mlflow docs.'
        importStatements: []
    databricks_mlflow_model:
        subCategory: MLflow
        description: '""subcategory: "MLflow"'
        name: databricks_mlflow_model
        title: databricks_mlflow_model Resource
        examples:
            - name: test
              manifest: |-
                {
                  "description": "My MLflow model description",
                  "name": "My MLflow Model",
                  "tags": [
                    {
                      "key": "key1",
                      "value": "value1"
                    },
                    {
                      "key": "key2",
                      "value": "value2"
                    }
                  ]
                }
        argumentDocs:
            description: '- The description of the MLflow model.'
            id: '- ID of the MLflow model, the same as name.'
            name: '- (Required) Name of MLflow model. Change of name triggers new resource.'
            tags: '- Tags for the MLflow model.'
        importStatements: []
    databricks_mlflow_webhook:
        subCategory: MLflow
        description: '""subcategory: "MLflow"'
        name: databricks_mlflow_webhook
        title: databricks_mlflow_webhook Resource
        examples:
            - name: job
              manifest: |-
                {
                  "description": "Databricks Job webhook trigger",
                  "events": [
                    "TRANSITION_REQUEST_CREATED"
                  ],
                  "job_spec": [
                    {
                      "access_token": "${databricks_token.pat_for_webhook.token_value}",
                      "job_id": "${databricks_job.this.id}",
                      "workspace_url": "${data.databricks_current_user.me.workspace_url}"
                    }
                  ],
                  "status": "ACTIVE"
                }
              references:
                job_spec.access_token: databricks_token.pat_for_webhook.token_value
                job_spec.job_id: databricks_job.this.id
                job_spec.workspace_url: data.databricks_current_user.me.workspace_url
              dependencies:
                databricks_job.this: |-
                    {
                      "name": "Terraform MLflowWebhook Demo (${data.databricks_current_user.me.alphanumeric})",
                      "task": [
                        {
                          "new_cluster": [
                            {
                              "node_type_id": "${data.databricks_node_type.smallest.id}",
                              "num_workers": 1,
                              "spark_version": "${data.databricks_spark_version.latest.id}"
                            }
                          ],
                          "notebook_task": [
                            {
                              "notebook_path": "${databricks_notebook.this.path}"
                            }
                          ],
                          "task_key": "task1"
                        }
                      ]
                    }
                databricks_notebook.this: |-
                    {
                      "content_base64": "${base64encode(\u003c\u003c-EOT\n    import json\n \n    event_message = dbutils.widgets.get(\"event_message\")\n    event_message_dict = json.loads(event_message)\n    print(f\"event data={event_message_dict}\")\n    EOT\n  )}",
                      "language": "PYTHON",
                      "path": "${data.databricks_current_user.me.home}/MLFlowWebhook"
                    }
                databricks_token.pat_for_webhook: |-
                    {
                      "comment": "MLflow Webhook",
                      "lifetime_seconds": 86400000
                    }
            - name: url
              manifest: |-
                {
                  "description": "URL webhook trigger",
                  "events": [
                    "TRANSITION_REQUEST_CREATED"
                  ],
                  "http_url_spec": [
                    {
                      "url": "https://my_cool_host/webhook"
                    }
                  ]
                }
        argumentDocs:
            access_token: '- (Required, Sensitive) The personal access token used to authorize webhook''s job runs.'
            authorization: '- (Optional) Value of the authorization header that should be sent in the request sent by the wehbook.  It should be of the form <auth type> <credentials>, e.g. Bearer <access_token>. If set to an empty string, no authorization header will be included in the request.'
            description: '- Optional description of the MLflow webhook.'
            enable_ssl_verification: '- (Optional) Enable/disable SSL certificate validation. Default is true. For self-signed certificates, this field must be false AND the destination server must disable certificate validation as well. For security purposes, it is encouraged to perform secret validation with the HMAC-encoded portion of the payload and acknowledge the risk associated with disabling hostname validation whereby it becomes more likely that requests can be maliciously routed to an unintended host.'
            events: '- (Required) The list of events that will trigger execution of Databricks job or POSTing to an URL, for example, MODEL_VERSION_CREATED, MODEL_VERSION_TRANSITIONED_STAGE, TRANSITION_REQUEST_CREATED, etc.  Refer to the Webhooks API documentation for a full list of supported events.'
            id: '- Unique ID of the MLflow Webhook.'
            job_id: '- (Required) ID of the Databricks job that the webhook runs.'
            model_name: '- (Optional) Name of MLflow model for which webhook will be created. If the model name is not specified, a registry-wide webhook is created that listens for the specified events across all versions of all registered models.'
            secret: '- (Optional, Sensitive) Shared secret required for HMAC encoding payload. The HMAC-encoded payload will be sent in the header as X-Databricks-Signature: encoded_payload.'
            status: '- Optional status of webhook. Possible values are ACTIVE, TEST_MODE, DISABLED. Default is ACTIVE.'
            url: '- (Required) External HTTPS URL called on event trigger (by using a POST request). Structure of payload depends on the event type, refer to documentation for more details.'
            workspace_url: '- (Optional) URL of the workspace containing the job that this webhook runs. If not specified, the job’s workspace URL is assumed to be the same as the workspace where the webhook is created.'
        importStatements: []
    databricks_model_serving:
        subCategory: Serving
        description: '""subcategory: "Serving"'
        name: databricks_model_serving
        title: databricks_model_serving Resource
        examples:
            - name: this
              manifest: |-
                {
                  "config": [
                    {
                      "served_entities": [
                        {
                          "entity_name": "ads-model",
                          "entity_version": "2",
                          "name": "prod_model",
                          "scale_to_zero_enabled": true,
                          "workload_size": "Small"
                        },
                        {
                          "entity_name": "ads-model",
                          "entity_version": "4",
                          "name": "candidate_model",
                          "scale_to_zero_enabled": false,
                          "workload_size": "Small"
                        }
                      ],
                      "traffic_config": [
                        {
                          "routes": [
                            {
                              "served_model_name": "prod_model",
                              "traffic_percentage": 90
                            },
                            {
                              "served_model_name": "candidate_model",
                              "traffic_percentage": 10
                            }
                          ]
                        }
                      ]
                    }
                  ],
                  "name": "ads-serving-endpoint"
                }
        argumentDocs:
            _plaintext: suffix) or in plain text (parameters with _plaintext suffix)!
            ai_gateway: '- (Optional) A block with AI Gateway configuration for the serving endpoint. Note: only external model endpoints are supported as of now.'
            ai_gateway.guardrails: '- (Optional) Block with configuration for AI Guardrails to prevent unwanted data and unsafe data in requests and responses. Consists of the following attributes:'
            ai_gateway.inference_table_config: '- (Optional) Block describing the configuration of usage tracking. Consists of the following attributes:'
            ai_gateway.rate_limits: '- (Optional) Block describing rate limits for AI gateway. For details see the description of rate_limits block above.'
            ai_gateway.usage_tracking_config: '- (Optional) Block with configuration for payload logging using inference tables. For details see the description of auto_capture_config block above.'
            ai21labs_api_key: '- The Databricks secret key reference for an AI21Labs API key.'
            ai21labs_api_key_plaintext: '- An AI21 Labs API key provided as a plaintext string.'
            ai21labs_config: '- AI21Labs Config'
            amazon_bedrock_config: '- Amazon Bedrock Config'
            anthropic_api_key: '- The Databricks secret key reference for an Anthropic API key.'
            anthropic_api_key_plaintext: '- The Anthropic API key provided as a plaintext string.'
            anthropic_config: '- Anthropic Config'
            auto_capture_config: '- Configuration for Inference Tables which automatically logs requests and responses to Unity Catalog.'
            auto_capture_config.catalog_name: '- The name of the catalog in Unity Catalog. NOTE: On update, you cannot change the catalog name if it was already set.'
            auto_capture_config.enabled: '- If inference tables are enabled or not. NOTE: If you have already disabled payload logging once, you cannot enable it again.'
            auto_capture_config.schema_name: '- The name of the schema in Unity Catalog. NOTE: On update, you cannot change the schema name if it was already set.'
            auto_capture_config.table_name_prefix: '- The prefix of the table in Unity Catalog. NOTE: On update, you cannot change the prefix name if it was already set.'
            aws_access_key_id: '- The Databricks secret key reference for an AWS Access Key ID with permissions to interact with Bedrock services.'
            aws_access_key_id_plaintext: '- An AWS access key ID with permissions to interact with Bedrock services provided as a plaintext string.'
            aws_region: '- The AWS region to use. Bedrock has to be enabled there.'
            aws_secret_access_key: '- The Databricks secret key reference for an AWS Secret Access Key paired with the access key ID, with permissions to interact with Bedrock services.'
            aws_secret_access_key_plaintext: '-  An AWS secret access key paired with the access key ID, with permissions to interact with Bedrock services provided as a plaintext string.'
            bedrock_provider: '- The underlying provider in Amazon Bedrock. Supported values (case insensitive) include: Anthropic, Cohere, AI21Labs, Amazon.'
            behavior: '- a string that describes the behavior for PII filter. Currently only BLOCK value is supported.'
            cohere_api_key: '- The Databricks secret key reference for a Cohere API key.'
            cohere_api_key_plaintext: '- The Cohere API key provided as a plaintext string.'
            cohere_config: '- Cohere Config'
            config: '- (Required) The model serving endpoint configuration.'
            databricks_api_token: '- The Databricks secret key reference for a Databricks API token that corresponds to a user or service principal with Can Query access to the model serving endpoint pointed to by this external model.'
            databricks_api_token_plaintext: '- The Databricks API token that corresponds to a user or service principal with Can Query access to the model serving endpoint pointed to by this external model provided as a plaintext string.'
            databricks_model_serving_config: '- Databricks Model Serving Config'
            databricks_workspace_url: '- The URL of the Databricks workspace containing the model serving endpoint pointed to by this external model.'
            enabled: '- boolean flag specifying if usage tracking is enabled.'
            google_cloud_vertex_ai_config: '- Google Cloud Vertex AI Config.'
            id: '- Equal to the name argument and used to identify the serving endpoint.'
            input: '- A block with configuration for input guardrail filters:'
            invalid_keywords: '- List of invalid keywords. AI guardrail uses keyword or string matching to decide if the keyword exists in the request or response content.'
            microsoft_entra_client_id: '- This field is only required for Azure AD OpenAI and is the Microsoft Entra Client ID.'
            microsoft_entra_client_secret: '- The Databricks secret key reference for a client secret used for Microsoft Entra ID authentication.'
            microsoft_entra_client_secret_plaintext: '- The client secret used for Microsoft Entra ID authentication provided as a plaintext string.'
            microsoft_entra_tenant_id: '- This field is only required for Azure AD OpenAI and is the Microsoft Entra Tenant ID.'
            name: '- (Required) The name of the model serving endpoint. This field is required and must be unique across a workspace. An endpoint name can consist of alphanumeric characters, dashes, and underscores. NOTE: Changing this name will delete the existing endpoint and create a new endpoint with the updated name.'
            openai_api_base: '- This is the base URL for the OpenAI API (default: "https://api.openai.com/v1"). For Azure OpenAI, this field is required and is the base URL for the Azure OpenAI API service provided by Azure.'
            openai_api_key: '- The Databricks secret key reference for an OpenAI or Azure OpenAI API key.'
            openai_api_key_plaintext: '- The OpenAI API key using the OpenAI or Azure service provided as a plaintext string.'
            openai_api_type: '- This is an optional field to specify the type of OpenAI API to use. For Azure OpenAI, this field is required, and this parameter represents the preferred security access validation protocol. For access token validation, use azure. For authentication using Azure Active Directory (Azure AD) use, azuread.'
            openai_api_version: '- This is an optional field to specify the OpenAI API version. For Azure OpenAI, this field is required and is the version of the Azure OpenAI service to utilize, specified by a date.'
            openai_config: '- OpenAI Config'
            openai_deployment_name: '- This field is only required for Azure OpenAI and is the name of the deployment resource for the Azure OpenAI service.'
            openai_organization: '- This is an optional field to specify the organization in OpenAI or Azure OpenAI.'
            output: '- A block with configuration for output guardrail filters.  Has the same structure as input block.'
            palm_api_key: '- The Databricks secret key reference for a PaLM API key.'
            palm_api_key_plaintext: '- The PaLM API key provided as a plaintext string.'
            palm_config: '- PaLM Config'
            pii: '- Block with configuration for guardrail PII filter:'
            private_key: '- The Databricks secret key reference for a private key for the service account that has access to the Google Cloud Vertex AI Service.'
            private_key_plaintext: '- The private key for the service account that has access to the Google Cloud Vertex AI Service is provided as a plaintext secret.'
            project_id: '- This is the Google Cloud project id that the service account is associated with.'
            provider: '- (Required) The name of the provider for the external model. Currently, the supported providers are ai21labs, anthropic, amazon-bedrock, cohere, databricks-model-serving, google-cloud-vertex-ai, openai, and palm.'
            rate_limits: '- A list of rate limit blocks to be applied to the serving endpoint. Note: only external and foundation model endpoints are supported as of now.'
            rate_limits.calls: '- (Required) Used to specify how many calls are allowed for a key within the renewal_period.'
            rate_limits.key: '- (Optional) Key field for a serving endpoint rate limit. Currently, only user and endpoint are supported, with endpoint being the default if not specified.'
            rate_limits.renewal_period: '- (Required) Renewal period field for a serving endpoint rate limit. Currently, only minute is supported.'
            region: '- This is the region for the Google Cloud Vertex AI Service.'
            route_optimized: '- (Optional) A boolean enabling route optimization for the endpoint. Note: only available for custom models.'
            safety: '- the boolean flag that indicates whether the safety filter is enabled.'
            served_entities: '- A list of served entities for the endpoint to serve. A serving endpoint can have up to 10 served entities.'
            served_entities.entity_name: '- The name of the entity to be served. The entity may be a model in the Databricks Model Registry, a model in the Unity Catalog (UC), or a function of type FEATURE_SPEC in the UC. If it is a UC object, the full name of the object should be given in the form of catalog_name.schema_name.model_name.'
            served_entities.entity_version: '- The version of the model in Databricks Model Registry to be served or empty if the entity is a FEATURE_SPEC.'
            served_entities.environment_vars: '- An object containing a set of optional, user-specified environment variable key-value pairs used for serving this entity. Note: this is an experimental feature and is subject to change. Example entity environment variables that refer to Databricks secrets: {"OPENAI_API_KEY": "{{secrets/my_scope/my_key}}", "DATABRICKS_TOKEN": "{{secrets/my_scope2/my_key2}}"}'
            served_entities.external_model: '- The external model to be served. NOTE: Only one of external_model and (entity_name, entity_version, workload_size, workload_type, and scale_to_zero_enabled) can be specified with the latter set being used for custom model serving for a Databricks registered model. When an external_model is present, the served entities list can only have one served_entity object. An existing endpoint with external_model can not be updated to an endpoint without external_model. If the endpoint is created without external_model, users cannot update it to add external_model later.'
            served_entities.instance_profile_arn: '- ARN of the instance profile that the served entity uses to access AWS resources.'
            served_entities.max_provisioned_throughput: '-  The maximum tokens per second that the endpoint can scale up to.'
            served_entities.min_provisioned_throughput: '- The minimum tokens per second that the endpoint can scale down to.'
            served_entities.name: '- The name of a served entity. It must be unique across an endpoint. A served entity name can consist of alphanumeric characters, dashes, and underscores. If not specified for an external model, this field defaults to external_model.name, with ''.'' and '':'' replaced with ''-'', and if not specified for other entities, it defaults to -.'
            served_entities.scale_to_zero_enabled: '- Whether the compute resources for the served entity should scale down to zero.'
            served_entities.workload_size: '- The workload size of the served entity. The workload size corresponds to a range of provisioned concurrency that the compute autoscales between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are Small (4 - 4 provisioned concurrency), Medium (8 - 16 provisioned concurrency), and Large (16 - 64 provisioned concurrency). If scale-to-zero is enabled, the lower bound of the provisioned concurrency for each workload size is 0.'
            served_entities.workload_type: '- The workload type of the served entity. The workload type selects which type of compute to use in the endpoint. The default value for this parameter is CPU. For deep learning workloads, GPU acceleration is available by selecting workload types like GPU_SMALL and others. See the available GPU types.'
            served_entity_name: '- (Required) The name of the served entity this route configures traffic for. This needs to match the name of a served_entity block.'
            served_models: '- (Deprecated, use served_entities instead) Each block represents a served model for the endpoint to serve. A model serving endpoint can have up to 10 served models.'
            served_models.environment_vars: '- (Optional) a map of environment variable names/values that will be used for serving this model.  Environment variables may refer to Databricks secrets using the standard syntax: {{secrets/secret_scope/secret_key}}.'
            served_models.instance_profile_arn: '- (Optional) ARN of the instance profile that the served model will use to access AWS resources.'
            served_models.model_name: '- (Required) The name of the model in Databricks Model Registry to be served.'
            served_models.model_version: '- (Required) The version of the model in Databricks Model Registry to be served.'
            served_models.name: '- The name of a served model. It must be unique across an endpoint. If not specified, this field will default to modelname-modelversion. A served model name can consist of alphanumeric characters, dashes, and underscores.'
            served_models.scale_to_zero_enabled: '- Whether the compute resources for the served model should scale down to zero. If scale-to-zero is enabled, the lower bound of the provisioned concurrency for each workload size will be 0. The default value is true.'
            served_models.workload_size: '- (Required) The workload size of the served model. The workload size corresponds to a range of provisioned concurrency that the compute will autoscale between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are Small (4 - 4 provisioned concurrency), Medium (8 - 16 provisioned concurrency), and Large (16 - 64 provisioned concurrency).'
            served_models.workload_type: '- The workload type of the served model. The workload type selects which type of compute to use in the endpoint. For deep learning workloads, GPU acceleration is available by selecting workload types like GPU_SMALL and others. See the documentation for all options. The default value is CPU.'
            serving_endpoint_id: '- Unique identifier of the serving endpoint primarily used to set permissions and refer to this instance for other operations.'
            tags: '- Tags to be attached to the serving endpoint and automatically propagated to billing logs.'
            tags.key: '- The key field for a tag.'
            tags.value: '- The value field for a tag.'
            task: '- The task type of the external model.'
            traffic_config: '- A single block represents the traffic split configuration amongst the served models.'
            traffic_config.routes: '- (Required) Each block represents a route that defines traffic to each served entity. Each served_entity block needs to have a corresponding routes block.'
            traffic_percentage: '- (Required) The percentage of endpoint traffic to send to this route. It must be an integer between 0 and 100 inclusive.'
            valid_topics: '- The list of allowed topics. Given a chat request, this guardrail flags the request if its topic is not in the allowed topics.'
        importStatements: []
    databricks_mount:
        subCategory: Storage
        description: '""subcategory: "Storage"'
        name: databricks_mount
        title: databricks_mount Resource
        examples:
            - name: this
              manifest: |-
                {
                  "extra_configs": {
                    "fs.azure.account.auth.type": "OAuth",
                    "fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
                    "fs.azure.account.oauth2.client.endpoint": "https://login.microsoftonline.com/${local.tenant_id}/oauth2/token",
                    "fs.azure.account.oauth2.client.id": "${local.client_id}",
                    "fs.azure.account.oauth2.client.secret": "{{secrets/${local.secret_scope}/${local.secret_key}}}",
                    "fs.azure.createRemoteFileSystemDuringInitialization": "false"
                  },
                  "name": "tf-abfss",
                  "uri": "abfss://${local.container}@${local.storage_acc}.dfs.core.windows.net"
                }
            - name: passthrough
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.shared_passthrough.id}",
                  "extra_configs": {
                    "fs.azure.account.auth.type": "CustomAccessToken",
                    "fs.azure.account.custom.token.provider.class": "{{sparkconf/spark.databricks.passthrough.adls.gen2.tokenProviderClassName}}"
                  },
                  "name": "passthrough-test",
                  "uri": "abfss://${var.container}@${var.storage_acc}.dfs.core.windows.net"
                }
              references:
                cluster_id: databricks_cluster.shared_passthrough.id
              dependencies:
                databricks_cluster.shared_passthrough: |-
                    {
                      "autotermination_minutes": 10,
                      "cluster_name": "Shared Passthrough for mount",
                      "custom_tags": {
                        "ResourceClass": "Serverless"
                      },
                      "node_type_id": "${data.databricks_node_type.smallest.id}",
                      "num_workers": 1,
                      "spark_conf": {
                        "spark.databricks.cluster.profile": "serverless",
                        "spark.databricks.passthrough.enabled": "true",
                        "spark.databricks.pyspark.enableProcessIsolation": "true",
                        "spark.databricks.repl.allowedLanguages": "python,sql"
                      },
                      "spark_version": "${data.databricks_spark_version.latest.id}"
                    }
            - name: this
              manifest: |-
                {
                  "name": "experiments",
                  "s3": [
                    {
                      "bucket_name": "${aws_s3_bucket.this.bucket}",
                      "instance_profile": "${databricks_instance_profile.ds.id}"
                    }
                  ]
                }
              references:
                s3.bucket_name: aws_s3_bucket.this.bucket
                s3.instance_profile: databricks_instance_profile.ds.id
            - name: marketing
              manifest: |-
                {
                  "abfs": [
                    {
                      "client_id": "${data.azurerm_client_config.current.client_id}",
                      "client_secret_key": "${databricks_secret.service_principal_key.key}",
                      "client_secret_scope": "${databricks_secret_scope.terraform.name}",
                      "initialize_file_system": true
                    }
                  ],
                  "name": "marketing",
                  "resource_id": "${azurerm_storage_container.this.resource_manager_id}"
                }
              references:
                abfs.client_id: data.azurerm_client_config.current.client_id
                abfs.client_secret_key: databricks_secret.service_principal_key.key
                abfs.client_secret_scope: databricks_secret_scope.terraform.name
                resource_id: azurerm_storage_container.this.resource_manager_id
              dependencies:
                azurerm_role_assignment.this: |-
                    {
                      "principal_id": "${data.azurerm_client_config.current.object_id}",
                      "role_definition_name": "Storage Blob Data Contributor",
                      "scope": "${azurerm_storage_account.this.id}"
                    }
                azurerm_storage_account.this: |-
                    {
                      "account_kind": "StorageV2",
                      "account_replication_type": "GRS",
                      "account_tier": "Standard",
                      "is_hns_enabled": true,
                      "location": "${var.resource_group_location}",
                      "name": "${var.prefix}datalake",
                      "resource_group_name": "${var.resource_group_name}"
                    }
                azurerm_storage_container.this: |-
                    {
                      "container_access_type": "private",
                      "name": "marketing",
                      "storage_account_name": "${azurerm_storage_account.this.name}"
                    }
                databricks_secret.service_principal_key: |-
                    {
                      "key": "service_principal_key",
                      "scope": "${databricks_secret_scope.terraform.name}",
                      "string_value": "${var.ARM_CLIENT_SECRET}"
                    }
                databricks_secret_scope.terraform: |-
                    {
                      "initial_manage_principal": "users",
                      "name": "application"
                    }
            - name: this_gs
              manifest: |-
                {
                  "gs": [
                    {
                      "bucket_name": "mybucket",
                      "service_account": "acc@company.iam.gserviceaccount.com"
                    }
                  ],
                  "name": "gs-mount"
                }
            - name: mount
              manifest: |-
                {
                  "adl": [
                    {
                      "client_id": "${data.azurerm_client_config.current.client_id}",
                      "client_secret_key": "${databricks_secret.service_principal_key.key}",
                      "client_secret_scope": "${databricks_secret_scope.terraform.name}",
                      "spark_conf_prefix": "fs.adl",
                      "storage_resource_name": "{env.TEST_STORAGE_ACCOUNT_NAME}",
                      "tenant_id": "${data.azurerm_client_config.current.tenant_id}"
                    }
                  ],
                  "name": "{var.RANDOM}"
                }
              references:
                adl.client_id: data.azurerm_client_config.current.client_id
                adl.client_secret_key: databricks_secret.service_principal_key.key
                adl.client_secret_scope: databricks_secret_scope.terraform.name
                adl.tenant_id: data.azurerm_client_config.current.tenant_id
            - name: marketing
              manifest: |-
                {
                  "name": "marketing",
                  "wasb": [
                    {
                      "auth_type": "ACCESS_KEY",
                      "container_name": "${azurerm_storage_container.marketing.name}",
                      "storage_account_name": "${azurerm_storage_account.blobaccount.name}",
                      "token_secret_key": "${databricks_secret.storage_key.key}",
                      "token_secret_scope": "${databricks_secret_scope.terraform.name}"
                    }
                  ]
                }
              references:
                wasb.container_name: azurerm_storage_container.marketing.name
                wasb.storage_account_name: azurerm_storage_account.blobaccount.name
                wasb.token_secret_key: databricks_secret.storage_key.key
                wasb.token_secret_scope: databricks_secret_scope.terraform.name
              dependencies:
                azurerm_storage_account.blobaccount: |-
                    {
                      "account_kind": "StorageV2",
                      "account_replication_type": "LRS",
                      "account_tier": "Standard",
                      "location": "${var.resource_group_location}",
                      "name": "${var.prefix}blob",
                      "resource_group_name": "${var.resource_group_name}"
                    }
                azurerm_storage_container.marketing: |-
                    {
                      "container_access_type": "private",
                      "name": "marketing",
                      "storage_account_name": "${azurerm_storage_account.blobaccount.name}"
                    }
                databricks_secret.storage_key: |-
                    {
                      "key": "blob_storage_key",
                      "scope": "${databricks_secret_scope.terraform.name}",
                      "string_value": "${azurerm_storage_account.blobaccount.primary_access_key}"
                    }
                databricks_secret_scope.terraform: |-
                    {
                      "initial_manage_principal": "users",
                      "name": "application"
                    }
        argumentDocs:
            abfs: '- to mount ADLS Gen2 using Azure Blob Filesystem (ABFS) driver'
            abfs.client_id: '- (Required) (String) This is the client_id (Application Object ID) for the enterprise application for the service principal.'
            abfs.client_secret_key: '- (Required) (String) This is the secret key in which your service principal/enterprise app client secret will be stored.'
            abfs.client_secret_scope: '- (Required) (String) This is the secret scope in which your service principal/enterprise app client secret will be stored.'
            abfs.container_name: '- (Required) (String) ADLS gen2 container name. (Could be omitted if resource_id is provided)'
            abfs.directory: '- (Computed) (String) This is optional if you don''t want to add an additional directory that you wish to mount. This must start with a "/".'
            abfs.initialize_file_system: '- (Required) (Bool) either or not initialize FS for the first use'
            abfs.storage_account_name: '- (Required) (String) The name of the storage resource in which the data is. (Could be omitted if resource_id is provided)'
            abfs.tenant_id: '- (Optional) (String) This is your azure directory tenant id. It is required for creating the mount. (Could be omitted if Azure authentication is used, and we can extract tenant_id from it).'
            adl: '- to mount ADLS Gen1 using Azure Data Lake (ADL) driver'
            adl.client_id: '- (Required) (String) This is the client_id for the enterprise application for the service principal.'
            adl.client_secret_key: '- (Required) (String) This is the secret key in which your service principal/enterprise app client secret will be stored.'
            adl.client_secret_scope: '- (Required) (String) This is the secret scope in which your service principal/enterprise app client secret will be stored.'
            adl.directory: '- (Computed) (String) This is optional if you don''t want to add an additional directory that you wish to mount. This must start with a "/".'
            adl.spark_conf_prefix: '- (Optional) (String) This is the spark configuration prefix for adls gen 1 mount. The options are fs.adl, dfs.adls. Use fs.adl for runtime 6.0 and above for the clusters. Otherwise use dfs.adls. The default value is: fs.adl.'
            adl.storage_resource_name: '- (Required) (String) The name of the storage resource in which the data is for ADLS gen 1. This is what you are trying to mount. (Could be omitted if resource_id is provided)'
            adl.tenant_id: '- (Optional) (String) This is your azure directory tenant id. It is required for creating the mount. (Could be omitted if Azure authentication is used, and we can extract tenant_id from it)'
            bucket_name: for AWS S3 and Google Cloud Storage
            cluster_id: '- (Optional, String) Cluster to use for mounting. If no cluster is specified, a new cluster will be created and will mount the bucket for all of the clusters in this workspace. If the cluster is not running - it''s going to be started, so be aware to set auto-termination rules on it.'
            container_name: for ADLS Gen2 and Azure Blob Storage
            encryption_type: '- (Optional, String) encryption type. Currently used only for AWS S3 mounts'
            extra_configs: '- (Optional, String map) configuration parameters that are necessary for mounting of specific storage'
            gs: '- to mount Google Cloud Storage'
            gs.bucket_name: '- (Required) (String) GCS bucket name to be mounted.'
            gs.service_account: '- (Optional) (String) email of registered Google Service Account for data access.  If it''s not specified, then the cluster_id should be provided, and the cluster should have a Google service account attached to it.'
            id: '- mount name'
            mount_name: to name
            name: '- (Optional, String) Name, under which mount will be accessible in dbfs:/mnt/<MOUNT_NAME>. If not specified, provider will try to infer it from depending on the resource type:'
            resource_id: '- (Optional, String) resource ID for a given storage account. Could be used to fill defaults, such as storage account & container names on Azure.'
            s3: '- to mount AWS S3'
            s3.bucket_name: '- (Required) (String) S3 bucket name to be mounted.'
            s3.instance_profile: '- (Optional) (String) ARN of registered instance profile for data access.  If it''s not specified, then the cluster_id should be provided, and the cluster should have an instance profile attached to it. If both cluster_id & instance_profile are specified, then cluster_id takes precedence.'
            s3_bucket_name: to bucket_name
            source: '- (String) HDFS-compatible url'
            storage_resource_name: for ADLS Gen1
            uri: '- (Optional, String) the URI for accessing specific storage (s3a://...., abfss://...., gs://...., etc.)'
            wasb: '- to mount Azure Blob Storage using Windows Azure Storage Blob (WASB) driver'
            wasb.auth_type: '- (Required) (String) This is the auth type for blob storage. This can either be SAS tokens (SAS) or account access keys (ACCESS_KEY).'
            wasb.container_name: '- (Required) (String) The container in which the data is. This is what you are trying to mount. (Could be omitted if resource_id is provided)'
            wasb.directory: '- (Computed) (String) This is optional if you don''t want to add an additional directory that you wish to mount. This must start with a "/".'
            wasb.storage_account_name: '- (Required) (String) The name of the storage resource in which the data is. (Could be omitted if resource_id is provided)'
            wasb.token_secret_key: '- (Required) (String) This is the secret key in which your auth type token is stored.'
            wasb.token_secret_scope: '- (Required) (String) This is the secret scope in which your auth type token is stored.'
        importStatements: []
    databricks_mws_credentials:
        subCategory: Deployment
        description: '""subcategory: "Deployment"'
        name: databricks_mws_credentials
        title: databricks_mws_credentials Resource
        examples:
            - name: this
              manifest: |-
                {
                  "credentials_name": "${var.prefix}-creds",
                  "provider": "${databricks.mws}",
                  "role_arn": "${aws_iam_role.cross_account_role.arn}"
                }
              references:
                provider: databricks.mws
                role_arn: aws_iam_role.cross_account_role.arn
              dependencies:
                aws_iam_role.cross_account_role: |-
                    {
                      "assume_role_policy": "${data.databricks_aws_assume_role_policy.this.json}",
                      "name": "${var.prefix}-crossaccount",
                      "tags": "${var.tags}"
                    }
                aws_iam_role_policy.this: |-
                    {
                      "name": "${var.prefix}-policy",
                      "policy": "${data.databricks_aws_crossaccount_policy.this.json}",
                      "role": "${aws_iam_role.cross_account_role.id}"
                    }
        argumentDocs:
            account_id: '- (Deprecated) Maintained for backwards compatibility and will be removed in a later version. It should now be specified under a provider instance where host = "https://accounts.cloud.databricks.com"'
            creation_time: '- (Integer) time of credentials registration'
            credentials_id: '- (String) identifier of credentials'
            credentials_name: '- (Required) name of credentials to register'
            id: '- Canonical unique identifier for the mws credentials.'
            role_arn: '- (Required) ARN of cross-account role'
        importStatements: []
    databricks_mws_customer_managed_keys:
        subCategory: Deployment
        description: '""subcategory: "Deployment"'
        name: databricks_mws_customer_managed_keys
        title: databricks_mws_customer_managed_keys Resource
        examples:
            - name: managed_services
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "aws_key_info": [
                    {
                      "key_alias": "${aws_kms_alias.managed_services_customer_managed_key_alias.name}",
                      "key_arn": "${aws_kms_key.managed_services_customer_managed_key.arn}"
                    }
                  ],
                  "use_cases": [
                    "MANAGED_SERVICES"
                  ]
                }
              references:
                account_id: var.databricks_account_id
                aws_key_info.key_alias: aws_kms_alias.managed_services_customer_managed_key_alias.name
                aws_key_info.key_arn: aws_kms_key.managed_services_customer_managed_key.arn
              dependencies:
                aws_kms_alias.managed_services_customer_managed_key_alias: |-
                    {
                      "name": "alias/managed-services-customer-managed-key-alias",
                      "target_key_id": "${aws_kms_key.managed_services_customer_managed_key.key_id}"
                    }
                aws_kms_key.managed_services_customer_managed_key: |-
                    {
                      "policy": "${data.aws_iam_policy_document.databricks_managed_services_cmk.json}"
                    }
            - name: managed_services
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "gcp_key_info": [
                    {
                      "kms_key_id": "${var.cmek_resource_id}"
                    }
                  ],
                  "use_cases": [
                    "MANAGED_SERVICES"
                  ]
                }
              references:
                account_id: var.databricks_account_id
                gcp_key_info.kms_key_id: var.cmek_resource_id
            - name: storage
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "aws_key_info": [
                    {
                      "key_alias": "${aws_kms_alias.storage_customer_managed_key_alias.name}",
                      "key_arn": "${aws_kms_key.storage_customer_managed_key.arn}"
                    }
                  ],
                  "use_cases": [
                    "STORAGE"
                  ]
                }
              references:
                account_id: var.databricks_account_id
                aws_key_info.key_alias: aws_kms_alias.storage_customer_managed_key_alias.name
                aws_key_info.key_arn: aws_kms_key.storage_customer_managed_key.arn
              dependencies:
                aws_kms_alias.storage_customer_managed_key_alias: |-
                    {
                      "name": "alias/storage-customer-managed-key-alias",
                      "target_key_id": "${aws_kms_key.storage_customer_managed_key.key_id}"
                    }
                aws_kms_key.storage_customer_managed_key: |-
                    {
                      "policy": "${data.aws_iam_policy_document.databricks_storage_cmk.json}"
                    }
            - name: storage
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "gcp_key_info": [
                    {
                      "kms_key_id": "${var.cmek_resource_id}"
                    }
                  ],
                  "use_cases": [
                    "STORAGE"
                  ]
                }
              references:
                account_id: var.databricks_account_id
                gcp_key_info.kms_key_id: var.cmek_resource_id
        argumentDocs:
            MANAGED_SERVICES: '- for encryption of the workspace objects (notebooks, secrets) that are stored in the control plane'
            STORAGE: '- for encryption of the DBFS Storage & Cluster EBS Volumes'
            account_id: '- Account Id that could be found in the top right corner of Accounts Console'
            aws_key_info: '- This field is a block and is documented below. This conflicts with gcp_key_info'
            aws_key_info.key_alias: '- (Optional) The AWS KMS key alias.'
            aws_key_info.key_arn: '- The AWS KMS key''s Amazon Resource Name (ARN).'
            aws_key_info.key_region: '- (Optional) (Computed) The AWS region in which KMS key is deployed to. This is not required.'
            creation_time: '- (Integer) Time in epoch milliseconds when the customer key was created.'
            customer_managed_key_id: '- (String) ID of the encryption key configuration object.'
            gcp_key_info: '- This field is a block and is documented below. This conflicts with aws_key_info'
            gcp_key_info.kms_key_id: '- The GCP KMS key''s resource name.'
            id: '- Canonical unique identifier for the mws customer managed keys.'
            use_cases: '- (since v0.3.4) List of use cases for which this key will be used. If you''ve used the resource before, please add  Possible values are:'
            use_cases = ["MANAGED_SERVICES"]: to keep the previous behaviour.
        importStatements: []
    databricks_mws_log_delivery:
        subCategory: Log Delivery
        description: '""subcategory: "Log Delivery"'
        name: databricks_mws_log_delivery
        title: databricks_mws_log_delivery Resource
        examples:
            - name: usage_logs
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "config_name": "Usage Logs",
                  "credentials_id": "${databricks_mws_credentials.log_writer.credentials_id}",
                  "delivery_path_prefix": "billable-usage",
                  "log_type": "BILLABLE_USAGE",
                  "output_format": "CSV",
                  "storage_configuration_id": "${databricks_mws_storage_configurations.log_bucket.storage_configuration_id}"
                }
              references:
                account_id: var.databricks_account_id
                credentials_id: databricks_mws_credentials.log_writer.credentials_id
                storage_configuration_id: databricks_mws_storage_configurations.log_bucket.storage_configuration_id
              dependencies:
                aws_iam_role.logdelivery: |-
                    {
                      "assume_role_policy": "${data.databricks_aws_assume_role_policy.logdelivery.json}",
                      "description": "(${var.prefix}) UsageDelivery role",
                      "name": "${var.prefix}-logdelivery",
                      "tags": "${var.tags}"
                    }
                aws_s3_bucket.logdelivery: |-
                    {
                      "acl": "private",
                      "bucket": "${var.prefix}-logdelivery",
                      "force_destroy": true,
                      "tags": "${merge(var.tags, {\n    Name = \"${var.prefix}-logdelivery\"\n  })}"
                    }
                aws_s3_bucket_policy.logdelivery: |-
                    {
                      "bucket": "${aws_s3_bucket.logdelivery.id}",
                      "policy": "${data.databricks_aws_bucket_policy.logdelivery.json}"
                    }
                aws_s3_bucket_public_access_block.logdelivery: |-
                    {
                      "bucket": "${aws_s3_bucket.logdelivery.id}",
                      "ignore_public_acls": true
                    }
                aws_s3_bucket_versioning.logdelivery_versioning: |-
                    {
                      "bucket": "${aws_s3_bucket.logdelivery.id}",
                      "versioning_configuration": [
                        {
                          "status": "Disabled"
                        }
                      ]
                    }
                databricks_mws_credentials.log_writer: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "credentials_name": "Usage Delivery",
                      "role_arn": "${aws_iam_role.logdelivery.arn}"
                    }
                databricks_mws_storage_configurations.log_bucket: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "bucket_name": "${aws_s3_bucket.logdelivery.bucket}",
                      "storage_configuration_name": "Usage Logs"
                    }
            - name: audit_logs
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "config_name": "Audit Logs",
                  "credentials_id": "${databricks_mws_credentials.log_writer.credentials_id}",
                  "delivery_path_prefix": "audit-logs",
                  "log_type": "AUDIT_LOGS",
                  "output_format": "JSON",
                  "storage_configuration_id": "${databricks_mws_storage_configurations.log_bucket.storage_configuration_id}"
                }
              references:
                account_id: var.databricks_account_id
                credentials_id: databricks_mws_credentials.log_writer.credentials_id
                storage_configuration_id: databricks_mws_storage_configurations.log_bucket.storage_configuration_id
              dependencies:
                aws_iam_role.logdelivery: |-
                    {
                      "assume_role_policy": "${data.databricks_aws_assume_role_policy.logdelivery.json}",
                      "description": "(${var.prefix}) UsageDelivery role",
                      "name": "${var.prefix}-logdelivery",
                      "tags": "${var.tags}"
                    }
                aws_s3_bucket.logdelivery: |-
                    {
                      "acl": "private",
                      "bucket": "${var.prefix}-logdelivery",
                      "force_destroy": true,
                      "tags": "${merge(var.tags, {\n    Name = \"${var.prefix}-logdelivery\"\n  })}"
                    }
                aws_s3_bucket_policy.logdelivery: |-
                    {
                      "bucket": "${aws_s3_bucket.logdelivery.id}",
                      "policy": "${data.databricks_aws_bucket_policy.logdelivery.json}"
                    }
                aws_s3_bucket_public_access_block.logdelivery: |-
                    {
                      "bucket": "${aws_s3_bucket.logdelivery.id}",
                      "ignore_public_acls": true
                    }
                aws_s3_bucket_versioning.logdelivery_versioning: |-
                    {
                      "bucket": "${aws_s3_bucket.logdelivery.id}",
                      "versioning_configuration": [
                        {
                          "status": "Disabled"
                        }
                      ]
                    }
                databricks_mws_credentials.log_writer: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "credentials_name": "Usage Delivery",
                      "role_arn": "${aws_iam_role.logdelivery.arn}"
                    }
                databricks_mws_storage_configurations.log_bucket: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "bucket_name": "${aws_s3_bucket.logdelivery.bucket}",
                      "storage_configuration_name": "Usage Logs"
                    }
            - name: usage_logs
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "config_name": "Usage Logs",
                  "credentials_id": "${databricks_mws_credentials.log_writer.credentials_id}",
                  "delivery_path_prefix": "billable-usage",
                  "log_type": "BILLABLE_USAGE",
                  "output_format": "CSV",
                  "storage_configuration_id": "${databricks_mws_storage_configurations.log_bucket.storage_configuration_id}"
                }
              references:
                account_id: var.databricks_account_id
                credentials_id: databricks_mws_credentials.log_writer.credentials_id
                storage_configuration_id: databricks_mws_storage_configurations.log_bucket.storage_configuration_id
            - name: audit_logs
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "config_name": "Audit Logs",
                  "credentials_id": "${databricks_mws_credentials.log_writer.credentials_id}",
                  "delivery_path_prefix": "audit-logs",
                  "log_type": "AUDIT_LOGS",
                  "output_format": "JSON",
                  "storage_configuration_id": "${databricks_mws_storage_configurations.log_bucket.storage_configuration_id}"
                }
              references:
                account_id: var.databricks_account_id
                credentials_id: databricks_mws_credentials.log_writer.credentials_id
                storage_configuration_id: databricks_mws_storage_configurations.log_bucket.storage_configuration_id
        argumentDocs:
            account_id: '- Account Id that could be found in the top right corner of Accounts Console.'
            config_id: '- Databricks log delivery configuration ID.'
            config_name: '- The optional human-readable name of the log delivery configuration. Defaults to empty.'
            credentials_id: '- The ID for a Databricks credential configuration that represents the AWS IAM role with policy and trust relationship as described in the main billable usage documentation page.'
            delivery_path_prefix: '- (Optional) Defaults to empty, which means that logs are delivered to the root of the bucket. The value must be a valid S3 object key. It must not start or end with a slash character.'
            delivery_start_time: '- (Optional) The optional start month and year for delivery, specified in YYYY-MM format. Defaults to current year and month. Usage is not available before 2019-03.'
            id: '- the ID of log delivery configuration in form of account_id|config_id.'
            log_type: '- The type of log delivery. BILLABLE_USAGE and AUDIT_LOGS are supported.'
            output_format: '- The file type of log delivery. Currently CSV (for BILLABLE_USAGE) and JSON (for AUDIT_LOGS) are supported.'
            status: '- Status of log delivery configuration. Set to ENABLED or DISABLED. Defaults to ENABLED. This is the only field you can update.'
            storage_configuration_id: '- The ID for a Databricks storage configuration that represents the S3 bucket with bucket policy as described in the main billable usage documentation page.'
            workspace_ids_filter: '- (Optional) By default, this log configuration applies to all workspaces associated with your account ID. If your account is on the multitenant version of the platform or on a select custom plan that allows multiple workspaces per account, you may have multiple workspaces associated with your account ID. You can optionally set the field as mentioned earlier to an array of workspace IDs. If you plan to use different log delivery configurations for several workspaces, set this explicitly rather than leaving it blank. If you leave this blank and your account ID gets additional workspaces in the future, this configuration will also apply to the new workspaces.'
        importStatements: []
    databricks_mws_ncc_binding:
        subCategory: Deployment
        description: '""subcategory: "Deployment"'
        name: databricks_mws_ncc_binding
        title: databricks_mws_ncc_binding Resource
        examples:
            - name: ncc_binding
              manifest: |-
                {
                  "network_connectivity_config_id": "${databricks_mws_network_connectivity_config.ncc.network_connectivity_config_id}",
                  "provider": "${databricks.account}",
                  "workspace_id": "${var.databricks_workspace_id}"
                }
              references:
                network_connectivity_config_id: databricks_mws_network_connectivity_config.ncc.network_connectivity_config_id
                provider: databricks.account
                workspace_id: var.databricks_workspace_id
              dependencies:
                databricks_mws_network_connectivity_config.ncc: |-
                    {
                      "name": "ncc-for-${var.prefix}",
                      "provider": "${databricks.account}",
                      "region": "${var.region}"
                    }
        argumentDocs:
            network_connectivity_config_id: '- Canonical unique identifier of Network Connectivity Config in Databricks Account.'
            workspace_id: '- Identifier of the workspace to attach the NCC to. Change forces creation of a new resource.'
        importStatements: []
    databricks_mws_ncc_private_endpoint_rule:
        subCategory: Deployment
        description: '""subcategory: "Deployment"'
        name: databricks_mws_ncc_private_endpoint_rule
        title: databricks_mws_ncc_private_endpoint_rule Resource
        examples:
            - name: storage
              manifest: |-
                {
                  "group_id": "blob",
                  "network_connectivity_config_id": "${databricks_mws_network_connectivity_config.ncc.network_connectivity_config_id}",
                  "provider": "${databricks.account}",
                  "resource_id": "/subscriptions/653bb673-1234-abcd-a90b-d064d5d53ca4/resourcegroups/example-resource-group/providers/Microsoft.Storage/storageAccounts/examplesa"
                }
              references:
                network_connectivity_config_id: databricks_mws_network_connectivity_config.ncc.network_connectivity_config_id
                provider: databricks.account
              dependencies:
                databricks_mws_network_connectivity_config.ncc: |-
                    {
                      "name": "ncc-for-${var.prefix}",
                      "provider": "${databricks.account}",
                      "region": "${var.region}"
                    }
        argumentDocs:
            DISCONNECTED: ': Connection was removed by the private link resource owner, the private endpoint becomes informative and should be deleted for clean-up.'
            ESTABLISHED: ': The endpoint has been approved and is ready to be used in your serverless compute resources.'
            PENDING: ': The endpoint has been created and pending approval.'
            REJECTED: ': Connection was rejected by the private link resource owner.'
            connection_state: |-
                - The current status of this private endpoint. The private endpoint rules are effective only if the connection state is ESTABLISHED. Remember that you must approve new endpoints on your resources in the Azure portal before they take effect.
                The possible values are:
            creation_time: '- Time in epoch milliseconds when this object was created.'
            deactivated: '- Whether this private endpoint is deactivated.'
            deactivated_at: '- Time in epoch milliseconds when this object was deactivated.'
            endpoint_name: '- The name of the Azure private endpoint resource, e.g. "databricks-088781b3-77fa-4132-b429-1af0d91bc593-pe-3cb31234"'
            group_id: '- The sub-resource type (group ID) of the target resource. Must be one of blob, dfs, sqlServer or mysqlServer. Note that to connect to workspace root storage (root DBFS), you need two endpoints, one for blob and one for dfs. Change forces creation of a new resource.'
            network_connectivity_config_id: '- Canonical unique identifier of Network Connectivity Config in Databricks Account. Change forces creation of a new resource.'
            resource_id: '- The Azure resource ID of the target resource. Change forces creation of a new resource.'
            rule_id: '- the ID of a private endpoint rule.'
            updated_time: '- Time in epoch milliseconds when this object was updated.'
        importStatements: []
    databricks_mws_network_connectivity_config:
        subCategory: Deployment
        description: '""subcategory: "Deployment"'
        name: databricks_mws_network_connectivity_config
        title: databricks_mws_network_connectivity_config Resource
        examples:
            - name: ncc
              manifest: |-
                {
                  "name": "ncc-for-${var.prefix}",
                  "provider": "${databricks.account}",
                  "region": "${var.region}"
                }
              references:
                provider: databricks.account
                region: var.region
              dependencies:
                databricks_mws_ncc_binding.ncc_binding: |-
                    {
                      "network_connectivity_config_id": "${databricks_mws_network_connectivity_config.ncc.network_connectivity_config_id}",
                      "provider": "${databricks.account}",
                      "workspace_id": "${var.databricks_workspace_id}"
                    }
        argumentDocs:
            aws_stable_ip_rule: '(AWS only) - block with information about stable AWS IP CIDR blocks. You can use these to configure the firewall of your resources to allow traffic from your Databricks workspace.  Consists of the following fields:'
            azure_private_endpoint_rules: (Azure only) - list containing information about configure Azure Private Endpoints.
            azure_service_endpoint_rule: '(Azure only) - block with information about stable Azure service endpoints. You can configure the firewall of your Azure resources to allow traffic from your Databricks serverless compute resources.  Consists of the following fields:'
            cidr_blocks: '- list of IP CIDR blocks.'
            default_rules: '- block describing network connectivity rules that are applied by default without resource specific configurations.  Consists of the following fields:'
            egress_conf: '- block containing information about network connectivity rules that apply to network traffic from your serverless compute resources. Consists of the following fields:'
            id: '- combination of account_id and network_connectivity_config_id separated by / character'
            name: '- Name of Network Connectivity Config in Databricks Account. Change forces creation of a new resource.'
            network_connectivity_config_id: '- Canonical unique identifier of Network Connectivity Config in Databricks Account'
            region: '- Region of the Network Connectivity Config. NCCs can only be referenced by your workspaces in the same region. Change forces creation of a new resource.'
            subnets: '- list of subnets from which Databricks network traffic originates when accessing your Azure resources.'
            target_region: '- the Azure region in which this service endpoint rule applies.'
            target_rules: '- block describing network connectivity rules that configured for each destinations. These rules override default rules.  Consists of the following fields:'
            target_services: '- the Azure services to which this service endpoint rule applies to.'
        importStatements: []
    databricks_mws_networks:
        subCategory: Deployment
        description: '""subcategory: "Deployment"'
        name: databricks_mws_networks
        title: databricks_mws_networks Resource
        examples:
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "network_name": "${local.prefix}-network",
                  "provider": "${databricks.mws}",
                  "security_group_ids": [
                    "${module.vpc.default_security_group_id}"
                  ],
                  "subnet_ids": "${module.vpc.private_subnets}",
                  "vpc_id": "${module.vpc.vpc_id}"
                }
              references:
                account_id: var.databricks_account_id
                provider: databricks.mws
                subnet_ids: module.vpc.private_subnets
                vpc_id: module.vpc.vpc_id
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "depends_on": [
                    "${aws_vpc_endpoint.workspace}",
                    "${aws_vpc_endpoint.relay}"
                  ],
                  "network_name": "${local.prefix}-network",
                  "provider": "${databricks.mws}",
                  "security_group_ids": [
                    "${module.vpc.default_security_group_id}"
                  ],
                  "subnet_ids": "${module.vpc.private_subnets}",
                  "vpc_endpoints": [
                    {
                      "dataplane_relay": [
                        "${databricks_mws_vpc_endpoint.relay.vpc_endpoint_id}"
                      ],
                      "rest_api": [
                        "${databricks_mws_vpc_endpoint.workspace.vpc_endpoint_id}"
                      ]
                    }
                  ],
                  "vpc_id": "${module.vpc.vpc_id}"
                }
              references:
                account_id: var.databricks_account_id
                provider: databricks.mws
                subnet_ids: module.vpc.private_subnets
                vpc_id: module.vpc.vpc_id
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "gcp_network_info": [
                    {
                      "network_project_id": "${var.google_project}",
                      "pod_ip_range_name": "pods",
                      "service_ip_range_name": "svc",
                      "subnet_id": "${google_compute_subnetwork.network_with_private_secondary_ip_ranges.name}",
                      "subnet_region": "${google_compute_subnetwork.network_with_private_secondary_ip_ranges.region}",
                      "vpc_id": "${google_compute_network.dbx_private_vpc.name}"
                    }
                  ],
                  "network_name": "test-demo-${random_string.suffix.result}"
                }
              references:
                account_id: var.databricks_account_id
                gcp_network_info.network_project_id: var.google_project
                gcp_network_info.subnet_id: google_compute_subnetwork.network_with_private_secondary_ip_ranges.name
                gcp_network_info.subnet_region: google_compute_subnetwork.network_with_private_secondary_ip_ranges.region
                gcp_network_info.vpc_id: google_compute_network.dbx_private_vpc.name
              dependencies:
                google_compute_network.dbx_private_vpc: |-
                    {
                      "auto_create_subnetworks": false,
                      "name": "tf-network-${random_string.suffix.result}",
                      "project": "${var.google_project}"
                    }
                google_compute_router.router: |-
                    {
                      "name": "my-router-${random_string.suffix.result}",
                      "network": "${google_compute_network.dbx_private_vpc.id}",
                      "region": "${google_compute_subnetwork.network-with-private-secondary-ip-ranges.region}"
                    }
                google_compute_router_nat.nat: |-
                    {
                      "name": "my-router-nat-${random_string.suffix.result}",
                      "nat_ip_allocate_option": "AUTO_ONLY",
                      "region": "${google_compute_router.router.region}",
                      "router": "${google_compute_router.router.name}",
                      "source_subnetwork_ip_ranges_to_nat": "ALL_SUBNETWORKS_ALL_IP_RANGES"
                    }
                google_compute_subnetwork.network-with-private-secondary-ip-ranges: |-
                    {
                      "ip_cidr_range": "10.0.0.0/16",
                      "name": "test-dbx-${random_string.suffix.result}",
                      "network": "${google_compute_network.dbx_private_vpc.id}",
                      "private_ip_google_access": true,
                      "region": "us-central1",
                      "secondary_ip_range": [
                        {
                          "ip_cidr_range": "10.1.0.0/16",
                          "range_name": "pods"
                        },
                        {
                          "ip_cidr_range": "10.2.0.0/20",
                          "range_name": "svc"
                        }
                      ]
                    }
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "gcp_network_info": [
                    {
                      "network_project_id": "${var.google_project}",
                      "pod_ip_range_name": "pods",
                      "service_ip_range_name": "svc",
                      "subnet_id": "${google_compute_subnetwork.network_with_private_secondary_ip_ranges.name}",
                      "subnet_region": "${google_compute_subnetwork.network_with_private_secondary_ip_ranges.region}",
                      "vpc_id": "${google_compute_network.dbx_private_vpc.name}"
                    }
                  ],
                  "network_name": "test-demo-${random_string.suffix.result}",
                  "vpc_endpoints": [
                    {
                      "dataplane_relay": [
                        "${databricks_mws_vpc_endpoint.relay.vpc_endpoint_id}"
                      ],
                      "rest_api": [
                        "${databricks_mws_vpc_endpoint.workspace.vpc_endpoint_id}"
                      ]
                    }
                  ]
                }
              references:
                account_id: var.databricks_account_id
                gcp_network_info.network_project_id: var.google_project
                gcp_network_info.subnet_id: google_compute_subnetwork.network_with_private_secondary_ip_ranges.name
                gcp_network_info.subnet_region: google_compute_subnetwork.network_with_private_secondary_ip_ranges.region
                gcp_network_info.vpc_id: google_compute_network.dbx_private_vpc.name
        argumentDocs:
            account_id: '- Account Id that could be found in the top right corner of Accounts Console'
            gcp_network_info: '- (GCP only) a block consists of Google Cloud specific information for this network, for example the VPC ID, subnet ID, and secondary IP ranges. It has the following fields:'
            id: '- Canonical unique identifier for the mws networks.'
            network_id: '- (String) id of network to be used for databricks_mws_workspaces resource.'
            network_name: '- name under which this network is registered'
            network_project_id: '- The Google Cloud project ID of the VPC network.'
            pod_ip_range_name: '- The name of the secondary IP range for pods. A Databricks-managed GKE cluster uses this IP range for its pods. This secondary IP range can only be used by one workspace.'
            security_group_ids: '- (AWS only) ids of aws_security_group'
            service_ip_range_name: '- The name of the secondary IP range for services. A Databricks-managed GKE cluster uses this IP range for its services. This secondary IP range can only be used by one workspace.'
            subnet_id: '- The ID of the subnet associated with this network.'
            subnet_ids: '- (AWS only) ids of aws_subnet'
            subnet_region: '- The Google Cloud region of the workspace data plane. For example, us-east4.'
            vpc_endpoints: '- (Optional) mapping of databricks_mws_vpc_endpoint for PrivateLink or Private Service Connect connections'
            vpc_id: '- (AWS only) aws_vpc id'
            vpc_status: '- (String) VPC attachment status'
            workspace_id: '- (Integer) id of associated workspace'
        importStatements: []
    databricks_mws_permission_assignment:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_mws_permission_assignment
        title: databricks_mws_permission_assignment Resource
        examples:
            - name: add_admin_group
              manifest: |-
                {
                  "permissions": [
                    "ADMIN"
                  ],
                  "principal_id": "${databricks_group.data_eng.id}",
                  "workspace_id": "${databricks_mws_workspaces.this.workspace_id}"
                }
              references:
                principal_id: databricks_group.data_eng.id
                workspace_id: databricks_mws_workspaces.this.workspace_id
              dependencies:
                databricks_group.data_eng: |-
                    {
                      "display_name": "Data Engineering"
                    }
            - name: add_user
              manifest: |-
                {
                  "permissions": [
                    "USER"
                  ],
                  "principal_id": "${databricks_user.me.id}",
                  "workspace_id": "${databricks_mws_workspaces.this.workspace_id}"
                }
              references:
                principal_id: databricks_user.me.id
                workspace_id: databricks_mws_workspaces.this.workspace_id
              dependencies:
                databricks_user.me: |-
                    {
                      "user_name": "me@example.com"
                    }
            - name: add_admin_spn
              manifest: |-
                {
                  "permissions": [
                    "ADMIN"
                  ],
                  "principal_id": "${databricks_service_principal.sp.id}",
                  "workspace_id": "${databricks_mws_workspaces.this.workspace_id}"
                }
              references:
                principal_id: databricks_service_principal.sp.id
                workspace_id: databricks_mws_workspaces.this.workspace_id
              dependencies:
                databricks_service_principal.sp: |-
                    {
                      "display_name": "Automation-only SP"
                    }
        argumentDocs:
            '"ADMIN"': '- Can access the workspace and has workspace admin privileges to manage users and groups, workspace configurations, and more.'
            '"USER"': '- Can access the workspace with basic privileges.'
            id: '- ID of the permission assignment in form of workspace_id|principal_id.'
            permissions: '- The list of workspace permissions to assign to the principal:'
            principal_id: '- Databricks ID of the user, service principal, or group. The principal ID can be retrieved using the SCIM API, or using databricks_user, databricks_service_principal or databricks_group data sources.'
            workspace_id: '- Databricks workspace ID.'
        importStatements: []
    databricks_mws_private_access_settings:
        subCategory: Deployment
        description: '""subcategory: "Deployment"'
        name: databricks_mws_private_access_settings
        title: databricks_mws_private_access_settings Resource
        examples:
            - name: pas
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "private_access_settings_name": "Private Access Settings for ${local.prefix}",
                  "provider": "${databricks.mws}",
                  "public_access_enabled": true,
                  "region": "${var.region}"
                }
              references:
                account_id: var.databricks_account_id
                provider: databricks.mws
                region: var.region
        argumentDocs:
            allowed_vpc_endpoint_ids: '- (Optional) An array of databricks_mws_vpc_endpoint vpc_endpoint_id (not id). Only used when private_access_level is set to ENDPOINT. This is an allow list of databricks_mws_vpc_endpoint that in your account that can connect to your databricks_mws_workspaces over AWS PrivateLink. If hybrid access to your workspace is enabled by setting public_access_enabled to true, then this control only works for PrivateLink connections. To control how your workspace is accessed via public internet, see the article for databricks_ip_access_list.'
            id: '- the ID of the Private Access Settings in form of account_id/private_access_settings_id.'
            private_access_level: '- (Optional) The private access level controls which VPC endpoints can connect to the UI or API of any workspace that attaches this private access settings object. ACCOUNT level access (default) lets only databricks_mws_vpc_endpoint that are registered in your Databricks account connect to your databricks_mws_workspaces. ENDPOINT level access lets only specified databricks_mws_vpc_endpoint connect to your workspace. Please see the allowed_vpc_endpoint_ids documentation for more details.'
            private_access_settings_id: '- Canonical unique identifier of Private Access Settings in Databricks Account'
            private_access_settings_name: '- Name of Private Access Settings in Databricks Account'
            public_access_enabled: (Boolean, Optional, false by default on AWS, true by default on GCP) - If true, the databricks_mws_workspaces can be accessed over the databricks_mws_vpc_endpoint as well as over the public network. In such a case, you could also configure an databricks_ip_access_list for the workspace, to restrict the source networks that could be used to access it over the public network. If false, the workspace can be accessed only over VPC endpoints, and not over the public network. Once explicitly set, this field becomes mandatory.
            region: '- Region of AWS VPC or the Google Cloud VPC network'
            status: '- (AWS only) Status of Private Access Settings'
        importStatements: []
    databricks_mws_storage_configurations:
        subCategory: Deployment
        description: '""subcategory: "Deployment"'
        name: databricks_mws_storage_configurations
        title: databricks_mws_storage_configurations Resource
        examples:
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "bucket_name": "${aws_s3_bucket.root_storage_bucket.bucket}",
                  "provider": "${databricks.mws}",
                  "storage_configuration_name": "${var.prefix}-storage"
                }
              references:
                account_id: var.databricks_account_id
                bucket_name: aws_s3_bucket.root_storage_bucket.bucket
                provider: databricks.mws
              dependencies:
                aws_s3_bucket.root_storage_bucket: |-
                    {
                      "acl": "private",
                      "bucket": "${var.prefix}-rootbucket"
                    }
                aws_s3_bucket_versioning.root_versioning: |-
                    {
                      "bucket": "${aws_s3_bucket.root_storage_bucket.id}",
                      "versioning_configuration": [
                        {
                          "status": "Disabled"
                        }
                      ]
                    }
        argumentDocs:
            account_id: '- Account Id that could be found in the top right corner of Accounts Console'
            bucket_name: '- name of AWS S3 bucket'
            id: '- Canonical unique identifier for the mws storage configurations.'
            storage_configuration_id: '- (String) id of storage config to be used for databricks_mws_workspace resource.'
            storage_configuration_name: '- name under which this storage configuration is stored'
        importStatements: []
    databricks_mws_vpc_endpoint:
        subCategory: Deployment
        description: '""subcategory: "Deployment"'
        name: databricks_mws_vpc_endpoint
        title: databricks_mws_vpc_endpoint Resource
        examples:
            - name: workspace
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "aws_vpc_endpoint_id": "${aws_vpc_endpoint.workspace.id}",
                  "depends_on": [
                    "${aws_vpc_endpoint.workspace}"
                  ],
                  "provider": "${databricks.mws}",
                  "region": "${var.region}",
                  "vpc_endpoint_name": "VPC Relay for ${module.vpc.vpc_id}"
                }
              references:
                account_id: var.databricks_account_id
                aws_vpc_endpoint_id: aws_vpc_endpoint.workspace.id
                provider: databricks.mws
                region: var.region
            - name: relay
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "aws_vpc_endpoint_id": "${aws_vpc_endpoint.relay.id}",
                  "depends_on": [
                    "${aws_vpc_endpoint.relay}"
                  ],
                  "provider": "${databricks.mws}",
                  "region": "${var.region}",
                  "vpc_endpoint_name": "VPC Relay for ${module.vpc.vpc_id}"
                }
              references:
                account_id: var.databricks_account_id
                aws_vpc_endpoint_id: aws_vpc_endpoint.relay.id
                provider: databricks.mws
                region: var.region
            - name: workspace
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "gcp_vpc_endpoint_info": [
                    {
                      "endpoint_region": "${var.subnet_region}",
                      "project_id": "${var.google_project}",
                      "psc_endpoint_name": "PSC Rest API endpoint"
                    }
                  ],
                  "provider": "${databricks.mws}",
                  "vpc_endpoint_name": "PSC Rest API endpoint"
                }
              references:
                account_id: var.databricks_account_id
                gcp_vpc_endpoint_info.endpoint_region: var.subnet_region
                gcp_vpc_endpoint_info.project_id: var.google_project
                provider: databricks.mws
            - name: relay
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "gcp_vpc_endpoint_info": [
                    {
                      "endpoint_region": "${var.subnet_region}",
                      "project_id": "${var.google_project}",
                      "psc_endpoint_name": "PSC Relay endpoint"
                    }
                  ],
                  "provider": "${databricks.mws}",
                  "vpc_endpoint_name": "PSC Relay endpoint"
                }
              references:
                account_id: var.databricks_account_id
                gcp_vpc_endpoint_info.endpoint_region: var.subnet_region
                gcp_vpc_endpoint_info.project_id: var.google_project
                provider: databricks.mws
        argumentDocs:
            account_id: '- Account Id that could be found in the Accounts Console for AWS or GCP'
            aws_endpoint_service_id: '- (AWS Only) The ID of the Databricks endpoint service that this VPC endpoint is connected to. Please find the list of endpoint service IDs for each supported region in the Databricks PrivateLink documentation'
            aws_vpc_endpoint_id: '- (AWS only) ID of configured aws_vpc_endpoint'
            endpoint_region: '- Region of the PSC endpoint.'
            gcp_vpc_endpoint_info: '- (GCP only) a block consists of Google Cloud specific information for this PSC endpoint. It has the following fields:'
            id: '- the ID of VPC Endpoint in form of account_id/vpc_endpoint_id'
            project_id: '- The Google Cloud project ID of the VPC network where the PSC connection resides.'
            psc_connection_id: '- The unique ID of this PSC connection.'
            psc_endpoint_name: '- The name of the PSC endpoint in the Google Cloud project.'
            region: '- (AWS only) Region of AWS VPC'
            service_attachment_id: '- The service attachment this PSC connection connects to.'
            state: '- (AWS Only) State of VPC Endpoint'
            vpc_endpoint_id: '- Canonical unique identifier of VPC Endpoint in Databricks Account'
            vpc_endpoint_name: '- Name of VPC Endpoint in Databricks Account'
        importStatements: []
    databricks_mws_workspaces:
        subCategory: Deployment
        description: '""subcategory: "Deployment"'
        name: databricks_mws_workspaces
        title: databricks_mws_workspaces Resource
        examples:
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "aws_region": "${var.region}",
                  "credentials_id": "${databricks_mws_credentials.this.credentials_id}",
                  "network_id": "${databricks_mws_networks.this.network_id}",
                  "provider": "${databricks.mws}",
                  "storage_configuration_id": "${databricks_mws_storage_configurations.this.storage_configuration_id}",
                  "token": [
                    {}
                  ],
                  "workspace_name": "${var.prefix}"
                }
              references:
                account_id: var.databricks_account_id
                aws_region: var.region
                credentials_id: databricks_mws_credentials.this.credentials_id
                network_id: databricks_mws_networks.this.network_id
                provider: databricks.mws
                storage_configuration_id: databricks_mws_storage_configurations.this.storage_configuration_id
                workspace_name: var.prefix
              dependencies:
                databricks_mws_credentials.this: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "credentials_name": "${var.prefix}-creds",
                      "provider": "${databricks.mws}",
                      "role_arn": "${var.crossaccount_arn}"
                    }
                databricks_mws_networks.this: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "network_name": "${var.prefix}-network",
                      "provider": "${databricks.mws}",
                      "security_group_ids": [
                        "${var.security_group}"
                      ],
                      "subnet_ids": "${var.subnets_private}",
                      "vpc_id": "${var.vpc_id}"
                    }
                databricks_mws_storage_configurations.this: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "bucket_name": "${var.root_bucket}",
                      "provider": "${databricks.mws}",
                      "storage_configuration_name": "${var.prefix}-storage"
                    }
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "aws_region": "us-east-1",
                  "credentials_id": "${databricks_mws_credentials.this.credentials_id}",
                  "custom_tags": {
                    "SoldToCode": "1234"
                  },
                  "provider": "${databricks.mws}",
                  "storage_configuration_id": "${databricks_mws_storage_configurations.this.storage_configuration_id}",
                  "token": [
                    {}
                  ],
                  "workspace_name": "${local.prefix}"
                }
              references:
                account_id: var.databricks_account_id
                credentials_id: databricks_mws_credentials.this.credentials_id
                provider: databricks.mws
                storage_configuration_id: databricks_mws_storage_configurations.this.storage_configuration_id
                workspace_name: local.prefix
              dependencies:
                aws_iam_role.cross_account_role: |-
                    {
                      "assume_role_policy": "${data.databricks_aws_assume_role_policy.this.json}",
                      "name": "${local.prefix}-crossaccount",
                      "tags": "${var.tags}"
                    }
                aws_iam_role_policy.this: |-
                    {
                      "name": "${local.prefix}-policy",
                      "policy": "${data.databricks_aws_crossaccount_policy.this.json}",
                      "role": "${aws_iam_role.cross_account_role.id}"
                    }
                aws_s3_bucket.root_storage_bucket: |-
                    {
                      "acl": "private",
                      "bucket": "${local.prefix}-rootbucket",
                      "force_destroy": true,
                      "tags": "${var.tags}"
                    }
                aws_s3_bucket_policy.root_bucket_policy: |-
                    {
                      "bucket": "${aws_s3_bucket.root_storage_bucket.id}",
                      "depends_on": [
                        "${aws_s3_bucket_public_access_block.root_storage_bucket}"
                      ],
                      "policy": "${data.databricks_aws_bucket_policy.this.json}"
                    }
                aws_s3_bucket_public_access_block.root_storage_bucket: |-
                    {
                      "block_public_acls": true,
                      "block_public_policy": true,
                      "bucket": "${aws_s3_bucket.root_storage_bucket.id}",
                      "depends_on": [
                        "${aws_s3_bucket.root_storage_bucket}"
                      ],
                      "ignore_public_acls": true,
                      "restrict_public_buckets": true
                    }
                aws_s3_bucket_server_side_encryption_configuration.root_storage_bucket: |-
                    {
                      "bucket": "${aws_s3_bucket.root_storage_bucket.bucket}",
                      "rule": [
                        {
                          "apply_server_side_encryption_by_default": [
                            {
                              "sse_algorithm": "AES256"
                            }
                          ]
                        }
                      ]
                    }
                aws_s3_bucket_versioning.root_versioning: |-
                    {
                      "bucket": "${aws_s3_bucket.root_storage_bucket.id}",
                      "versioning_configuration": [
                        {
                          "status": "Disabled"
                        }
                      ]
                    }
                databricks_mws_credentials.this: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "credentials_name": "${local.prefix}-creds",
                      "provider": "${databricks.mws}",
                      "role_arn": "${aws_iam_role.cross_account_role.arn}"
                    }
                databricks_mws_storage_configurations.this: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "bucket_name": "${aws_s3_bucket.root_storage_bucket.bucket}",
                      "provider": "${databricks.mws}",
                      "storage_configuration_name": "${local.prefix}-storage"
                    }
                random_string.naming: |-
                    {
                      "length": 6,
                      "special": false,
                      "upper": false
                    }
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "cloud_resource_container": [
                    {
                      "gcp": [
                        {
                          "project_id": "${var.google_project}"
                        }
                      ]
                    }
                  ],
                  "gke_config": [
                    {
                      "connectivity_type": "PRIVATE_NODE_PUBLIC_MASTER",
                      "master_ip_range": "10.3.0.0/28"
                    }
                  ],
                  "location": "${var.subnet_region}",
                  "network_id": "${databricks_mws_networks.this.network_id}",
                  "token": [
                    {}
                  ],
                  "workspace_name": "${var.prefix}"
                }
              references:
                account_id: var.databricks_account_id
                cloud_resource_container.gcp.project_id: var.google_project
                location: var.subnet_region
                network_id: databricks_mws_networks.this.network_id
                workspace_name: var.prefix
              dependencies:
                databricks_mws_networks.this: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "gcp_network_info": [
                        {
                          "network_project_id": "${var.google_project}",
                          "pod_ip_range_name": "pods",
                          "service_ip_range_name": "svc",
                          "subnet_id": "${var.subnet_id}",
                          "subnet_region": "${var.subnet_region}",
                          "vpc_id": "${var.vpc_id}"
                        }
                      ],
                      "network_name": "${var.prefix}-network"
                    }
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "cloud_resource_container": [
                    {
                      "gcp": [
                        {
                          "project_id": "${data.google_client_config.current.project}"
                        }
                      ]
                    }
                  ],
                  "gke_config": [
                    {
                      "connectivity_type": "PRIVATE_NODE_PUBLIC_MASTER",
                      "master_ip_range": "10.3.0.0/28"
                    }
                  ],
                  "location": "${data.google_client_config.current.region}",
                  "provider": "${databricks.accounts}",
                  "token": [
                    {}
                  ],
                  "workspace_name": "${var.prefix}"
                }
              references:
                account_id: var.databricks_account_id
                cloud_resource_container.gcp.project_id: data.google_client_config.current.project
                location: data.google_client_config.current.region
                provider: databricks.accounts
                workspace_name: var.prefix
        argumentDocs:
            account_id: '- Account Id that could be found in the top right corner of Accounts Console.'
            aws_region: '- (AWS only) region of VPC.'
            cloud_resource_container: '- (GCP only) A block that specifies GCP workspace configurations, consisting of following blocks:'
            connectivity_type: ': Specifies the network connectivity types for the GKE nodes and the GKE master network. Possible values are: PRIVATE_NODE_PUBLIC_MASTER, PUBLIC_NODE_PUBLIC_MASTER.'
            creation_time: '- (Integer) time when workspace was created'
            custom_tags: '- (Optional / AWS only) - The custom tags key-value pairing that is attached to this workspace. These tags will be applied to clusters automatically in addition to any default_tags or custom_tags on a cluster level. Please note it can take up to an hour for custom_tags to be set due to scheduling on Control Plane. After custom tags are applied, they can be modified however they can never be completely removed.'
            deployment_name: '- (Optional) part of URL as in https://<prefix>-<deployment-name>.cloud.databricks.com. Deployment name cannot be used until a deployment name prefix is defined. Please contact your Databricks representative. Once a new deployment prefix is added/updated, it only will affect the new workspaces created.'
            gcp: '- A block that consists of the following field:'
            gcp_workspace_sa: '- (String, GCP only) identifier of a service account created for the workspace in form of db-<workspace-id>@prod-gcp-<region>.iam.gserviceaccount.com'
            gke_config: '- (GCP only) A block that specifies GKE configuration for the Databricks workspace:'
            id: '- (String) Canonical unique identifier for the workspace, of the format <account-id>/<workspace-id>'
            location: '- (GCP only) region of the subnet.'
            managed_services_customer_managed_key_id: '- (Optional) customer_managed_key_id from customer managed keys with use_cases set to MANAGED_SERVICES. This is used to encrypt the workspace''s notebook and secret data in the control plane.'
            master_ip_range: ': The IP range from which to allocate GKE cluster master resources. This field will be ignored if GKE private cluster is not enabled. It must be exactly as big as /28.'
            network_id: '- (Optional) network_id from networks.'
            pricing_tier: '- (Optional) - The pricing tier of the workspace.'
            private_access_settings_id: '- (Optional) Canonical unique identifier of databricks_mws_private_access_settings in Databricks Account.'
            project_id: '- The Google Cloud project ID, which the workspace uses to instantiate cloud resources for your workspace.'
            storage_configuration_id: '- (AWS only)storage_configuration_id from storage configuration.'
            storage_customer_managed_key_id: '- (Optional) customer_managed_key_id from customer managed keys with use_cases set to STORAGE. This is used to encrypt the DBFS Storage & Cluster Volumes.'
            token {}.comment: '- (Optional) Comment, that will appear in "User Settings / Access Tokens" page on Workspace UI. By default it''s "Terraform PAT".'
            token {}.lifetime_seconds: '- (Optional) Token expiry lifetime. By default its 2592000 (30 days).'
            workspace_id: '- (String) workspace id'
            workspace_name: '- name of the workspace, will appear on UI.'
            workspace_status: '- (String) workspace status'
            workspace_status_message: '- (String) updates on workspace status'
            workspace_url: '- (String) URL of the workspace'
        importStatements: []
    databricks_notebook:
        subCategory: Workspace
        description: '""subcategory: "Workspace"'
        name: databricks_notebook
        title: databricks_notebook Resource
        examples:
            - name: ddl
              manifest: |-
                {
                  "path": "${data.databricks_current_user.me.home}/AA/BB/CC",
                  "source": "${path.module}/DDLgen.py"
                }
            - name: notebook
              manifest: |-
                {
                  "content_base64": "${base64encode(\u003c\u003c-EOT\n    # created from ${abspath(path.module)}\n    display(spark.range(10))\n    EOT\n  )}",
                  "language": "PYTHON",
                  "path": "/Shared/Demo"
                }
            - name: lesson
              manifest: |-
                {
                  "path": "/Shared/Intro",
                  "source": "${path.module}/IntroNotebooks.dbc"
                }
        argumentDocs:
            content_base64: '- The base64-encoded notebook source code. Conflicts with source. Use of content_base64 is discouraged, as it''s increasing memory footprint of Terraform state and should only be used in exceptional circumstances, like creating a notebook with configuration properties for a data pipeline.'
            id: '-  Path of notebook on workspace'
            language: '-  (required with content_base64) One of SCALA, PYTHON, SQL, R.'
            object_id: '-  Unique identifier for a NOTEBOOK'
            path: '-  (Required) The absolute path of the notebook or directory, beginning with "/", e.g. "/Demo".'
            source: '- Path to notebook in source code format on local filesystem. Conflicts with content_base64.'
            url: '- Routable URL of the notebook'
            workspace_path: '- path on Workspace File System (WSFS) in form of /Workspace + path'
        importStatements: []
    databricks_notification_destination:
        subCategory: Workspace
        description: '""subcategory: "Workspace"'
        name: databricks_notification_destination
        title: databricks_notification_destination Resource
        examples:
            - name: ndresource
              manifest: |-
                {
                  "config": [
                    {
                      "email": [
                        {
                          "addresses": [
                            "abc@gmail.com"
                          ]
                        }
                      ]
                    }
                  ],
                  "display_name": "Notification Destination"
                }
            - name: ndresource
              manifest: |-
                {
                  "config": [
                    {
                      "slack": [
                        {
                          "url": "https://hooks.slack.com/services/..."
                        }
                      ]
                    }
                  ],
                  "display_name": "Notification Destination"
                }
            - name: ndresource
              manifest: |-
                {
                  "config": [
                    {
                      "pagerduty": [
                        {
                          "integration_key": "xxxxxx"
                        }
                      ]
                    }
                  ],
                  "display_name": "Notification Destination"
                }
            - name: ndresource
              manifest: |-
                {
                  "config": [
                    {
                      "microsoft_teams": [
                        {
                          "url": "https://outlook.office.com/webhook/..."
                        }
                      ]
                    }
                  ],
                  "display_name": "Notification Destination"
                }
            - name: ndresource
              manifest: |-
                {
                  "config": [
                    {
                      "generic_webhook": [
                        {
                          "password": "password",
                          "url": "https://example.com/webhook",
                          "username": "username"
                        }
                      ]
                    }
                  ],
                  "display_name": "Notification Destination"
                }
        argumentDocs:
            addresses: '- (Required) The list of email addresses to send notifications to.'
            config: '- (Required) The configuration of the Notification Destination. It must contain exactly one of the following blocks:'
            destination_type: '- the type of Notification Destination.'
            display_name: '- (Required) The display name of the Notification Destination.'
            email: '- The email configuration of the Notification Destination. It must contain the following:'
            generic_webhook: '- The Generic Webhook configuration of the Notification Destination. It must contain the following:'
            id: '- The unique ID of the Notification Destination.'
            integration_key: '- (Required) The PagerDuty integration key.'
            microsoft_teams: '- The Microsoft Teams configuration of the Notification Destination. It must contain the following:'
            pagerduty: '- The PagerDuty configuration of the Notification Destination. It must contain the following:'
            password: '- (Optional) The password for basic authentication.'
            slack: '- The Slack configuration of the Notification Destination. It must contain the following:'
            url: '- (Required) The Slack webhook URL.'
            username: '- (Optional) The username for basic authentication.'
        importStatements: []
    databricks_obo_token:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_obo_token
        title: databricks_obo_token Resource
        examples:
            - name: this
              manifest: |-
                {
                  "application_id": "${databricks_service_principal.this.application_id}",
                  "comment": "PAT on behalf of ${databricks_service_principal.this.display_name}",
                  "depends_on": [
                    "${databricks_permissions.token_usage}"
                  ],
                  "lifetime_seconds": 3600
                }
              references:
                application_id: databricks_service_principal.this.application_id
              dependencies:
                databricks_permissions.token_usage: |-
                    {
                      "access_control": [
                        {
                          "permission_level": "CAN_USE",
                          "service_principal_name": "${databricks_service_principal.this.application_id}"
                        }
                      ],
                      "authorization": "tokens"
                    }
                databricks_service_principal.this: |-
                    {
                      "display_name": "Automation-only SP"
                    }
            - name: this
              manifest: |-
                {
                  "application_id": "${databricks_service_principal.this.application_id}",
                  "comment": "PAT on behalf of ${databricks_service_principal.this.display_name}",
                  "depends_on": [
                    "${databricks_group_member.this}"
                  ],
                  "lifetime_seconds": 3600
                }
              references:
                application_id: databricks_service_principal.this.application_id
              dependencies:
                databricks_group_member.this: |-
                    {
                      "group_id": "${data.databricks_group.admins.id}",
                      "member_id": "${databricks_service_principal.this.id}"
                    }
                databricks_service_principal.this: |-
                    {
                      "display_name": "Terraform"
                    }
        argumentDocs:
            application_id: '- Application ID of databricks_service_principal to create a PAT token for.'
            comment: '- (String, Optional) Comment that describes the purpose of the token.'
            id: '- Canonical unique identifier for the token.'
            lifetime_seconds: '- (Integer, Optional) The number of seconds before the token expires. Token resource is re-created when it expires. If no lifetime is specified, the token remains valid indefinitely.'
            token_value: '- Sensitive value of the newly-created token.'
        importStatements: []
    databricks_online_table:
        subCategory: Unity Catalog
        description: '""subcategory: "Unity Catalog"'
        name: databricks_online_table
        title: databricks_online_table Resource
        examples:
            - name: this
              manifest: |-
                {
                  "name": "main.default.online_table",
                  "spec": [
                    {
                      "primary_key_columns": [
                        "id"
                      ],
                      "run_triggered": [
                        {}
                      ],
                      "source_table_full_name": "main.default.source_table"
                    }
                  ]
                }
        argumentDocs:
            detailed_state: '- The state of the online table.'
            id: '- The same as the name of the online table.'
            message: '- A text description of the current state of the online table.'
            name: '- (Required) 3-level name of the Online Table to create.'
            perform_full_copy: '- (Optional) Whether to create a full-copy pipeline -- a pipeline that stops after creates a full copy of the source table upon initialization and does not process any change data feeds (CDFs) afterwards. The pipeline can still be manually triggered afterwards, but it always perform a full copy of the source table and there are no incremental updates. This mode is useful for syncing views or tables without CDFs to online tables. Note that the full-copy pipeline only supports "triggered" scheduling policy.'
            pipeline_id: '- ID of the associated Delta Live Table pipeline.'
            primary_key_columns: '- (Required) list of the columns comprising the primary key.'
            run_continuously: '- empty block that specifies that pipeline runs continuously after generating the initial data.  Conflicts with run_triggered.'
            run_triggered: '- empty block that specifies that pipeline stops after generating the initial data and can be triggered later (manually, through a cron job or through data triggers).'
            source_table_full_name: '- (Required) full name of the source table.'
            spec: '- (Required) object containing specification of the online table:'
            status: '- object describing status of the online table:'
            table_serving_url: '- Data serving REST API URL for this table.'
            timeseries_key: '- (Optional) Time series key to deduplicate (tie-break) rows with the same primary key.'
            unity_catalog_provisioning_state: '- The provisioning state of the online table entity in Unity Catalog. This is distinct from the state of the data synchronization pipeline (i.e. the table may be in "ACTIVE" but the pipeline may be in "PROVISIONING" as it runs asynchronously).'
        importStatements: []
    databricks_permission_assignment:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_permission_assignment
        title: databricks_permission_assignment Resource
        examples:
            - name: add_user
              manifest: |-
                {
                  "permissions": [
                    "USER"
                  ],
                  "principal_id": "${data.databricks_user.me.id}",
                  "provider": "${databricks.workspace}"
                }
              references:
                principal_id: data.databricks_user.me.id
                provider: databricks.workspace
            - name: add_admin_spn
              manifest: |-
                {
                  "permissions": [
                    "ADMIN"
                  ],
                  "principal_id": "${data.databricks_service_principal.sp.id}",
                  "provider": "${databricks.workspace}"
                }
              references:
                principal_id: data.databricks_service_principal.sp.id
                provider: databricks.workspace
            - name: this
              manifest: |-
                {
                  "permissions": [
                    "USER"
                  ],
                  "principal_id": "${data.databricks_group.account_level.id}",
                  "provider": "${databricks.workspace}"
                }
              references:
                principal_id: data.databricks_group.account_level.id
                provider: databricks.workspace
        argumentDocs:
            '"ADMIN"': '- Can access the workspace and has workspace admin privileges to manage users and groups, workspace configurations, and more.'
            '"USER"': '- Can access the workspace with basic privileges.'
            id: '- ID of the permission assignment - same as principal_id.'
            permissions: '- The list of workspace permissions to assign to the principal:'
            principal_id: '- Databricks ID of the user, service principal, or group. The principal ID can be retrieved using the account-level SCIM API, or using databricks_user, databricks_service_principal or databricks_group data sources with account API (and has to be an account admin). A more sensible approach is to retrieve the list of principal_id as outputs from another Terraform stack.'
        importStatements: []
    databricks_permissions:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_permissions
        title: databricks_permissions Resource
        examples:
            - name: cluster_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_ATTACH_TO"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_RESTART"
                    },
                    {
                      "group_name": "${databricks_group.ds.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "cluster_id": "${databricks_cluster.shared_autoscaling.id}"
                }
              references:
                access_control.group_name: databricks_group.ds.display_name
                cluster_id: databricks_cluster.shared_autoscaling.id
              dependencies:
                databricks_cluster.shared_autoscaling: |-
                    {
                      "autoscale": [
                        {
                          "max_workers": 10,
                          "min_workers": 1
                        }
                      ],
                      "autotermination_minutes": 60,
                      "cluster_name": "Shared Autoscaling",
                      "node_type_id": "${data.databricks_node_type.smallest.id}",
                      "spark_version": "${data.databricks_spark_version.latest.id}"
                    }
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.ds: |-
                    {
                      "display_name": "Data Science"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: policy_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.ds.display_name}",
                      "permission_level": "CAN_USE"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_USE"
                    }
                  ],
                  "cluster_policy_id": "${databricks_cluster_policy.something_simple.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                cluster_policy_id: databricks_cluster_policy.something_simple.id
              dependencies:
                databricks_cluster_policy.something_simple: |-
                    {
                      "definition": "${jsonencode({\n    \"spark_conf.spark.hadoop.javax.jdo.option.ConnectionURL\" : {\n      \"type\" : \"forbidden\"\n    },\n    \"spark_conf.spark.secondkey\" : {\n      \"type\" : \"forbidden\"\n    }\n  })}",
                      "name": "Some simple policy"
                    }
                databricks_group.ds: |-
                    {
                      "display_name": "Data Science"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: pool_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_ATTACH_TO"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "instance_pool_id": "${databricks_instance_pool.this.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                instance_pool_id: databricks_instance_pool.this.id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_instance_pool.this: |-
                    {
                      "idle_instance_autotermination_minutes": 60,
                      "instance_pool_name": "Reserved Instances",
                      "max_capacity": 10,
                      "min_idle_instances": 0,
                      "node_type_id": "${data.databricks_node_type.smallest.id}"
                    }
            - name: job_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_VIEW"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_MANAGE_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    },
                    {
                      "permission_level": "IS_OWNER",
                      "service_principal_name": "${databricks_service_principal.aws_principal.application_id}"
                    }
                  ],
                  "job_id": "${databricks_job.this.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                access_control.service_principal_name: databricks_service_principal.aws_principal.application_id
                job_id: databricks_job.this.id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_job.this: |-
                    {
                      "max_concurrent_runs": 1,
                      "name": "Featurization",
                      "task": [
                        {
                          "new_cluster": [
                            {
                              "node_type_id": "${data.databricks_node_type.smallest.id}",
                              "num_workers": 300,
                              "spark_version": "${data.databricks_spark_version.latest.id}"
                            }
                          ],
                          "notebook_task": [
                            {
                              "notebook_path": "/Production/MakeFeatures"
                            }
                          ],
                          "task_key": "task1"
                        }
                      ]
                    }
                databricks_service_principal.aws_principal: |-
                    {
                      "display_name": "main"
                    }
            - name: dlt_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_VIEW"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "pipeline_id": "${databricks_pipeline.this.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                pipeline_id: databricks_pipeline.this.id
              dependencies:
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_notebook.dlt_demo: |-
                    {
                      "content_base64": "${base64encode(\u003c\u003c-EOT\n    import dlt\n    json_path = \"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"\n    @dlt.table(\n       comment=\"The raw wikipedia clickstream dataset, ingested from /databricks-datasets.\"\n    )\n    def clickstream_raw():\n        return (spark.read.format(\"json\").load(json_path))\n    EOT\n  )}",
                      "language": "PYTHON",
                      "path": "${data.databricks_current_user.me.home}/DLT_Demo"
                    }
                databricks_pipeline.this: |-
                    {
                      "configuration": {
                        "key1": "value1",
                        "key2": "value2"
                      },
                      "continuous": false,
                      "filters": [
                        {
                          "exclude": [
                            "com.databricks.exclude"
                          ],
                          "include": [
                            "com.databricks.include"
                          ]
                        }
                      ],
                      "library": [
                        {
                          "notebook": [
                            {
                              "path": "${databricks_notebook.dlt_demo.id}"
                            }
                          ]
                        }
                      ],
                      "name": "DLT Demo Pipeline (${data.databricks_current_user.me.alphanumeric})",
                      "storage": "/test/tf-pipeline"
                    }
            - name: notebook_usage_by_path
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "notebook_path": "${databricks_notebook.this.path}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                notebook_path: databricks_notebook.this.path
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_notebook.this: |-
                    {
                      "content_base64": "${base64encode(\"# Welcome to your Python notebook\")}",
                      "language": "PYTHON",
                      "path": "/Production/ETL/Features"
                    }
            - name: notebook_usage_by_id
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "notebook_id": "${databricks_notebook.this.object_id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                notebook_id: databricks_notebook.this.object_id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_notebook.this: |-
                    {
                      "content_base64": "${base64encode(\"# Welcome to your Python notebook\")}",
                      "language": "PYTHON",
                      "path": "/Production/ETL/Features"
                    }
            - name: workspace_file_usage_by_path
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "workspace_file_path": "${databricks_workspace_file.this.path}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                workspace_file_path: databricks_workspace_file.this.path
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_workspace_file.this: |-
                    {
                      "content_base64": "${base64encode(\"print('Hello World')\")}",
                      "path": "/Production/ETL/Features.py"
                    }
            - name: workspace_file_usage_by_id
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "workspace_file_id": "${databricks_workspace_file.this.object_id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                workspace_file_id: databricks_workspace_file.this.object_id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_workspace_file.this: |-
                    {
                      "content_base64": "${base64encode(\"print('Hello World')\")}",
                      "path": "/Production/ETL/Features.py"
                    }
            - name: folder_usage_by_path
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "directory_path": "${databricks_directory.this.path}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                directory_path: databricks_directory.this.path
              dependencies:
                databricks_directory.this: |-
                    {
                      "path": "/Production/ETL"
                    }
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: folder_usage_by_id
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "directory_id": "${databricks_directory.this.object_id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                directory_id: databricks_directory.this.object_id
              dependencies:
                databricks_directory.this: |-
                    {
                      "path": "/Production/ETL"
                    }
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: repo_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "repo_id": "${databricks_repo.this.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                repo_id: databricks_repo.this.id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_repo.this: |-
                    {
                      "url": "https://github.com/user/demo.git"
                    }
            - name: experiment_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_MANAGE"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "experiment_id": "${databricks_mlflow_experiment.this.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                experiment_id: databricks_mlflow_experiment.this.id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_mlflow_experiment.this: |-
                    {
                      "artifact_location": "dbfs:/tmp/my-experiment",
                      "description": "My MLflow experiment description",
                      "name": "${data.databricks_current_user.me.home}/Sample"
                    }
            - name: model_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_MANAGE_PRODUCTION_VERSIONS"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE_STAGING_VERSIONS"
                    }
                  ],
                  "registered_model_id": "${databricks_mlflow_model.this.registered_model_id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                registered_model_id: databricks_mlflow_model.this.registered_model_id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_mlflow_model.this: |-
                    {
                      "name": "SomePredictions"
                    }
            - name: ml_serving_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_VIEW"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_MANAGE"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_QUERY"
                    }
                  ],
                  "serving_endpoint_id": "${databricks_model_serving.this.serving_endpoint_id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                serving_endpoint_id: databricks_model_serving.this.serving_endpoint_id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_model_serving.this: |-
                    {
                      "config": [
                        {
                          "served_models": [
                            {
                              "model_name": "test",
                              "model_version": "1",
                              "name": "prod_model",
                              "scale_to_zero_enabled": true,
                              "workload_size": "Small"
                            }
                          ]
                        }
                      ],
                      "name": "tf-test"
                    }
            - name: password_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.guests.display_name}",
                      "permission_level": "CAN_USE"
                    }
                  ],
                  "authorization": "passwords"
                }
              references:
                access_control.group_name: databricks_group.guests.display_name
              dependencies:
                databricks_group.guests: |-
                    {
                      "display_name": "Guest Users"
                    }
            - name: token_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_USE"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_USE"
                    }
                  ],
                  "authorization": "tokens"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: endpoint_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_USE"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "sql_endpoint_id": "${databricks_sql_endpoint.this.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                sql_endpoint_id: databricks_sql_endpoint.this.id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_sql_endpoint.this: |-
                    {
                      "cluster_size": "Small",
                      "max_num_clusters": 1,
                      "name": "Endpoint of ${data.databricks_current_user.me.alphanumeric}",
                      "tags": [
                        {
                          "custom_tags": [
                            {
                              "key": "City",
                              "value": "Amsterdam"
                            }
                          ]
                        }
                      ]
                    }
            - name: dashboard_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "dashboard_id": "${databricks_dashboard.dashboard.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                dashboard_id: databricks_dashboard.dashboard.id
              dependencies:
                databricks_dashboard.dashboard: |-
                    {
                      "display_name": "TF New Dashboard"
                    }
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: sql_dashboard_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "sql_dashboard_id": "3244325"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: query_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "sql_query_id": "3244325"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: alert_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "sql_alert_id": "3244325"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: model_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    }
                  ],
                  "registered_model_id": "${databricks_mlflow_model.model.registered_model_id}"
                }
              references:
                registered_model_id: databricks_mlflow_model.model.registered_model_id
              dependencies:
                databricks_mlflow_model.model: |-
                    {
                      "description": "MLflow registered model",
                      "name": "example_model"
                    }
        argumentDocs:
            CAN_MANAGE: permission for items in the Workspace > Shared Icon Shared folder. You can grant CAN_MANAGE permission to notebooks and folders by moving them to the Shared Icon Shared folder.
            IS_OWNER: permission. Destroying databricks_permissions resource for a job would revert ownership to the creator.
            admins.group_name: '- (Optional) name of the group. We recommend setting permissions on groups.'
            admins.permission_level: '- (Required) permission level according to specific resource. See examples above for the reference.'
            admins.service_principal_name: '- (Optional) Application ID of the service_principal.'
            admins.user_name: '- (Optional) name of the user.'
            authorization: '- either tokens or passwords.'
            cluster_id: '- cluster id'
            cluster_policy_id: '- cluster policy id'
            directory_id: '- directory id'
            directory_path: '- path of directory'
            experiment_id: '- MLflow experiment id'
            id: '- Canonical unique identifier for the permissions in form of /<object type>/<object id>.'
            instance_pool_id: '- instance pool id'
            job_id: '- job id'
            notebook_id: '- ID of notebook within workspace'
            notebook_path: '- path of notebook'
            object_type: '- type of permissions.'
            pipeline_id: '- pipeline id'
            registered_model_id: '- MLflow registered model id'
            repo_id: '- repo id'
            repo_path: '- path of databricks repo directory(/Repos/<username>/...)'
            serving_endpoint_id: '- Model Serving endpoint id.'
            sql_alert_id: '- SQL alert id'
            sql_dashboard_id: '- SQL dashboard id'
            sql_endpoint_id: '- SQL warehouse id'
            sql_query_id: '- SQL query id'
        importStatements: []
    databricks_pipeline:
        subCategory: Compute
        description: '""subcategory: "Compute"'
        name: databricks_pipeline
        title: databricks_pipeline Resource
        examples:
            - name: this
              manifest: |-
                {
                  "cluster": [
                    {
                      "custom_tags": {
                        "cluster_type": "default"
                      },
                      "label": "default",
                      "num_workers": 2
                    },
                    {
                      "custom_tags": {
                        "cluster_type": "maintenance"
                      },
                      "label": "maintenance",
                      "num_workers": 1
                    }
                  ],
                  "configuration": {
                    "key1": "value1",
                    "key2": "value2"
                  },
                  "continuous": false,
                  "library": [
                    {
                      "notebook": [
                        {
                          "path": "${databricks_notebook.dlt_demo.id}"
                        }
                      ]
                    },
                    {
                      "file": [
                        {
                          "path": "${databricks_repo.dlt_demo.path}/pipeline.sql"
                        }
                      ]
                    }
                  ],
                  "name": "Pipeline Name",
                  "notification": [
                    {
                      "alerts": [
                        "on-update-failure",
                        "on-update-fatal-failure",
                        "on-update-success",
                        "on-flow-failure"
                      ],
                      "email_recipients": [
                        "user@domain.com",
                        "user1@domain.com"
                      ]
                    }
                  ],
                  "storage": "/test/first-pipeline"
                }
              references:
                library.notebook.path: databricks_notebook.dlt_demo.id
              dependencies:
                databricks_notebook.dlt_demo: '{}'
                databricks_repo.dlt_demo: '{}'
        argumentDocs:
            alerts: (Required) non-empty list of alert types. Right now following alert types are supported, consult documentation for actual list
            allow_duplicate_names: '- Optional boolean flag. If false, deployment will fail if name conflicts with that of another pipeline. default is false.'
            budget_policy_id: '- optional string specifying ID of the budget policy for this DLT pipeline.'
            catalog: '- The name of catalog in Unity Catalog. Change of this parameter forces recreation of the pipeline. (Conflicts with storage).'
            channel: '- optional name of the release channel for Spark version used by DLT pipeline.  Supported values are: CURRENT (default) and PREVIEW.'
            cluster: blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. Please note that DLT pipeline clusters are supporting only subset of attributes as described in   Also, note that autoscale block is extended with the mode parameter that controls the autoscaling algorithm (possible values are ENHANCED for new, enhanced autoscaling algorithm, or LEGACY for old algorithm).
            configuration: '- An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.'
            connection_id: '- Immutable. The Unity Catalog connection this gateway pipeline uses to communicate with the source.'
            continuous: '- A flag indicating whether to run the pipeline continuously. The default value is false.'
            deployment: '- Deployment type of this pipeline. Supports following attributes:'
            development: '- A flag indicating whether to run the pipeline in development mode. The default value is false.'
            edition: '- optional name of the product edition. Supported values are: CORE, PRO, ADVANCED (default).  Not required when serverless is set to true.'
            email_recipients: (Required) non-empty list of emails to notify.
            exclude: '- Paths to exclude.'
            filters: '- Filters on which Pipeline packages to include in the deployed graph.  This block consists of following attributes:'
            gateway_definition: '- The definition of a gateway pipeline to support CDC. Consists of following attributes:'
            gateway_storage_catalog: '- Required, Immutable. The name of the catalog for the gateway pipeline''s storage location.'
            gateway_storage_name: '- Required. The Unity Catalog-compatible naming for the gateway storage location. This is the destination to use for the data that is extracted by the gateway. Delta Live Tables system will automatically create the storage location under the catalog and schema.'
            gateway_storage_schema: '- Required, Immutable. The name of the schema for the gateway pipelines''s storage location.'
            id: '- Canonical unique identifier of the DLT pipeline.'
            include: '- Paths to include.'
            kind: '- The deployment method that manages the pipeline.'
            library: blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special notebook & file library types that should have the path attribute. Right now only the
            library.connection_name: '- Immutable. The Unity Catalog connection this ingestion pipeline uses to communicate with the source. Specify either ingestion_gateway_id or connection_name.'
            library.ingestion_gateway_id: '- Immutable. Identifier for the ingestion gateway used by this ingestion pipeline to communicate with the source. Specify either ingestion_gateway_id or connection_name.'
            library.objects: '- Required. Settings specifying tables to replicate and the destination for the replicated tables.'
            library.table_configuration: '- Configuration settings to control the ingestion of tables. These settings are applied to all tables in the pipeline.'
            metadata_file_path: '- The path to the file containing metadata about the deployment.'
            name: '- A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.'
            notebook: '& file types are supported.'
            on-flow-failure: '- a single data flow fails.'
            on-update-failure: '- a pipeline update fails with a retryable error.'
            on-update-fatal-failure: '- a pipeline update fails with a non-retryable (fatal) error.'
            on-update-success: '- a pipeline update completes successfully.'
            photon: '- A flag indicating whether to use Photon engine. The default value is false.'
            schema: '- (Optional, String, Conflicts with target) The default schema (database) where tables are read from or published to. The presence of this attribute implies that the pipeline is in direct publishing mode.'
            serverless: '- An optional flag indicating if serverless compute should be used for this DLT pipeline.  Requires catalog to be set, as it could be used only with Unity Catalog.'
            storage: '- A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. Change of this parameter forces recreation of the pipeline. (Conflicts with catalog).'
            target: '- (Optional, String, Conflicts with schema) The name of a database (in either the Hive metastore or in a UC catalog) for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.'
            url: '- URL of the DLT pipeline on the given workspace.'
        importStatements: []
    databricks_provider:
        subCategory: Delta Sharing
        description: '""subcategory: "Delta Sharing"'
        name: databricks_provider
        title: databricks_provider Resource
        examples:
            - name: dbprovider
              manifest: |-
                {
                  "authentication_type": "TOKEN",
                  "comment": "made by terraform 2",
                  "name": "terraform-test-provider",
                  "recipient_profile_str": "${jsonencode(\n    {\n      \"shareCredentialsVersion\" : 1,\n      \"bearerToken\" : \"token\",\n      \"endpoint\" : \"endpoint\",\n      \"expirationTime\" : \"expiration-time\"\n    }\n  )}"
                }
        argumentDocs:
            authentication_type: '- (Optional) The delta sharing authentication type. Valid values are TOKEN.'
            comment: '- (Optional) Description about the provider.'
            id: '- ID of this provider - same as the name.'
            name: '- Name of provider. Change forces creation of a new resource.'
            recipient_profile_str: '- (Optional) This is the json file that is created from a recipient url.'
        importStatements: []
    databricks_quality_monitor:
        subCategory: Unity Catalog
        description: '""subcategory: "Unity Catalog"'
        name: databricks_quality_monitor
        title: databricks_quality_monitor Resource
        examples:
            - name: testTimeseriesMonitor
              manifest: |-
                {
                  "assets_dir": "/Shared/provider-test/databricks_quality_monitoring/${databricks_sql_table.myTestTable.name}",
                  "output_schema_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}",
                  "table_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}.${databricks_sql_table.myTestTable.name}",
                  "time_series": [
                    {
                      "granularities": [
                        "1 hour"
                      ],
                      "timestamp_col": "timestamp"
                    }
                  ]
                }
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.id}",
                      "comment": "this database is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
                databricks_sql_table.myTestTable: |-
                    {
                      "catalog_name": "main",
                      "column": [
                        {
                          "name": "timestamp",
                          "type": "int"
                        }
                      ],
                      "data_source_format": "DELTA",
                      "name": "bar",
                      "schema_name": "${databricks_schema.things.name}",
                      "table_type": "MANAGED"
                    }
            - name: testMonitorInference
              manifest: |-
                {
                  "assets_dir": "/Shared/provider-test/databricks_quality_monitoring/${databricks_table.myTestTable.name}",
                  "inference_log": [
                    {
                      "granularities": [
                        "1 hour"
                      ],
                      "model_id_col": "model_id",
                      "prediction_col": "prediction",
                      "problem_type": "PROBLEM_TYPE_REGRESSION",
                      "timestamp_col": "timestamp"
                    }
                  ],
                  "output_schema_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}",
                  "table_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}.${databricks_table.myTestTable.name}"
                }
            - name: testMonitorInference
              manifest: |-
                {
                  "assets_dir": "/Shared/provider-test/databricks_quality_monitoring/${databricks_table.myTestTable.name}",
                  "output_schema_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}",
                  "snapshot": [
                    {}
                  ],
                  "table_name": "${databricks_catalog.sandbox.name}.${databricks_schema.things.name}.${databricks_table.myTestTable.name}"
                }
        argumentDocs:
            assets_dir: '- (Required) - The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)'
            baseline_table_name: |-
                - Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline
                table.
            custom_metrics: '- Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).'
            dashboard_id: '- The ID of the generated dashboard.'
            data_classification_config: '- The data classification config for the monitor'
            definition: '- create metric definition'
            drift_metrics_table_name: '- The full name of the drift metrics table. Format: catalog_name.schema_name.table_name.'
            granularities: '-  List of granularities to use when aggregating data into time windows based on their timestamp.'
            id: '-  ID of this monitor is the same as the full table name of the format {catalog}.{schema_name}.{table_name}'
            inference_log: '- Configuration for the inference log monitor'
            input_columns: '- Columns on the monitored table to apply the custom metrics to.'
            label_col: '- Column of the model label'
            model_id_col: '- Column of the model id or version'
            monitor_version: '- The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted'
            name: '- Name of the custom metric.'
            notifications: '- The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name email_addresses containing a list of emails to notify:'
            on_failure: '- who to send notifications to on monitor failure.'
            on_new_classification_tag_detected: '- Who to send notifications to when new data classification tags are detected.'
            output_data_type: '- The output type of the custom metric.'
            output_schema_name: '- (Required) - Schema where output metric tables are created'
            prediction_col: '- Column of the model prediction'
            prediction_proba_col: '- Column of the model prediction probabilities'
            problem_type: '- Problem type the model aims to solve. Either PROBLEM_TYPE_CLASSIFICATION or PROBLEM_TYPE_REGRESSION'
            profile_metrics_table_name: '- The full name of the profile metrics table. Format: catalog_name.schema_name.table_name.'
            quartz_cron_expression: '- string expression that determines when to run the monitor. See Quartz documentation for examples.'
            schedule: '- The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:'
            skip_builtin_dashboard: '- Whether to skip creating a default dashboard summarizing data quality metrics.'
            slicing_exprs: '- List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.'
            snapshot: '- Configuration for monitoring snapshot tables.'
            status: '- Status of the Monitor'
            table_name: '- (Required) - The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}'
            time_series: '- Configuration for monitoring timeseries tables.'
            timestamp_col: '- Column of the timestamp of predictions'
            timezone_id: '- string with timezone id (e.g., PST) in which to evaluate the Quartz expression.'
            type: '- The type of the custom metric.'
            warehouse_id: '- Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.'
        importStatements: []
    databricks_query:
        subCategory: Databricks SQL
        description: '""subcategory: "Databricks SQL"'
        name: databricks_query
        title: databricks_query Resource
        examples:
            - name: this
              manifest: |-
                {
                  "display_name": "My Query Name",
                  "parent_path": "${databricks_directory.shared_dir.path}",
                  "query_text": "SELECT 42 as value",
                  "warehouse_id": "${databricks_sql_endpoint.example.id}"
                }
              references:
                parent_path: databricks_directory.shared_dir.path
                warehouse_id: databricks_sql_endpoint.example.id
              dependencies:
                databricks_directory.shared_dir: |-
                    {
                      "path": "/Shared/Queries"
                    }
            - name: query
              manifest: |-
                {
                  "display_name": "My Query",
                  "parameter": [
                    {
                      "name": "p1",
                      "text_value": [
                        {
                          "value": "default"
                        }
                      ],
                      "title": "Title for p1"
                    }
                  ],
                  "parent_path": "${databricks_directory.shared_dir.path}",
                  "query_text": "select 42 as value",
                  "warehouse_id": "${databricks_sql_endpoint.example.id}"
                }
              references:
                parent_path: databricks_directory.shared_dir.path
                warehouse_id: databricks_sql_endpoint.example.id
        argumentDocs:
            apply_auto_limit: '- (Optional, Boolean) Whether to apply a 1000 row limit to the query result.'
            catalog: '- (Optional, String) Name of the catalog where this query will be executed.'
            create_time: '- The timestamp string indicating when the query was created.'
            databricks_permissions: .
            databricks_sql_query: ', for example, by executing the terraform state show databricks_sql_query.query command.'
            date_range_value: '- (Block) Date-range query parameter value. Consists of following attributes (Can only specify one of dynamic_date_range_value or date_range_value):'
            date_value: '- (Block) Date query parameter value. Consists of following attributes (Can only specify one of dynamic_date_value or date_value):'
            description: '- (Optional, String) General description that conveys additional information about this query such as usage notes.'
            display_name: '- (Required, String) Name of the query.'
            dynamic_date_range_value: '- (String) Dynamic date-time range value based on current date-time.  Possible values are TODAY, YESTERDAY, THIS_WEEK, THIS_MONTH, THIS_YEAR, LAST_WEEK, LAST_MONTH, LAST_YEAR, LAST_HOUR, LAST_8_HOURS, LAST_24_HOURS, LAST_7_DAYS, LAST_14_DAYS, LAST_30_DAYS, LAST_60_DAYS, LAST_90_DAYS, LAST_12_MONTHS.'
            dynamic_date_value: '- (String) Dynamic date-time value based on current date-time.  Possible values are NOW, YESTERDAY.'
            end: (Required, String) - end of the date range.
            enum_options: '- (String) List of valid query parameter values, newline delimited.'
            enum_value: '- (Block) Dropdown parameter value. Consists of following attributes:'
            id: '- unique ID of the created Query.'
            last_modifier_user_name: '- Username of the user who last saved changes to this query.'
            lifecycle_state: '- The workspace state of the query. Used for tracking trashed status. (Possible values are ACTIVE or TRASHED).'
            multi_values_options: '- (Optional, Block) If specified, allows multiple values to be selected for this parameter. Consists of following attributes:'
            name: '- (Required, String) Literal parameter marker that appears between double curly braces in the query text.'
            numeric_value: '-  (Block) Numeric parameter value. Consists of following attributes:'
            owner_user_name: '- (Optional, String) Query owner''s username.'
            parameter: '- (Optional, Block) Query parameter definition.  Consists of following attributes (one of *_value is required):'
            parent: (if exists) is renamed to parent_path attribute, and should be converted from folders/object_id to the actual path.
            parent_path: '- (Optional, String) The path to a workspace folder containing the query. The default is the user''s home folder.  If changed, the query will be recreated.'
            precision: '- (Optional, String) Date-time precision to format the value into when the query is run.  Possible values are DAY_PRECISION, MINUTE_PRECISION, SECOND_PRECISION.  Defaults to DAY_PRECISION (YYYY-MM-DD).'
            prefix: '- (Optional, String) Character that prefixes each selected parameter value.'
            query_backed_value: '- (Block) Query-based dropdown parameter value. Consists of following attributes:'
            query_id: '- (Required, String) ID of the query that provides the parameter values.'
            query_text: '- (Required, String) Text of SQL query.'
            run_as_mode: '- (Optional, String) Sets the "Run as" role for the object.'
            schema: '- (Optional, String) Name of the schema where this query will be executed.'
            separator: '- (Optional, String) Character that separates each selected parameter value. Defaults to a comma.'
            start: (Required, String) - begin of the date range.
            start_day_of_week: '- (Optional, Int) Specify what day that starts the week.'
            suffix: '- (Optional, String) Character that suffixes each selected parameter value.'
            tags: '- (Optional, List of strings) Tags that will be added to the query.'
            terraform import databricks_query.query <query-id>: command.
            terraform import.databricks_permissions: .
            terraform import.import: 'and removed blocks like this:'
            terraform import.terraform apply: command to apply changes.
            terraform import.terraform plan: command to check possible changes, such as value type change, etc.
            terraform plan: command to check possible changes, such as value type change, etc.
            terraform state rm databricks_sql_query.query: command.
            text_value: '- (Block) Text parameter value. Consists of following attributes:'
            title: '- (Optional, String) Text displayed in the user-facing parameter widget in the UI.'
            update_time: '- The timestamp string indicating when the query was updated.'
            value: '- (Required, String) - actual text value.'
            values: '- (Array of strings) List of selected query parameter values.'
            warehouse_id: '- (Required, String) ID of a SQL warehouse which will be used to execute this query.'
        importStatements: []
    databricks_recipient:
        subCategory: Delta Sharing
        description: '""subcategory: "Delta Sharing"'
        name: databricks_recipient
        title: databricks_recipient Resource
        examples:
            - name: db2open
              manifest: |-
                {
                  "authentication_type": "TOKEN",
                  "comment": "made by terraform",
                  "ip_access_list": [
                    {
                      "allowed_ip_addresses": []
                    }
                  ],
                  "name": "${data.databricks_current_user.current.alphanumeric}-recipient",
                  "sharing_code": "${random_password.db2opensharecode.result}"
                }
              references:
                sharing_code: random_password.db2opensharecode.result
              dependencies:
                random_password.db2opensharecode: |-
                    {
                      "length": 16,
                      "special": true
                    }
            - name: db2db
              manifest: |-
                {
                  "authentication_type": "DATABRICKS",
                  "comment": "made by terraform",
                  "data_recipient_global_metastore_id": "${databricks_metastore.recipient_metastore.global_metastore_id}",
                  "name": "${data.databricks_current_user.current.alphanumeric}-recipient"
                }
              references:
                data_recipient_global_metastore_id: databricks_metastore.recipient_metastore.global_metastore_id
              dependencies:
                databricks_metastore.recipient_metastore: |-
                    {
                      "delta_sharing_recipient_token_lifetime_in_seconds": "60000000",
                      "delta_sharing_scope": "INTERNAL",
                      "force_destroy": true,
                      "name": "recipient",
                      "storage_root": "${format(\"abfss://%s@%s.dfs.core.windows.net/\",\n    azurerm_storage_container.unity_catalog.name,\n  azurerm_storage_account.unity_catalog.name)}"
                    }
        argumentDocs:
            activation_url: '- Full activation URL to retrieve the access token. It will be empty if the token is already retrieved.'
            authentication_type: '- (Optional) The delta sharing authentication type. Valid values are TOKEN and DATABRICKS.'
            cloud: '- Cloud vendor of the recipient''s Unity Catalog Metstore. This field is only present when the authentication_type is DATABRICKS.'
            comment: '- (Optional) Description about the recipient.'
            created_at: '- Time at which this recipient Token was created, in epoch milliseconds.'
            created_by: '- Username of recipient token creator.'
            data_recipient_global_metastore_id: '- Required when authentication_type is DATABRICKS.'
            expiration_time: '- Expiration timestamp of the token in epoch milliseconds.'
            id: '- the ID of the recipient - the same as the name.'
            ip_access_list: '- (Optional) Recipient IP access list.'
            ip_access_list.allowed_ip_addresses: '- Allowed IP Addresses in CIDR notation. Limit of 100.'
            metastore_id: '- Unique identifier of recipient''s Unity Catalog metastore. This field is only present when the authentication_type is DATABRICKS.'
            name: '- Name of recipient. Change forces creation of a new resource.'
            owner: '- (Optional) Username/groupname/sp application_id of the recipient owner.'
            properties: (Required) a map of string key-value pairs with recipient's properties.  Properties with name starting with databricks. are reserved.
            properties_kvpairs: '- (Optional) Recipient properties - object consisting of following fields:'
            region: '- Cloud region of the recipient''s Unity Catalog Metstore. This field is only present when the authentication_type is DATABRICKS.'
            sharing_code: '- (Optional) The one-time sharing code provided by the data recipient.'
            tokens: '- List of Recipient Tokens. This field is only present when the authentication_type is TOKEN. Each list element is an object with following attributes:'
            updated_at: '- Time at which this recipient Token was updated, in epoch milliseconds.'
            updated_by: '- Username of recipient Token updater.'
        importStatements: []
    databricks_registered_model:
        subCategory: Unity Catalog
        description: '""subcategory: "Unity Catalog"'
        name: databricks_registered_model
        title: databricks_registered_model Resource
        examples:
            - name: this
              manifest: |-
                {
                  "catalog_name": "main",
                  "name": "my_model",
                  "schema_name": "default"
                }
        argumentDocs:
            ALL_PRIVILEGES: ', APPLY_TAG, and EXECUTE privileges.'
            catalog_name: '- (Required) The name of the catalog where the schema and the registered model reside. Change of this parameter forces recreation of the resource.'
            comment: '- (Optional) The comment attached to the registered model.'
            id: '- Equal to the full name of the model (catalog_name.schema_name.name) and used to identify the model uniquely across the metastore.'
            name: '- (Required) The name of the registered model.  Change of this parameter forces recreation of the resource.'
            owner: '- (Optional) Name of the registered model owner.'
            schema_name: '- (Required) The name of the schema where the registered model resides. Change of this parameter forces recreation of the resource.'
            storage_location: '- (Optional) The storage location under which model version data files are stored. Change of this parameter forces recreation of the resource.'
        importStatements: []
    databricks_repo:
        subCategory: Workspace
        description: '""subcategory: "Workspace"'
        name: databricks_repo
        title: databricks_repo Resource
        examples:
            - name: nutter_in_home
              manifest: |-
                {
                  "url": "https://github.com/user/demo.git"
                }
        argumentDocs:
            branch: '- (Optional) name of the branch for initial checkout. If not specified, the default branch of the repository will be used.  Conflicts with tag.  If branch is removed, and tag isn''t specified, then the repository will stay at the previously checked out state.'
            commit_hash: '- Hash of the HEAD commit at time of the last executed operation. It won''t change if you manually perform pull operation via UI or API'
            git_provider: '- (Optional, if it''s possible to detect Git provider by host name) case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult Repos API documentation): gitHub, gitHubEnterprise, bitbucketCloud, bitbucketServer, azureDevOpsServices, gitLab, gitLabEnterpriseEdition, awsCodeCommit.'
            id: '-  Git folder identifier'
            path: '- (Optional) path to put the checked out Git folder. If not specified, , then the Git folder will be created in the default location.  If the value changes, Git folder is re-created.'
            sparse_checkout.patterns: '- array of paths (directories) that will be used for sparse checkout.  List of patterns could be updated in-place.'
            tag: '- (Optional) name of the tag for initial checkout.  Conflicts with branch.'
            url: '-  (Required) The URL of the Git Repository to clone from. If the value changes, Git folder is re-created.'
            workspace_path: '- path on Workspace File System (WSFS) in form of /Workspace + path'
        importStatements: []
    databricks_restrict_workspace_admins_setting:
        subCategory: Settings
        description: '""subcategory: "Settings"'
        name: databricks_restrict_workspace_admins_setting
        title: databricks_restrict_workspace_admins_setting Resource
        examples:
            - name: this
              manifest: |-
                {
                  "restrict_workspace_admins": [
                    {
                      "status": "RESTRICT_TOKENS_AND_JOB_RUN_AS"
                    }
                  ]
                }
        argumentDocs:
            restrict_workspace_admins: '- (Required) The configuration details.'
            status: '- (Required) The restrict workspace admins status for the workspace.'
        importStatements: []
    databricks_schema:
        subCategory: Unity Catalog
        description: '""subcategory: "Unity Catalog"'
        name: databricks_schema
        title: databricks_schema Resource
        examples:
            - name: things
              manifest: |-
                {
                  "catalog_name": "${databricks_catalog.sandbox.id}",
                  "comment": "this database is managed by terraform",
                  "name": "things",
                  "properties": {
                    "kind": "various"
                  }
                }
              references:
                catalog_name: databricks_catalog.sandbox.id
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
        argumentDocs:
            catalog_name: '- Name of parent catalog. Change forces creation of a new resource.'
            comment: '- (Optional) User-supplied free-form text.'
            enable_predictive_optimization: '- (Optional) Whether predictive optimization should be enabled for this object and objects under it. Can be ENABLE, DISABLE or INHERIT'
            force_destroy: '- (Optional) Delete schema regardless of its contents.'
            id: '- ID of this schema in form of <catalog_name>.<name>.'
            name: '- Name of Schema relative to parent catalog. Change forces creation of a new resource.'
            owner: '- (Optional) Username/groupname/sp application_id of the schema owner.'
            properties: '- (Optional) Extensible Schema properties.'
            storage_root: '- (Optional) Managed location of the schema. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the catalog root location. Change forces creation of a new resource.'
        importStatements: []
    databricks_secret:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_secret
        title: databricks_secret Resource
        examples:
            - name: publishing_api
              manifest: |-
                {
                  "key": "publishing_api",
                  "scope": "${databricks_secret_scope.app.id}",
                  "string_value": "${data.azurerm_key_vault_secret.example.value}"
                }
              references:
                scope: databricks_secret_scope.app.id
                string_value: data.azurerm_key_vault_secret.example.value
              dependencies:
                databricks_cluster.this: |-
                    {
                      "spark_conf": {
                        "fs.azure.account.oauth2.client.secret": "${databricks_secret.publishing_api.config_reference}"
                      }
                    }
                databricks_secret_scope.app: |-
                    {
                      "name": "application-secret-scope"
                    }
        argumentDocs:
            config_reference: '- (String) value to use as a secret reference in Spark configuration and environment variables: {{secrets/scope/key}}.'
            id: '- Canonical unique identifier for the secret.'
            key: '- (Required) (String) key within secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.'
            last_updated_timestamp: '- (Integer) time secret was updated'
            scope: '- (Required) (String) name of databricks secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.'
            string_value: '- (Required) (String) super secret sensitive value.'
        importStatements: []
    databricks_secret_acl:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_secret_acl
        title: databricks_secret_acl Resource
        examples:
            - name: my_secret_acl
              manifest: |-
                {
                  "permission": "READ",
                  "principal": "${databricks_group.ds.display_name}",
                  "scope": "${databricks_secret_scope.app.name}"
                }
              references:
                principal: databricks_group.ds.display_name
                scope: databricks_secret_scope.app.name
              dependencies:
                databricks_group.ds: |-
                    {
                      "display_name": "data-scientists"
                    }
                databricks_secret.publishing_api: |-
                    {
                      "key": "publishing_api",
                      "scope": "${databricks_secret_scope.app.name}",
                      "string_value": "${data.azurerm_key_vault_secret.example.value}"
                    }
                databricks_secret_scope.app: |-
                    {
                      "name": "app-secret-scope"
                    }
        argumentDocs:
            application_id: attribute of databricks_service_principal.
            display_name: attribute of databricks_group.  Use users to allow access for all workspace users.
            permission: '- (Required) READ, WRITE or MANAGE.'
            principal: '- (Required) principal''s identifier. It can be:'
            scope: '- (Required) name of the scope'
            user_name: attribute of databricks_user.
        importStatements: []
    databricks_secret_scope:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_secret_scope
        title: databricks_secret_scope Resource
        examples:
            - name: this
              manifest: |-
                {
                  "name": "terraform-demo-scope"
                }
            - name: kv
              manifest: |-
                {
                  "keyvault_metadata": [
                    {
                      "dns_name": "${azurerm_key_vault.this.vault_uri}",
                      "resource_id": "${azurerm_key_vault.this.id}"
                    }
                  ],
                  "name": "keyvault-managed"
                }
              references:
                keyvault_metadata.dns_name: azurerm_key_vault.this.vault_uri
                keyvault_metadata.resource_id: azurerm_key_vault.this.id
              dependencies:
                azurerm_key_vault.this: |-
                    {
                      "location": "${azurerm_resource_group.example.location}",
                      "name": "${var.prefix}-kv",
                      "purge_protection_enabled": false,
                      "resource_group_name": "${azurerm_resource_group.example.name}",
                      "sku_name": "standard",
                      "soft_delete_enabled": false,
                      "tags": "${var.tags}",
                      "tenant_id": "${data.azurerm_client_config.current.tenant_id}"
                    }
                azurerm_key_vault_access_policy.this: |-
                    {
                      "key_vault_id": "${azurerm_key_vault.this.id}",
                      "object_id": "${data.azurerm_client_config.current.object_id}",
                      "secret_permissions": [
                        "Delete",
                        "Get",
                        "List",
                        "Set"
                      ],
                      "tenant_id": "${data.azurerm_client_config.current.tenant_id}"
                    }
        argumentDocs:
            backend_type: '- Either DATABRICKS or AZURE_KEYVAULT'
            id: '- The id for the secret scope object.'
            initial_manage_principal: '- (Optional) The principal with the only possible value users that is initially granted MANAGE permission to the created scope.  If it''s omitted, then the databricks_secret_acl with MANAGE permission applied to the scope is assigned to the API request issuer''s user identity (see documentation). This part of the state cannot be imported.'
            name: '- (Required) Scope name requested by the user. Must be unique within a workspace. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.'
        importStatements: []
    databricks_service_principal:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_service_principal
        title: databricks_service_principal Resource
        examples:
            - name: sp
              manifest: |-
                {
                  "display_name": "Admin SP"
                }
            - name: sp
              manifest: |-
                {
                  "display_name": "Admin SP"
                }
              dependencies:
                databricks_group_member.i-am-admin: |-
                    {
                      "group_id": "${data.databricks_group.admins.id}",
                      "member_id": "${databricks_service_principal.sp.id}"
                    }
            - name: sp
              manifest: |-
                {
                  "allow_cluster_create": true,
                  "application_id": "00000000-0000-0000-0000-000000000000",
                  "display_name": "Example service principal"
                }
            - name: sp
              manifest: |-
                {
                  "display_name": "Automation-only SP",
                  "provider": "${databricks.account}"
                }
              references:
                provider: databricks.account
            - name: sp
              manifest: |-
                {
                  "application_id": "00000000-0000-0000-0000-000000000000",
                  "provider": "${databricks.account}"
                }
              references:
                provider: databricks.account
        argumentDocs:
            acl_principal_id: '- identifier for use in databricks_access_control_rule_set, e.g. servicePrincipals/00000000-0000-0000-0000-000000000000.'
            active: '- (Optional) Either service principal is active or not. True by default, but can be set to false in case of service principal deactivation with preserving service principal assets.'
            allow_cluster_create: '- (Optional) Allow the service principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and cluster_id argument. Everyone without allow_cluster_create argument set, but with permission to use Cluster Policy would be able to create clusters, but within the boundaries of that specific policy.'
            allow_instance_pool_create: '- (Optional) Allow the service principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and instance_pool_id argument.'
            application_id: This is the Azure Application ID of the given Azure service principal and will be their form of access and identity. For Databricks-managed service principals this value is auto-generated.
            databricks_sql_access: '- (Optional) This is a field to allow the group to have access to Databricks SQL feature through databricks_sql_endpoint.'
            disable_as_user_deletion: '- (Optional) Deactivate the service principal when deleting the resource, rather than deleting the service principal entirely. Defaults to true when the provider is configured at the account-level and false when configured at the workspace-level. This flag is exclusive to force_delete_repos and force_delete_home_dir flags.'
            display_name: '- (Required for Databricks-managed service principals) This is an alias for the service principal and can be the full name of the service principal.'
            external_id: '- (Optional) ID of the service principal in an external identity provider.'
            force: '- (Optional) Ignore cannot create service principal: Service principal with application ID X already exists errors and implicitly import the specified service principal into Terraform state, enforcing entitlements defined in the instance of resource. This functionality is experimental and is designed to simplify corner cases, like Azure Active Directory synchronisation.'
            force_delete_home_dir: '- (Optional) This flag determines whether the service principal''s home directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.'
            force_delete_repos: '- (Optional) This flag determines whether the service principal''s repo directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.'
            home: '- Home folder of the service principal, e.g. /Users/00000000-0000-0000-0000-000000000000.'
            id: '- Canonical unique identifier for the service principal.'
            repos: '- Personal Repos location of the service principal, e.g. /Repos/00000000-0000-0000-0000-000000000000.'
            workspace_access: '- (Optional) This is a field to allow the group to have access to Databricks Workspace.'
        importStatements: []
    databricks_service_principal_role:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_service_principal_role
        title: databricks_service_principal_role Resource
        examples:
            - name: my_service_principal_instance_profile
              manifest: |-
                {
                  "role": "${databricks_instance_profile.instance_profile.id}",
                  "service_principal_id": "${databricks_service_principal.this.id}"
                }
              references:
                role: databricks_instance_profile.instance_profile.id
                service_principal_id: databricks_service_principal.this.id
              dependencies:
                databricks_instance_profile.instance_profile: |-
                    {
                      "instance_profile_arn": "my_instance_profile_arn"
                    }
                databricks_service_principal.this: |-
                    {
                      "display_name": "My Service Principal"
                    }
        argumentDocs:
            id: '- The id in the format <service_principal_id>|<role>.'
            role: '-  (Required) This is the id of the role or instance profile resource.'
            service_principal_id: '- (Required) This is the id of the service principal resource.'
        importStatements: []
    databricks_service_principal_secret:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_service_principal_secret
        title: databricks_service_principal_secret Resource
        examples:
            - name: terraform_sp
              manifest: |-
                {
                  "service_principal_id": "${databricks_service_principal.this.id}"
                }
              references:
                service_principal_id: databricks_service_principal.this.id
        argumentDocs:
            id: '- ID of the secret'
            secret: '- Generated secret for the service principal'
            service_principal_id: '- ID of the databricks_service_principal (not application ID).'
        importStatements: []
    databricks_share:
        subCategory: Delta Sharing
        description: '""subcategory: "Delta Sharing"'
        name: databricks_share
        title: databricks_share Resource
        examples:
            - name: some
              manifest: |-
                {
                  "dynamic": {
                    "object": [
                      {
                        "content": [
                          {
                            "data_object_type": "TABLE",
                            "name": "${object.value}"
                          }
                        ],
                        "for_each": "${data.databricks_tables.things.ids}"
                      }
                    ]
                  },
                  "name": "my_share"
                }
              references:
                dynamic.content.name: object.value
                dynamic.for_each: data.databricks_tables.things.ids
            - name: schema_share
              manifest: |-
                {
                  "name": "schema_share",
                  "object": [
                    {
                      "data_object_type": "SCHEMA",
                      "history_data_sharing_status": "ENABLED",
                      "name": "catalog_name.schema_name"
                    }
                  ]
                }
            - name: some
              manifest: |-
                {
                  "name": "my_share",
                  "object": [
                    {
                      "data_object_type": "TABLE",
                      "history_data_sharing_status": "ENABLED",
                      "name": "my_catalog.my_schema.my_table",
                      "partition": [
                        {
                          "value": [
                            {
                              "name": "year",
                              "op": "EQUAL",
                              "value": "2009"
                            },
                            {
                              "name": "month",
                              "op": "EQUAL",
                              "value": "12"
                            }
                          ]
                        },
                        {
                          "value": [
                            {
                              "name": "year",
                              "op": "EQUAL",
                              "value": "2010"
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
        argumentDocs:
            cdf_enabled: (Optional) - Whether to enable Change Data Feed (cdf) on the shared object. When this field is set, field history_data_sharing_status can not be set.
            comment: (Optional) -  Description about the object.
            created_at: '- Time when the share was created.'
            created_by: '- The principal that created the share.'
            data_object_type: (Required) - Type of the data object, currently TABLE, SCHEMA, VOLUME, and MODEL are supported.
            history_data_sharing_status: '(Optional) - Whether to enable history sharing, one of: ENABLED, DISABLED. When a table has history sharing enabled, recipients can query table data by version, starting from the current table version. If not specified, clients can only query starting from the version of the object at the time it was added to the share. NOTE: The start_version should be less than or equal the current version of the object. When this field is set, field cdf_enabled can not be set.'
            id: '- the ID of the share, the same as name.'
            name: (Required) - Name of share. Change forces creation of a new resource.
            op: '- The operator to apply for the value, one of: EQUAL, LIKE'
            owner: (Optional) -  User name/group name/sp application_id of the share owner.
            recipient_property_key: (Optional) - The key of a Delta Sharing recipient's property. For example databricks-account-id. When this field is set, field value can not be set.
            shared_as: (Optional) - A user-provided new name for the data object within the share. If this new name is not provided, the object's original name will be used as the shared_as name. The shared_as name must be unique within a Share. Change forces creation of a new resource.
            start_version: (Optional) -  The start version associated with the object for cdf. This allows data providers to control the lowest object version that is accessible by clients.
            status: '- Status of the object, one of: ACTIVE, PERMISSION_DENIED.'
            value: (Optional) - The value of the partition column. When this value is not set, it means null value. When this field is set, field recipient_property_key can not be set.
        importStatements: []
    databricks_sql_alert:
        subCategory: Databricks SQL
        description: '""subcategory: "Databricks SQL"'
        name: databricks_sql_alert
        title: databricks_sql_alert Resource
        examples:
            - name: alert
              manifest: |-
                {
                  "name": "My Alert",
                  "options": [
                    {
                      "column": "p1",
                      "muted": false,
                      "op": "==",
                      "value": "2"
                    }
                  ],
                  "parent": "folders/${databricks_directory.shared_dir.object_id}",
                  "query_id": "${databricks_sql_query.this.id}",
                  "rearm": 1
                }
              references:
                query_id: databricks_sql_query.this.id
              dependencies:
                databricks_directory.shared_dir: |-
                    {
                      "path": "/Shared/Queries"
                    }
                databricks_sql_query.this: |-
                    {
                      "data_source_id": "${databricks_sql_endpoint.example.data_source_id}",
                      "name": "My Query Name",
                      "parent": "folders/${databricks_directory.shared_dir.object_id}",
                      "query": "SELECT 1 AS p1, 2 as p2"
                    }
        argumentDocs:
            column: '- (Required, String) Name of column in the query result to compare in alert evaluation.'
            custom_body: '- (Optional, String) Custom body of alert notification, if it exists. See Alerts API reference for custom templating instructions.'
            custom_subject: '- (Optional, String) Custom subject of alert notification, if it exists. This includes email subject, Slack notification header, etc. See Alerts API reference for custom templating instructions.'
            empty_result_state: '- (Optional, String) State that alert evaluates to when query result is empty.  Currently supported values are unknown, triggered, ok - check API documentation for full list of supported values.'
            id: '- unique ID of the SQL Alert.'
            muted: '- (Optional, bool) Whether or not the alert is muted. If an alert is muted, it will not notify users and alert destinations when triggered.'
            name: '- (Required, String) Name of the alert.'
            op: '- (Required, String Enum) Operator used to compare in alert evaluation. (Enum: >, >=, <, <=, ==, !=)'
            options: '- (Required) Alert configuration options.'
            parent: '- (Optional, String) The identifier of the workspace folder containing the alert. The default is ther user''s home folder. The folder identifier is formatted as folder/<folder_id>.'
            query_id: '- (Required, String) ID of the query evaluated by the alert.'
            rearm: '- (Optional, Integer) Number of seconds after being triggered before the alert rearms itself and can be triggered again. If not defined, alert will never be triggered again.'
            value: '- (Required, String) Value used to compare in alert evaluation.'
        importStatements: []
    databricks_sql_dashboard:
        subCategory: Databricks SQL
        description: '""subcategory: "Databricks SQL"'
        name: databricks_sql_dashboard
        title: databricks_sql_dashboard Resource
        examples:
            - name: d1
              manifest: |-
                {
                  "name": "My Dashboard Name",
                  "parent": "folders/${databricks_directory.shared_dir.object_id}",
                  "tags": [
                    "some-tag",
                    "another-tag"
                  ]
                }
              dependencies:
                databricks_directory.shared_dir: |-
                    {
                      "path": "/Shared/Dashboards"
                    }
        argumentDocs:
            id: '- the unique ID of the SQL Dashboard.'
        importStatements: []
    databricks_sql_endpoint:
        subCategory: Databricks SQL
        description: '""subcategory: "Databricks SQL"'
        name: databricks_sql_endpoint
        title: databricks_sql_endpoint Resource
        examples:
            - name: this
              manifest: |-
                {
                  "cluster_size": "Small",
                  "max_num_clusters": 1,
                  "name": "Endpoint of ${data.databricks_current_user.me.alphanumeric}",
                  "tags": [
                    {
                      "custom_tags": [
                        {
                          "key": "City",
                          "value": "Amsterdam"
                        }
                      ]
                    }
                  ]
                }
        argumentDocs:
            auto_stop_mins: '- Time in minutes until an idle SQL warehouse terminates all clusters and stops. This field is optional. The default is 120, set to 0 to disable the auto stop.'
            channel: 'block, consisting of following fields:'
            channel.name: '- Name of the Databricks SQL release channel. Possible values are: CHANNEL_NAME_PREVIEW and CHANNEL_NAME_CURRENT. Default is CHANNEL_NAME_CURRENT.'
            cluster_size: '- (Required) The size of the clusters allocated to the endpoint: "2X-Small", "X-Small", "Small", "Medium", "Large", "X-Large", "2X-Large", "3X-Large", "4X-Large".'
            creator_name: '- The username of the user who created the endpoint.'
            data_source_id: '- ID of the data source for this endpoint. This is used to bind an Databricks SQL query to an endpoint.'
            databricks_sql_access: on databricks_group or databricks_user.
            enable_photon: '- Whether to enable Photon. This field is optional and is enabled by default.'
            enable_serverless_compute: '- Whether this SQL warehouse is a serverless endpoint. See below for details about the default values. To avoid ambiguity, especially for organizations with many workspaces, Databricks recommends that you always set this field explicitly.'
            "false": for most workspaces. However, if this workspace used the SQL Warehouses API to create a warehouse between September 1, 2022 and April 30, 2023, the default remains the previous behavior which is default to true if the workspace is enabled for serverless and fits the requirements for serverless SQL warehouses. If your account needs updated terms of use, workspace admins are prompted in the Databricks SQL UI. A workspace must meet the requirements and might require an update to its instance profile role to add a trust relationship.
            health: '- Health status of the endpoint.'
            id: '- the unique ID of the SQL warehouse.'
            jdbc_url: '- JDBC connection string.'
            max_num_clusters: '- Maximum number of clusters available when a SQL warehouse is running. This field is required. If multi-cluster load balancing is not enabled, this is default to 1.'
            min_num_clusters: '- Minimum number of clusters available when a SQL warehouse is running. The default is 1.'
            name: '- (Required) Name of the SQL warehouse. Must be unique.'
            num_active_sessions: '- The current number of clusters used by the endpoint.'
            num_clusters: '- The current number of clusters used by the endpoint.'
            odbc_params: '- ODBC connection params: odbc_params.hostname, odbc_params.path, odbc_params.protocol, and odbc_params.port.'
            spot_instance_policy: '- The spot policy to use for allocating instances to clusters: COST_OPTIMIZED or RELIABILITY_OPTIMIZED. This field is optional. Default is COST_OPTIMIZED.'
            state: '- The current state of the endpoint.'
            tags: '- Databricks tags all endpoint resources with these tags.'
            warehouse_type: '- SQL warehouse type. See for AWS or Azure. Set to PRO or CLASSIC. If the field enable_serverless_compute has the value true either explicitly or through the default logic (see that field above for details), the default is PRO, which is required for serverless SQL warehouses. Otherwise, the default is CLASSIC.'
        importStatements: []
    databricks_sql_global_config:
        subCategory: Databricks SQL
        description: '""subcategory: "Databricks SQL"'
        name: databricks_sql_global_config
        title: databricks_sql_global_config Resource
        examples:
            - name: this
              manifest: |-
                {
                  "data_access_config": {
                    "spark.sql.session.timeZone": "UTC"
                  },
                  "instance_profile_arn": "arn:....",
                  "security_policy": "DATA_ACCESS_CONTROL"
                }
            - name: this
              manifest: |-
                {
                  "data_access_config": {
                    "spark.hadoop.fs.azure.account.auth.type": "OAuth",
                    "spark.hadoop.fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
                    "spark.hadoop.fs.azure.account.oauth2.client.endpoint": "https://login.microsoftonline.com/${var.tenant_id}/oauth2/token",
                    "spark.hadoop.fs.azure.account.oauth2.client.id": "${var.application_id}",
                    "spark.hadoop.fs.azure.account.oauth2.client.secret": "{{secrets/${local.secret_scope}/${local.secret_key}}}"
                  },
                  "security_policy": "DATA_ACCESS_CONTROL",
                  "sql_config_params": {
                    "ANSI_MODE": "true"
                  }
                }
        argumentDocs:
            data_access_config: (Optional, Map) - Data access configuration for databricks_sql_endpoint, such as configuration for an external Hive metastore, Hadoop Filesystem configuration, etc.  Please note that the list of supported configuration properties is limited, so refer to the documentation for a full list.  Apply will fail if you're specifying not permitted configuration.
            google_service_account: (Optional, String) - used to access GCP services, such as Cloud Storage, from databricks_sql_endpoint. Please note that this parameter is only for GCP, and will generate an error if used on other clouds.
            instance_profile_arn: (Optional, String) - databricks_instance_profile used to access storage from databricks_sql_endpoint. Please note that this parameter is only for AWS, and will generate an error if used on other clouds.
            security_policy: '(Optional, String) - The policy for controlling access to datasets. Default value: DATA_ACCESS_CONTROL, consult documentation for list of possible values'
            sql_config_params: (Optional, Map) - SQL Configuration Parameters let you override the default behavior for all sessions with all endpoints.
        importStatements: []
    databricks_sql_permissions:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_sql_permissions
        title: databricks_sql_permissions Resource
        examples:
            - name: foo_table
              manifest: |-
                {
                  "privilege_assignments": [
                    {
                      "principal": "serge@example.com",
                      "privileges": [
                        "SELECT",
                        "MODIFY"
                      ]
                    },
                    {
                      "principal": "special group",
                      "privileges": [
                        "SELECT"
                      ]
                    }
                  ],
                  "table": "foo"
                }
            - name: foo_table
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.cluster_name.id}"
                }
              references:
                cluster_id: databricks_cluster.cluster_name.id
        argumentDocs:
            anonymous function/: '- anonymous function. / suffix is mandatory.'
            anonymous_function: '- (Boolean) If this access control for using anonymous function. Defaults to false.'
            any file/: '- direct access to any file. / suffix is mandatory.'
            any_file: '- (Boolean) If this access control for reading/writing any file. Defaults to false.'
            catalog: '- (Boolean) If this access control for the entire catalog. Defaults to false.'
            catalog/: '- entire catalog. / suffix is mandatory.'
            cluster_id: '- (Optional) Id of an existing databricks_cluster, where the appropriate GRANT/REVOKE commands are executed. This cluster must have the appropriate data security mode (USER_ISOLATION or LEGACY_TABLE_ACL specified). If no cluster_id is specified, a single-node TACL cluster named terraform-table-acl is automatically created.'
            database: '- Name of the database. Has default value of default.'
            database/bar: '- bar database.'
            privilege_assignments.CREATE: '- gives the ability to create an object (for example, a table in a database).'
            privilege_assignments.CREATE_NAMED_FUNCTION: '- gives the ability to create a named UDF in an existing catalog or database.'
            privilege_assignments.MODIFY: '- gives the ability to add, delete, and modify data to or from an object.'
            privilege_assignments.MODIFY_CLASSPATH: '- gives the ability to add files to the Spark class path.'
            privilege_assignments.READ_METADATA: '- gives the ability to view an object and its metadata.'
            privilege_assignments.SELECT: '- gives read access to an object.'
            privilege_assignments.USAGE: '- do not give any abilities, but is an additional requirement to perform any action on a database object.'
            privilege_assignments.principal: '- display_name for a databricks_group or databricks_user, application_id for a databricks_service_principal.'
            privilege_assignments.privileges: '- set of available privilege names in upper case.'
            table: '- Name of the table. Can be combined with database.'
            table/default.foo: '- table foo in a default database. Database is always mandatory.'
            view: '- Name of the view. Can be combined with database.'
            view/bar.foo: '- view foo in bar database.'
        importStatements: []
    databricks_sql_query:
        subCategory: Databricks SQL
        description: '""subcategory: "Databricks SQL"'
        name: databricks_sql_query
        title: databricks_sql_query Resource
        examples:
            - name: q1
              manifest: |-
                {
                  "data_source_id": "${databricks_sql_endpoint.example.data_source_id}",
                  "name": "My Query Name",
                  "parameter": [
                    {
                      "name": "p1",
                      "text": [
                        {
                          "value": "default"
                        }
                      ],
                      "title": "Title for p1"
                    },
                    {
                      "enum": [
                        {
                          "multiple": [
                            {
                              "prefix": "\"",
                              "separator": ",",
                              "suffix": "\""
                            }
                          ],
                          "options": [
                            "default",
                            "foo",
                            "bar"
                          ],
                          "value": "default"
                        }
                      ],
                      "name": "p2",
                      "title": "Title for p2"
                    },
                    {
                      "date": [
                        {
                          "value": "2022-01-01"
                        }
                      ],
                      "name": "p3",
                      "title": "Title for p3"
                    }
                  ],
                  "parent": "folders/${databricks_directory.shared_dir.object_id}",
                  "query": "                        SELECT {{ p1 }} AS p1\n                        WHERE 1=1\n                        AND p2 in ({{ p2 }})\n                        AND event_date \u003e date '{{ p3 }}'\n",
                  "run_as_role": "viewer",
                  "tags": [
                    "t1",
                    "t2"
                  ]
                }
              references:
                data_source_id: databricks_sql_endpoint.example.data_source_id
              dependencies:
                databricks_directory.shared_dir: |-
                    {
                      "path": "/Shared/Queries"
                    }
        argumentDocs:
            data_source_id: '- Data source ID of a SQL warehouse'
            description: '- General description that conveys additional information about this query such as usage notes.'
            id: '- the unique ID of the SQL Query.'
            name: '- The title of this query that appears in list views, widget headings, and on the query page.'
            parent: '- The identifier of the workspace folder containing the object.'
            query: '- The text of the query to be run.'
            run_as_role: '- Run as role. Possible values are viewer, owner.'
            text.value: '- The default value for this parameter.'
            title: '- The text displayed in a parameter picking widget.'
        importStatements: []
    databricks_sql_table:
        subCategory: Unity Catalog
        description: '""subcategory: "Unity Catalog"'
        name: databricks_sql_table
        title: databricks_sql_table Resource
        examples:
            - name: thing
              manifest: |-
                {
                  "catalog_name": "${databricks_catalog.sandbox.name}",
                  "column": [
                    {
                      "name": "id",
                      "type": "int"
                    },
                    {
                      "comment": "name of thing",
                      "name": "name",
                      "type": "string"
                    }
                  ],
                  "comment": "this table is managed by terraform",
                  "data_source_format": "DELTA",
                  "name": "quickstart_table",
                  "provider": "${databricks.workspace}",
                  "schema_name": "${databricks_schema.things.name}",
                  "storage_location": "",
                  "table_type": "MANAGED"
                }
              references:
                catalog_name: databricks_catalog.sandbox.name
                provider: databricks.workspace
                schema_name: databricks_schema.things.name
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.id}",
                      "comment": "this database is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
            - name: thing_view
              manifest: |-
                {
                  "catalog_name": "${databricks_catalog.sandbox.name}",
                  "cluster_id": "0423-201305-xsrt82qn",
                  "comment": "this view is managed by terraform",
                  "name": "quickstart_table_view",
                  "provider": "${databricks.workspace}",
                  "schema_name": "${databricks_schema.things.name}",
                  "table_type": "VIEW",
                  "view_definition": "${format(\"SELECT name FROM %s WHERE id == 1\", databricks_sql_table.thing.id)}"
                }
              references:
                catalog_name: databricks_catalog.sandbox.name
                provider: databricks.workspace
                schema_name: databricks_schema.things.name
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.id}",
                      "comment": "this database is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
            - name: thing
              manifest: |-
                {
                  "catalog_name": "${databricks_catalog.sandbox.name}",
                  "column": [
                    {
                      "name": "id",
                      "type": "int"
                    },
                    {
                      "comment": "name of thing",
                      "name": "name",
                      "type": "string"
                    }
                  ],
                  "comment": "this table is managed by terraform",
                  "data_source_format": "DELTA",
                  "name": "quickstart_table",
                  "provider": "${databricks.workspace}",
                  "schema_name": "${databricks_schema.things.name}",
                  "storage_location": "",
                  "table_type": "MANAGED",
                  "warehouse_id": "${databricks_sql_endpoint.this.id}"
                }
              references:
                catalog_name: databricks_catalog.sandbox.name
                provider: databricks.workspace
                schema_name: databricks_schema.things.name
                warehouse_id: databricks_sql_endpoint.this.id
              dependencies:
                databricks_sql_endpoint.this: |-
                    {
                      "cluster_size": "2X-Small",
                      "max_num_clusters": 1,
                      "name": "endpoint"
                    }
            - name: thing_view
              manifest: |-
                {
                  "catalog_name": "${databricks_catalog.sandbox.name}",
                  "comment": "this view is managed by terraform",
                  "name": "quickstart_table_view",
                  "provider": "${databricks.workspace}",
                  "schema_name": "${databricks_schema.things.name}",
                  "table_type": "VIEW",
                  "view_definition": "${format(\"SELECT name FROM %s WHERE id == 1\", databricks_sql_table.thing.id)}",
                  "warehouse_id": "${databricks_sql_endpoint.this.id}"
                }
              references:
                catalog_name: databricks_catalog.sandbox.name
                provider: databricks.workspace
                schema_name: databricks_schema.things.name
                warehouse_id: databricks_sql_endpoint.this.id
              dependencies:
                databricks_sql_endpoint.this: |-
                    {
                      "cluster_size": "2X-Small",
                      "max_num_clusters": 1,
                      "name": "endpoint"
                    }
            - name: thing
              manifest: |-
                {
                  "catalog_name": "${databricks_catalog.sandbox.name}",
                  "column": [
                    {
                      "identity": "default",
                      "name": "id",
                      "type": "bigint"
                    },
                    {
                      "comment": "name of thing",
                      "name": "name",
                      "type": "string"
                    }
                  ],
                  "comment": "this table is managed by terraform",
                  "data_source_format": "DELTA",
                  "name": "quickstart_table",
                  "provider": "${databricks.workspace}",
                  "schema_name": "${databricks_schema.things.name}",
                  "storage_location": "",
                  "table_type": "MANAGED"
                }
              references:
                catalog_name: databricks_catalog.sandbox.name
                provider: databricks.workspace
                schema_name: databricks_schema.things.name
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.id}",
                      "comment": "this database is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
        argumentDocs:
            catalog_name: '- Name of parent catalog. Change forces creation of a new resource.'
            cluster_id: '- (Optional) All table CRUD operations must be executed on a running cluster or SQL warehouse. If a cluster_id is specified, it will be used to execute SQL commands to manage this table. If empty, a cluster will be created automatically with the name terraform-sql-table.'
            cluster_keys: '- (Optional) a subset of columns to liquid cluster the table by. Conflicts with partitions.'
            comment: '- (Optional) User-supplied free-form text. Changing comment is not currently supported on VIEW table_type.'
            data_source_format: '- (Optional) External tables are supported in multiple data source formats. The string constants identifying these formats are DELTA, CSV, JSON, AVRO, PARQUET, ORC, TEXT. Change forces creation of a new resource. Not supported for MANAGED tables or VIEW.'
            id: '- ID of this table in form of <catalog_name>.<schema_name>.<name>.'
            identity: '- (Optional) Whether field is an identity column. Can be default, always or unset. It is unset by default.'
            name: '- Name of table relative to parent catalog and schema. Change forces creation of a new resource.'
            nullable: '- (Optional) Whether field is nullable (Default: true)'
            options: '- (Optional) Map of user defined table options. Change forces creation of a new resource.'
            owner: '- (Optional) Username/groupname/sp application_id of the schema owner.'
            partitions: '- (Optional) a subset of columns to partition the table by. Change forces creation of a new resource. Conflicts with cluster_keys. Change forces creation of a new resource.'
            properties: '- (Optional) Map of table properties.'
            schema_name: '- Name of parent Schema relative to parent Catalog. Change forces creation of a new resource.'
            storage_credential_name: '- (Optional) For EXTERNAL Tables only: the name of storage credential to use. Change forces creation of a new resource.'
            storage_location: '- (Optional) URL of storage location for Table data (required for EXTERNAL Tables). Not supported for VIEW or MANAGED table_type.'
            table_type: '- Distinguishes a view vs. managed/external Table. MANAGED, EXTERNAL or VIEW. Change forces creation of a new resource.'
            type: '- Column type spec (with metadata) as SQL text. Not supported for VIEW table_type.'
            view_definition: '- (Optional) SQL text defining the view (for table_type == "VIEW"). Not supported for MANAGED or EXTERNAL table_type.'
            warehouse_id: '- (Optional) All table CRUD operations must be executed on a running cluster or SQL warehouse. If a warehouse_id is specified, that SQL warehouse will be used to execute SQL commands to manage this table. Conflicts with cluster_id.'
        importStatements: []
    databricks_sql_visualization:
        subCategory: Databricks SQL
        description: '""subcategory: "Databricks SQL"'
        name: databricks_sql_visualization
        title: databricks_sql_visualization Resource
        examples:
            - name: q1v1
              manifest: |-
                {
                  "description": "Some Description",
                  "name": "My Table",
                  "options": "${jsonencode(\n    {\n      \"itemsPerPage\" : 25,\n      \"columns\" : [\n        {\n          \"name\" : \"p1\",\n          \"type\" : \"string\"\n          \"title\" : \"Parameter 1\",\n          \"displayAs\" : \"string\",\n        },\n        {\n          \"name\" : \"p2\",\n          \"type\" : \"string\"\n          \"title\" : \"Parameter 2\",\n          \"displayAs\" : \"link\",\n          \"highlightLinks\" : true,\n        }\n      ]\n    }\n  )}",
                  "query_id": "${databricks_sql_query.q1.id}",
                  "type": "table"
                }
              references:
                query_id: databricks_sql_query.q1.id
            - name: q1v1
              manifest: |-
                {
                  "description": "Some Description",
                  "name": "My Table",
                  "options": "${file(\"${path.module}/visualizations/q1v1.json\")}",
                  "query_id": "${databricks_sql_query.q1.id}",
                  "type": "table"
                }
              references:
                query_id: databricks_sql_query.q1.id
            - name: q1v2
              manifest: |-
                {
                  "description": "Some Description",
                  "name": "My Chart",
                  "options": "${file(\"${path.module}/visualizations/q1v2.json\")}",
                  "query_id": "${databricks_sql_query.q1.id}",
                  "type": "chart"
                }
              references:
                query_id: databricks_sql_query.q1.id
        argumentDocs: {}
        importStatements: []
    databricks_sql_widget:
        subCategory: Databricks SQL
        description: '""subcategory: "Databricks SQL"'
        name: databricks_sql_widget
        title: databricks_sql_widget Resource
        examples:
            - name: d1w1
              manifest: |-
                {
                  "dashboard_id": "${databricks_sql_dashboard.d1.id}",
                  "position": [
                    {
                      "pos_x": 0,
                      "pos_y": 0,
                      "size_x": 3,
                      "size_y": 4
                    }
                  ],
                  "text": "Hello! I'm a **text widget**!"
                }
              references:
                dashboard_id: databricks_sql_dashboard.d1.id
            - name: d1w2
              manifest: |-
                {
                  "dashboard_id": "${databricks_sql_dashboard.d1.id}",
                  "position": [
                    {
                      "pos_x": 3,
                      "pos_y": 0,
                      "size_x": 3,
                      "size_y": 4
                    }
                  ],
                  "visualization_id": "${databricks_sql_visualization.q1v1.id}"
                }
              references:
                dashboard_id: databricks_sql_dashboard.d1.id
                visualization_id: databricks_sql_visualization.q1v1.id
        argumentDocs: {}
        importStatements: []
    databricks_storage_credential:
        subCategory: Unity Catalog
        description: '""subcategory: "Unity Catalog"'
        name: databricks_storage_credential
        title: databricks_storage_credential Resource
        examples:
            - name: external
              manifest: |-
                {
                  "aws_iam_role": [
                    {
                      "role_arn": "${aws_iam_role.external_data_access.arn}"
                    }
                  ],
                  "comment": "Managed by TF",
                  "name": "${aws_iam_role.external_data_access.name}"
                }
              references:
                aws_iam_role.role_arn: aws_iam_role.external_data_access.arn
                name: aws_iam_role.external_data_access.name
              dependencies:
                databricks_grants.external_creds: |-
                    {
                      "grant": [
                        {
                          "principal": "Data Engineers",
                          "privileges": [
                            "CREATE_EXTERNAL_TABLE"
                          ]
                        }
                      ],
                      "storage_credential": "${databricks_storage_credential.external.id}"
                    }
            - name: external_mi
              manifest: |-
                {
                  "azure_managed_identity": [
                    {
                      "access_connector_id": "${azurerm_databricks_access_connector.example.id}"
                    }
                  ],
                  "comment": "Managed identity credential managed by TF",
                  "name": "mi_credential"
                }
              references:
                azure_managed_identity.access_connector_id: azurerm_databricks_access_connector.example.id
              dependencies:
                databricks_grants.external_creds: |-
                    {
                      "grant": [
                        {
                          "principal": "Data Engineers",
                          "privileges": [
                            "CREATE_EXTERNAL_TABLE"
                          ]
                        }
                      ],
                      "storage_credential": "${databricks_storage_credential.external.id}"
                    }
            - name: external
              manifest: |-
                {
                  "databricks_gcp_service_account": [
                    {}
                  ],
                  "name": "the-creds"
                }
              dependencies:
                databricks_grants.external_creds: |-
                    {
                      "grant": [
                        {
                          "principal": "Data Engineers",
                          "privileges": [
                            "CREATE_EXTERNAL_TABLE"
                          ]
                        }
                      ],
                      "storage_credential": "${databricks_storage_credential.external.id}"
                    }
        argumentDocs:
            aws_iam_role.role_arn: '- The Amazon Resource Name (ARN) of the AWS IAM role for S3 data access, of the form arn:aws:iam::1234567890:role/MyRole-AJJHDSKSDF'
            azure_managed_identity.access_connector_id: '- The Resource ID of the Azure Databricks Access Connector resource, of the form /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.Databricks/accessConnectors/connector-name.'
            azure_managed_identity.managed_identity_id: '- (Optional) The Resource ID of the Azure User Assigned Managed Identity associated with Azure Databricks Access Connector, of the form /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.ManagedIdentity/userAssignedIdentities/user-managed-identity-name.'
            azure_service_principal.application_id: '- The application ID of the application registration within the referenced AAD tenant'
            azure_service_principal.client_secret: '- The client secret generated for the above app ID in AAD. This field is redacted on output'
            azure_service_principal.directory_id: '- The directory ID corresponding to the Azure Active Directory (AAD) tenant of the application'
            cloudflare_api_token.access_key_id: '- R2 API token access key ID'
            cloudflare_api_token.account_id: '- R2 account ID'
            cloudflare_api_token.secret_access_key: '- R2 API token secret access key'
            databricks_gcp_service_account.email: (output only) - The email of the GCP service account created, to be granted access to relevant buckets.
            databricks_storage_credential: represents authentication methods to access cloud storage (e.g. an IAM role for Amazon S3 or a service principal/managed identity for Azure Storage). Storage credentials are access-controlled to determine which users can use the credential.
            force_destroy: '- (Optional) Delete storage credential regardless of its dependencies.'
            force_update: '- (Optional) Update storage credential regardless of its dependents.'
            id: '- ID of this storage credential - same as the name.'
            isolation_mode: '- (Optional) Whether the storage credential is accessible from all workspaces or a specific set of workspaces. Can be ISOLATION_MODE_ISOLATED or ISOLATION_MODE_OPEN. Setting the credential to ISOLATION_MODE_ISOLATED will automatically allow access from the current workspace.'
            metastore_id: '- (Required for account-level) Unique identifier of the parent Metastore. If set for workspace-level, it must match the ID of the metastore assigned to the worspace. When changing the metastore assigned to a workspace, this field becomes required.'
            name: '- Name of Storage Credentials, which must be unique within the databricks_metastore. Change forces creation of a new resource.'
            owner: '- (Optional) Username/groupname/sp application_id of the storage credential owner.'
            read_only: '- (Optional) Indicates whether the storage credential is only usable for read operations.'
            skip_validation: '- (Optional) Suppress validation errors if any & force save the storage credential.'
            storage_credential_id: '- Unique ID of storage credential.'
        importStatements: []
    databricks_system_schema:
        subCategory: Unity Catalog
        description: '""subcategory: "Unity Catalog"'
        name: databricks_system_schema
        title: databricks_system_schema Resource
        examples:
            - name: this
              manifest: |-
                {
                  "schema": "access"
                }
        argumentDocs:
            full_name: '- the full name of the system schema, in form of system.<schema>.'
            id: '- the ID of system schema in form of metastore_id|schema_name.'
            schema: '- (Required) name of the system schema.'
            state: '- The current state of enablement for the system schema.'
        importStatements: []
    databricks_token:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_token
        title: databricks_token Resource
        examples:
            - name: pat
              manifest: |-
                {
                  "comment": "Terraform Provisioning",
                  "lifetime_seconds": 8640000,
                  "provider": "${databricks.created_workspace}"
                }
              references:
                provider: databricks.created_workspace
            - name: pat
              manifest: |-
                {
                  "comment": "Terraform (created: ${time_rotating.this.rfc3339})",
                  "lifetime_seconds": "${60 * 24 * 60 * 60}"
                }
              dependencies:
                time_rotating.this: |-
                    {
                      "rotation_days": 30
                    }
        argumentDocs:
            comment: '- (Optional) (String) Comment that will appear on the user’s settings page for this token.'
            id: '- Canonical unique identifier for the token.'
            lifetime_seconds: '- (Optional) (Integer) The lifetime of the token, in seconds. If no lifetime is specified, the token remains valid indefinitely.'
            token_value: '- Sensitive value of the newly-created token.'
        importStatements: []
    databricks_user:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_user
        title: databricks_user Resource
        examples:
            - name: me
              manifest: |-
                {
                  "user_name": "me@example.com"
                }
            - name: me
              manifest: |-
                {
                  "user_name": "me@example.com"
                }
              dependencies:
                databricks_group_member.i-am-admin: |-
                    {
                      "group_id": "${data.databricks_group.admins.id}",
                      "member_id": "${databricks_user.me.id}"
                    }
            - name: me
              manifest: |-
                {
                  "allow_cluster_create": true,
                  "display_name": "Example user",
                  "user_name": "me@example.com"
                }
            - name: account_user
              manifest: |-
                {
                  "display_name": "Example user",
                  "provider": "${databricks.mws}",
                  "user_name": "me@example.com"
                }
              references:
                provider: databricks.mws
            - name: account_user
              manifest: |-
                {
                  "display_name": "Example user",
                  "provider": "${databricks.azure_account}",
                  "user_name": "me@example.com"
                }
              references:
                provider: databricks.azure_account
        argumentDocs:
            acl_principal_id: '- identifier for use in databricks_access_control_rule_set, e.g. users/mr.foo@example.com.'
            active: '- (Optional) Either user is active or not. True by default, but can be set to false in case of user deactivation with preserving user assets.'
            allow_cluster_create: '-  (Optional) Allow the user to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and cluster_id argument. Everyone without allow_cluster_create argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.'
            allow_instance_pool_create: '-  (Optional) Allow the user to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and instance_pool_id argument.'
            databricks_sql_access: '- (Optional) This is a field to allow the group to have access to Databricks SQL feature in User Interface and through databricks_sql_endpoint.'
            disable_as_user_deletion: '- (Optional) Deactivate the user when deleting the resource, rather than deleting the user entirely. Defaults to true when the provider is configured at the account-level and false when configured at the workspace-level. This flag is exclusive to force_delete_repos and force_delete_home_dir flags.'
            display_name: '- (Optional) This is an alias for the username that can be the full name of the user.'
            external_id: '- (Optional) ID of the user in an external identity provider.'
            force: '- (Optional) Ignore cannot create user: User with username X already exists errors and implicitly import the specific user into Terraform state, enforcing entitlements defined in the instance of resource. This functionality is experimental and is designed to simplify corner cases, like Azure Active Directory synchronisation.'
            force_delete_home_dir: '- (Optional) This flag determines whether the user''s home directory is deleted when the user is deleted. It will have not impact when in the accounts SCIM API. False by default.'
            force_delete_repos: '- (Optional) This flag determines whether the user''s repo directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.'
            home: '- Home folder of the user, e.g. /Users/mr.foo@example.com.'
            id: '- Canonical unique identifier for the user.'
            repos: '- Personal Repos location of the user, e.g. /Repos/mr.foo@example.com.'
            user_name: '- (Required) This is the username of the given user and will be their form of access and identity.  Provided username will be converted to lower case if it contains upper case characters.'
        importStatements: []
    databricks_user_instance_profile:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_user_instance_profile
        title: databricks_user_instance_profile Resource
        examples:
            - name: my_user_instance_profile
              manifest: |-
                {
                  "instance_profile_id": "${databricks_instance_profile.instance_profile.id}",
                  "user_id": "${databricks_user.my_user.id}"
                }
              references:
                instance_profile_id: databricks_instance_profile.instance_profile.id
                user_id: databricks_user.my_user.id
              dependencies:
                databricks_instance_profile.instance_profile: |-
                    {
                      "instance_profile_arn": "my_instance_profile_arn"
                    }
                databricks_user.my_user: |-
                    {
                      "user_name": "me@example.com"
                    }
        argumentDocs:
            id: '- The id in the format <user_id>|<instance_profile_id>.'
            instance_profile_id: '-  (Required) This is the id of the instance profile resource.'
            user_id: '- (Required) This is the id of the user resource.'
        importStatements: []
    databricks_user_role:
        subCategory: Security
        description: '""subcategory: "Security"'
        name: databricks_user_role
        title: databricks_user_role Resource
        examples:
            - name: my_user_role
              manifest: |-
                {
                  "role": "${databricks_instance_profile.instance_profile.id}",
                  "user_id": "${databricks_user.my_user.id}"
                }
              references:
                role: databricks_instance_profile.instance_profile.id
                user_id: databricks_user.my_user.id
              dependencies:
                databricks_instance_profile.instance_profile: |-
                    {
                      "instance_profile_arn": "my_instance_profile_arn"
                    }
                databricks_user.my_user: |-
                    {
                      "user_name": "me@example.com"
                    }
            - name: my_user_account_admin
              manifest: |-
                {
                  "role": "account_admin",
                  "user_id": "${databricks_user.my_user.id}"
                }
              references:
                user_id: databricks_user.my_user.id
              dependencies:
                databricks_user.my_user: |-
                    {
                      "user_name": "me@example.com"
                    }
        argumentDocs:
            id: '- The id in the format <user_id>|<role>.'
            role: '-  (Required) Either a role name or the ARN/ID of the instance profile resource.'
            user_id: '- (Required) This is the id of the user resource.'
        importStatements: []
    databricks_vector_search_endpoint:
        subCategory: Mosaic AI Vector Search
        description: '""subcategory: "Mosaic AI Vector Search"'
        name: databricks_vector_search_endpoint
        title: databricks_vector_search_endpoint Resource
        examples:
            - name: this
              manifest: |-
                {
                  "endpoint_type": "STANDARD",
                  "name": "vector-search-test"
                }
        argumentDocs:
            creation_timestamp: '- Timestamp of endpoint creation (milliseconds).'
            creator: '- Creator of the endpoint.'
            endpoint_id: '- Unique internal identifier of the endpoint (UUID).'
            endpoint_status: '- Object describing the current status of the endpoint consisting of the following fields:'
            endpoint_type: '(Required) Type of Mosaic AI Vector Search Endpoint.  Currently only accepting single value: STANDARD (See documentation for the list of currently supported values).'
            id: '- The same as the name of the endpoint.'
            last_updated_timestamp: '- Timestamp of the last update to the endpoint (milliseconds).'
            last_updated_user: '- User who last updated the endpoint.'
            message: '- Additional status message.'
            name: '- (Required) Name of the Mosaic AI Vector Search Endpoint to create.'
            num_indexes: '- Number of indexes on the endpoint.'
            state: '- Current state of the endpoint. Currently following values are supported: PROVISIONING, ONLINE, and OFFLINE.'
        importStatements: []
    databricks_vector_search_index:
        subCategory: Mosaic AI Vector Search
        description: '""subcategory: "Mosaic AI Vector Search"'
        name: databricks_vector_search_index
        title: databricks_vector_search_index Resource
        examples:
            - name: sync
              manifest: |-
                {
                  "delta_sync_index_spec": [
                    {
                      "embedding_source_columns": [
                        {
                          "embedding_model_endpoint_name": "${databricks_model_serving.this.name}",
                          "name": "text"
                        }
                      ],
                      "pipeline_type": "TRIGGERED",
                      "source_table": "main.default.source_table"
                    }
                  ],
                  "endpoint_name": "${databricks_vector_search_endpoint.this.name}",
                  "index_type": "DELTA_SYNC",
                  "name": "main.default.vector_search_index",
                  "primary_key": "id"
                }
              references:
                delta_sync_index_spec.embedding_source_columns.embedding_model_endpoint_name: databricks_model_serving.this.name
                endpoint_name: databricks_vector_search_endpoint.this.name
        argumentDocs:
            CONTINUOUS: ': If the pipeline uses continuous execution, the pipeline processes new data as it arrives in the source table to keep the vector index fresh.'
            DELTA_SYNC: ': An index that automatically syncs with a source Delta Table, automatically and incrementally updating the index as the underlying data in the Delta Table changes.'
            DIRECT_ACCESS: ': An index that supports the direct read and write of vectors and metadata through our REST and SDK APIs. With this model, the user manages index updates.'
            TRIGGERED: ': If the pipeline uses the triggered execution mode, the system stops processing after successfully refreshing the source table in the pipeline once, ensuring the table is updated based on the data available when the update started.'
            columns_to_sync: '- (optional) list of columns to sync. If not specified, all columns are syncronized.'
            creator: '- Creator of the endpoint.'
            delta_sync_index_spec: '- (object) Specification for Delta Sync Index. Required if index_type is DELTA_SYNC.'
            direct_access_index_spec: '- (object) Specification for Direct Vector Access Index. Required if index_type is DIRECT_ACCESS.'
            embedding_dimension: '- Dimension of the embedding vector.'
            embedding_model_endpoint_name: '- The name of the embedding model endpoint'
            embedding_source_columns: '- (required if embedding_vector_columns isn''t provided) array of objects representing columns that contain the embedding source.  Each entry consists of:'
            embedding_vector_columns: '- (required if embedding_source_columns isn''t provided)  array of objects representing columns that contain the embedding vectors. Each entry consists of:'
            endpoint_name: '- (required) The name of the Mosaic AI Vector Search Endpoint that will be used for indexing the data.'
            id: '- The same as the name of the index.'
            index_type: '- (required) Mosaic AI Vector Search index type. Currently supported values are:'
            index_url: '- Index API Url to be used to perform operations on the index'
            indexed_row_count: '- Number of rows indexed'
            message: '- Message associated with the index status'
            name: '- (required) Three-level name of the Mosaic AI Vector Search Index to create (catalog.schema.index_name).'
            pipeline_id: '- ID of the associated Delta Live Table pipeline.'
            pipeline_type: '- Pipeline execution mode. Possible values are:'
            primary_key: '- (required) The column name that will be used as a primary key.'
            ready: '- Whether the index is ready for search'
            schema_json: '- The schema of the index in JSON format.  Check the API documentation for a list of supported data types.'
            source_table: (required) The name of the source table.
            status: '- Object describing the current status of the index consisting of the following fields:'
        importStatements: []
    databricks_volume:
        subCategory: Unity Catalog
        description: '""subcategory: "Unity Catalog"'
        name: databricks_volume
        title: databricks_volume Resource
        examples:
            - name: this
              manifest: |-
                {
                  "catalog_name": "${databricks_catalog.sandbox.name}",
                  "comment": "this volume is managed by terraform",
                  "name": "quickstart_volume",
                  "schema_name": "${databricks_schema.things.name}",
                  "storage_location": "${databricks_external_location.some.url}",
                  "volume_type": "EXTERNAL"
                }
              references:
                catalog_name: databricks_catalog.sandbox.name
                schema_name: databricks_schema.things.name
                storage_location: databricks_external_location.some.url
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
                databricks_external_location.some: |-
                    {
                      "credential_name": "${databricks_storage_credential.external.name}",
                      "name": "external_location",
                      "url": "s3://${aws_s3_bucket.external.id}/some"
                    }
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.name}",
                      "comment": "this schema is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
                databricks_storage_credential.external: |-
                    {
                      "aws_iam_role": [
                        {
                          "role_arn": "${aws_iam_role.external_data_access.arn}"
                        }
                      ],
                      "name": "creds"
                    }
        argumentDocs:
            <catalogName>: ': The name of the catalog containing the Volume.'
            <schemaName>: ': The name of the schema containing the Volume.'
            <volumeName>: ': The name of the Volume. It identifies the volume object.'
            catalog_name: '- Name of parent Catalog. Change forces creation of a new resource.'
            comment: '- (Optional) Free-form text.'
            id: '- ID of this Unity Catalog Volume in form of <catalog>.<schema>.<name>.'
            name: '- Name of the Volume'
            owner: '- (Optional) Name of the volume owner.'
            schema_name: '- Name of parent Schema relative to parent Catalog. Change forces creation of a new resource.'
            storage_location: '- (Optional) Path inside an External Location. Only used for EXTERNAL Volumes. Change forces creation of a new resource.'
            volume_path: '- base file path for this Unity Catalog Volume in form of /Volumes/<catalog>/<schema>/<name>.'
            volume_type: '- Volume type. EXTERNAL or MANAGED. Change forces creation of a new resource.'
        importStatements: []
    databricks_workspace_binding:
        subCategory: Unity Catalog
        description: '""subcategory: "Unity Catalog"'
        name: databricks_workspace_binding
        title: databricks_workspace_binding Resource
        examples:
            - name: sandbox
              manifest: |-
                {
                  "securable_name": "${databricks_catalog.sandbox.name}",
                  "workspace_id": "${databricks_mws_workspaces.other.workspace_id}"
                }
              references:
                securable_name: databricks_catalog.sandbox.name
                workspace_id: databricks_mws_workspaces.other.workspace_id
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "isolation_mode": "ISOLATED",
                      "name": "sandbox"
                    }
        argumentDocs:
            binding_type: '- (Optional) Binding mode. Default to BINDING_TYPE_READ_WRITE. Possible values are BINDING_TYPE_READ_ONLY, BINDING_TYPE_READ_WRITE.'
            securable_name: '- Name of securable. Change forces creation of a new resource.'
            securable_type: '- Type of securable. Can be catalog, external-location or storage-credential. Default to catalog. Change forces creation of a new resource.'
            workspace_id: '- ID of the workspace. Change forces creation of a new resource.'
        importStatements: []
    databricks_workspace_conf:
        subCategory: Workspace
        description: '""subcategory: "Workspace"'
        name: databricks_workspace_conf
        title: databricks_workspace_conf Resource
        examples:
            - name: this
              manifest: |-
                {
                  "custom_config": {
                    "enableIpAccessLists": true
                  }
                }
        argumentDocs:
            custom_config: '- (Required) Key-value map of strings that represent workspace configuration. Upon resource deletion, properties that start with enable or enforce will be reset to false value, regardless of initial default one.'
            enableDeprecatedClusterNamedInitScripts: '- (boolean) Enable or disable legacy cluster-named init scripts for this workspace.'
            enableDeprecatedGlobalInitScripts: '- (boolean) Enable or disable legacy global init scripts for this workspace.'
            enableIpAccessLists: '- enables the use of databricks_ip_access_list resources'
            enableTokensConfig: '- (boolean) Enable or disable personal access tokens for this workspace.'
            maxTokenLifetimeDays: '- (string) Maximum token lifetime of new tokens in days, as an integer. If zero, new tokens are permitted to have no lifetime limit. Negative numbers are unsupported. WARNING: This limit only applies to new tokens, so there may be tokens with lifetimes longer than this value, including unlimited lifetime. Such tokens may have been created before the current maximum token lifetime was set.'
        importStatements: []
    databricks_workspace_file:
        subCategory: Workspace
        description: '""subcategory: "Workspace"'
        name: databricks_workspace_file
        title: databricks_workspace_file Resource
        examples:
            - name: module
              manifest: |-
                {
                  "path": "${data.databricks_current_user.me.home}/AA/BB/CC",
                  "source": "${path.module}/module.py"
                }
            - name: init_script
              manifest: |-
                {
                  "content_base64": "${base64encode(\u003c\u003c-EOT\n    #!/bin/bash\n    echo \"Hello World\"\n    EOT\n  )}",
                  "path": "/Shared/init-script.sh"
                }
        argumentDocs:
            content_base64: '- The base64-encoded file content. Conflicts with source. Use of content_base64 is discouraged, as it''s increasing memory footprint of Terraform state and should only be used in exceptional circumstances, like creating a workspace file with configuration properties for a data pipeline.'
            id: '-  Path of workspace file'
            object_id: '-  Unique identifier for a workspace file'
            path: '-  (Required) The absolute path of the workspace file, beginning with "/", e.g. "/Demo".'
            source: '- Path to file on local filesystem. Conflicts with content_base64.'
            url: '- Routable URL of the workspace file'
            workspace_path: '- path on Workspace File System (WSFS) in form of /Workspace + path'
        importStatements: []
